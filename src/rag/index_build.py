# src/rag/index_build.py
from __future__ import annotations
import os
from typing import Callable, Mapping, Dict, List, Any

import streamlit as st

from .storage import _make_storage_context
from .checkpoint import _load_checkpoint, _mark_done
from .quality import get_opt, preprocess_docs, maybe_summarize_docs, load_quality_report, save_quality_report

def build_index_with_checkpoint(
    update_pct: Callable[[int, str | None], None],
    update_msg: Callable[[str], None],
    gdrive_folder_id: str,
    gcp_creds: Mapping[str, Any],
    persist_dir: str,
    remote_manifest: Dict[str, Dict],
    should_stop: Callable[[], bool] | None = None,
):
    """
    íŒŒì¼ID ë‹¨ìœ„ ì²˜ë¦¬ â†’ ê° íŒŒì¼ ì™„ì£¼ í›„ ì €ì¥ & ì²´í¬í¬ì¸íŠ¸ ê¸°ë¡.
    ì „ì²˜ë¦¬/ì¤‘ë³µì œê±°/ë¬¸ì„œìš”ì•½ â†’ SentenceSplitter ì²­í‚¹ â†’ ì¸ë±ìŠ¤ ëˆ„ì .
    ì¤‘ì§€ ë²„íŠ¼(should_stop=True) ê°ì§€ ì‹œ 'í˜„ì¬ íŒŒì¼ê¹Œì§€' ì €ì¥ í›„ ì•ˆì „ ì¢…ë£Œ.
    """
    from llama_index.core import VectorStoreIndex, load_index_from_storage
    from llama_index.core.node_parser import SentenceSplitter
    from llama_index.readers.google import GoogleDriveReader

    if should_stop is None:
        should_stop = lambda: False

    opt = get_opt()

    update_pct(15, "Drive ë¦¬ë” ì´ˆê¸°í™”")
    loader = GoogleDriveReader(service_account_key=gcp_creds)

    cp = _load_checkpoint()
    todo_ids: List[str] = []
    for fid, meta in remote_manifest.items():
        done = cp.get(fid)
        if done and done.get("md5") and meta.get("md5") and done["md5"] == meta["md5"]:
            continue
        todo_ids.append(fid)

    total = len(remote_manifest)
    pending = len(todo_ids)
    done_cnt = total - pending
    update_pct(30, f"ë¬¸ì„œ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ â€¢ ì „ì²´ {total}ê°œ, ì´ë²ˆì— ì²˜ë¦¬ {pending}ê°œ")

    os.makedirs(persist_dir, exist_ok=True)
    storage_context = _make_storage_context(persist_dir)
    try:
        _ = load_index_from_storage(storage_context)
    except Exception:
        pass

    # í’ˆì§ˆ ë¦¬í¬íŠ¸
    qrep = load_quality_report()
    qrep.setdefault("summary", {}).setdefault("total_docs", total)
    for k in ("processed_docs","kept_docs","skipped_low_text","skipped_dup","total_chars"):
        qrep["summary"].setdefault(k, 0)
    qrep.setdefault("files", {})
    seen_hashes = set(h for h in [])  # ì´ˆê¸°í™”(íŒŒì¼ë³„ë¡œ ëˆ„ì )

    if pending == 0:
        update_pct(95, "ë³€ê²½ ì—†ìŒ â†’ ì €ì¥ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        return load_index_from_storage(storage_context)

    splitter = SentenceSplitter(chunk_size=opt["chunk_size"], chunk_overlap=opt["chunk_overlap"])

    for i, fid in enumerate(todo_ids, start=1):
        if should_stop():
            update_msg("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ â€” í˜„ì¬ê¹Œì§€ ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        meta = remote_manifest.get(fid, {})
        fname = meta.get("name") or fid
        update_msg(f"ì „ì²˜ë¦¬ â€¢ {fname} ({done_cnt + i}/{total})")

        # 1) íŒŒì¼ ë¡œë“œ
        try:
            docs = loader.load_data(file_ids=[fid])
        except TypeError:
            st.error("GoogleDriveReader ë²„ì „ì´ ì˜¤ë˜ë˜ì–´ file_ids ì˜µì…˜ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. requirements ì—…ë°ì´íŠ¸ í•„ìš”.")
            st.stop()

        # 2) ì „ì²˜ë¦¬/ì¤‘ë³µì œê±°
        kept, stats = preprocess_docs(
            docs, seen_hashes,
            min_chars=opt["min_chars"], dedup=opt["dedup"]
        )
        maybe_summarize_docs(kept)

        # 3) í’ˆì§ˆ ë¦¬í¬íŠ¸ ê°±ì‹ (íŒŒì¼ ë‹¨ìœ„)
        qrep.setdefault("files", {})[fid] = {
            "name": fname,
            "md5": meta.get("md5"),
            "modifiedTime": meta.get("modifiedTime"),
            "kept": stats["kept"],
            "skipped_low_text": stats["skipped_low_text"],
            "skipped_dup": stats["skipped_dup"],
            "total_chars": stats["total_chars"],
        }
        qs = qrep.setdefault("summary", {})
        qs["processed_docs"] = qs.get("processed_docs", 0) + 1
        qs["kept_docs"] = qs.get("kept_docs", 0) + stats["kept"]
        qs["skipped_low_text"] = qs.get("skipped_low_text", 0) + stats["skipped_low_text"]
        qs["skipped_dup"] = qs.get("skipped_dup", 0) + stats["skipped_dup"]
        qs["total_chars"] = qs.get("total_chars", 0) + stats["total_chars"]
        save_quality_report(qrep)

        if stats["kept"] == 0:
            _mark_done(cp, fid, meta)  # ì™„ë£Œ ì²´í¬ë§Œ
            pct = 30 + int((i / max(1, pending)) * 60)
            update_pct(pct, f"ê±´ë„ˆëœ€ â€¢ {fname} (ì €í’ˆì§ˆ/ì¤‘ë³µ)")
            if should_stop():
                update_msg("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ â€” í˜„ì¬ê¹Œì§€ ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            continue

        # 4) ì¸ë±ìŠ¤ì— ëˆ„ì  ì¶”ê°€
        update_msg(f"ì¸ë±ìŠ¤ ìƒì„± â€¢ {fname} ({done_cnt + i}/{total})")
        try:
            VectorStoreIndex.from_documents(
                kept, storage_context=storage_context, show_progress=False,
                transformations=[splitter]
            )
            storage_context.persist(persist_dir=persist_dir)  # ë¶€ë¶„ ì €ì¥
            _mark_done(cp, fid, meta)                         # íŒŒì¼ ì™„ë£Œ ê¸°ë¡
        except Exception as e:
            st.error(f"ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {fname}")
            with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°"):
                st.exception(e)
            st.stop()

        pct = 30 + int((i / max(1, pending)) * 60)
        update_pct(pct, f"ì™„ë£Œ â€¢ {fname}")

        if should_stop():
            update_msg("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ â€” í˜„ì¬ íŒŒì¼ê¹Œì§€ ì €ì¥ ì™„ë£Œ, ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    update_pct(95, "ë‘ë‡Œ ì €ì¥ ì¤‘")
    try:
        storage_context.persist(persist_dir=persist_dir)
    except Exception as e:
        st.error("ì¸ë±ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°", expanded=True):
            st.exception(e)
        st.stop()

    update_pct(100, "ì™„ë£Œ")
    return load_index_from_storage(storage_context)
