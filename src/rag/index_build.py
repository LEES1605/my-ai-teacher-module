# ===== [01] IMPORTS ==========================================================
from __future__ import annotations

import os
from typing import Callable, Mapping, Dict, List, Any, Set

import streamlit as st

from .storage import _make_storage_context
from .checkpoint import (
    _load_checkpoint,
    _mark_done,
    save_checkpoint_copy_to_persist,
)
from .quality import (
    get_opt,
    preprocess_docs,
    maybe_summarize_docs,
    load_quality_report,
    save_quality_report,
)

# ===== [02] TYPE HINTS =======================================================
UpdatePct = Callable[[int, str | None], None]
UpdateMsg = Callable[[str], None]
Manifest = Dict[str, Dict[str, Any]]

# ===== [03] SAFE IMPORT (RUNTIME) ============================================
def _safe_import_llama():
    try:
        from llama_index.core import VectorStoreIndex, load_index_from_storage
        from llama_index.core.node_parser import SentenceSplitter
        from llama_index.readers.google import GoogleDriveReader
        return VectorStoreIndex, load_index_from_storage, SentenceSplitter, GoogleDriveReader
    except Exception as e:
        st.error("llama_index ëª¨ë“ˆ ë¡œë“œ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. requirements ë²„ì „ì„ í™•ì¸í•˜ì„¸ìš”.")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°", expanded=True):
            st.exception(e)
        st.stop()
        raise  # for type-checkers

# ===== [04] MAIN ENTRY =======================================================
def build_index_with_checkpoint(
    update_pct: UpdatePct,
    update_msg: UpdateMsg,
    gdrive_folder_id: str,
    gcp_creds: Mapping[str, Any],
    persist_dir: str,
    remote_manifest: Manifest,
    should_stop: Callable[[], bool] | None = None,
):
    """
    íŒŒì¼ID ë‹¨ìœ„ ì²˜ë¦¬ â†’ ê° íŒŒì¼ ì™„ì£¼ í›„ ì €ì¥ & ì²´í¬í¬ì¸íŠ¸ ê¸°ë¡.
    ì „ì²˜ë¦¬/ì¤‘ë³µì œê±°/ë¬¸ì„œìš”ì•½ â†’ SentenceSplitter ì²­í‚¹ â†’ ì¸ë±ìŠ¤ ëˆ„ì .
    ì¤‘ì§€ ë²„íŠ¼(should_stop=True) ê°ì§€ ì‹œ 'í˜„ì¬ íŒŒì¼ê¹Œì§€' ì €ì¥ í›„ ì•ˆì „ ì¢…ë£Œ.
    """
    VectorStoreIndex, load_index_from_storage, SentenceSplitter, GoogleDriveReader = _safe_import_llama()

    # ----- [04.1] STOP SWITCH -------------------------------------------------
    if should_stop is None:
        should_stop = lambda: False  # noqa: E731

    # ----- [04.2] OPTIONS -----------------------------------------------------
    opt = get_opt()

    # ----- [04.3] LOADER & CHECKPOINT ----------------------------------------
    update_pct(15, "Drive ë¦¬ë” ì´ˆê¸°í™”")
    loader = GoogleDriveReader(service_account_key=gcp_creds)

    # persist_dirì— ë‚¨ì•„ìˆì„ ìˆ˜ ìˆëŠ” checkpoint.jsonë„ ìë™ ë¡œë“œ
    cp: Dict[str, Dict[str, Any]] = _load_checkpoint(also_from_persist_dir=persist_dir)

# ----- [04.4] TODO LIST (ë³€ê²½ë¶„ë§Œ ì²˜ë¦¬) -----------------------------------
todo_ids: List[str] = []
for fid, manifest_meta in remote_manifest.items():
    done = cp.get(fid)
    if done and done.get("md5") and manifest_meta.get("md5") and done["md5"] == manifest_meta["md5"]:
        continue
    todo_ids.append(fid)

total = len(remote_manifest)
pending = len(todo_ids)
done_cnt = total - pending
update_pct(30, f"ë¬¸ì„œ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ â€¢ ì „ì²´ {total}ê°œ, ì´ë²ˆì— ì²˜ë¦¬ {pending}ê°œ")

    # ----- [04.5] STORAGE CONTEXT & PRELOAD ----------------------------------
    os.makedirs(persist_dir, exist_ok=True)
    storage_context = _make_storage_context(persist_dir)
    try:
        _ = load_index_from_storage(storage_context)
    except Exception:
        # ì €ì¥ëœ ì¸ë±ìŠ¤ê°€ ì—†ê±°ë‚˜, ì¼ë¶€ íŒŒì¼ë§Œ ìˆì„ ìˆ˜ ìˆìŒ â†’ í†µê³¼
        pass

    # ===== [05] QUALITY REPORT & STATE =======================================
    qrep = load_quality_report()
    qrep.setdefault("summary", {}).setdefault("total_docs", total)
    for k in ("processed_docs", "kept_docs", "skipped_low_text", "skipped_dup", "total_chars"):
        qrep["summary"].setdefault(k, 0)
    qrep.setdefault("files", {})

    # í•´ì‹œ ì¤‘ë³µ ì œê±°ìš© ì§‘í•© â€” ëª…ì‹œì  íƒ€ì… (mypy)
    seen_hashes: Set[str] = set()

    # ===== [06] EARLY RETURN (NO PENDING) ====================================
    if pending == 0:
        update_pct(95, "ë³€ê²½ ì—†ìŒ â†’ ì €ì¥ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        return load_index_from_storage(storage_context)

    # ===== [07] CHUNKER INITIALIZE ===========================================
    splitter = SentenceSplitter(
        chunk_size=int(opt["chunk_size"]),
        chunk_overlap=int(opt["chunk_overlap"]),
    )

# ===== [08] MAIN LOOP =====================================================
for i, fid in enumerate(todo_ids, start=1):
    # ----- [08.1] STOP CHECK ---------------------------------------------
    if should_stop():
        update_msg("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ â€” í˜„ì¬ê¹Œì§€ ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break

    # ----- [08.2] LOAD SINGLE FILE ---------------------------------------
    fmeta: Dict[str, Any] = remote_manifest.get(fid, {})
    fname = str(fmeta.get("name") or fid)
    update_msg(f"ì „ì²˜ë¦¬ â€¢ {fname} ({done_cnt + i}/{total})")

    try:
        # ì¼ë¶€ ë²„ì „ì€ file_ids íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•Šì•„ TypeErrorê°€ ë‚  ìˆ˜ ìˆìŒ
        docs = loader.load_data(file_ids=[fid])
    except TypeError:
        st.error(
            "GoogleDriveReader ë²„ì „ì´ ì˜¤ë˜ë˜ì–´ file_ids ì˜µì…˜ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
            "requirements ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        )
        st.stop()

    # ----- [08.3] PREPROCESS & DEDUP -------------------------------------
    kept, stats = preprocess_docs(
        docs,
        seen_hashes,
        min_chars=int(opt["min_chars"]),
        dedup=bool(opt["dedup"]),
    )
    maybe_summarize_docs(kept)

    # ----- [08.4] QUALITY REPORT UPDATE ----------------------------------
    from_name = fmeta.get("name")
    qrep.setdefault("files", {})[fid] = {
        "name": from_name or fid,
        "md5": fmeta.get("md5"),
        "modifiedTime": fmeta.get("modifiedTime"),
        "kept": stats["kept"],
        "skipped_low_text": stats["skipped_low_text"],
        "skipped_dup": stats["skipped_dup"],
        "total_chars": stats["total_chars"],
    }
    qs = qrep.setdefault("summary", {})
    qs["processed_docs"] = int(qs.get("processed_docs", 0)) + 1
    qs["kept_docs"] = int(qs.get("kept_docs", 0)) + int(stats["kept"])
    qs["skipped_low_text"] = int(qs.get("skipped_low_text", 0)) + int(stats["skipped_low_text"])
    qs["skipped_dup"] = int(qs.get("skipped_dup", 0)) + int(stats["skipped_dup"])
    qs["total_chars"] = int(qs.get("total_chars", 0)) + int(stats["total_chars"])
    save_quality_report(qrep)

    # ----- [08.5] SKIP CASE (NO KEPT) ------------------------------------
    if int(stats["kept"]) == 0:
        _mark_done(cp, fid, fmeta)                          # ì™„ë£Œ ì²´í¬
        save_checkpoint_copy_to_persist(cp, persist_dir)    # persist_dirì—ë„ ë™ê¸°í™”
        pct = 30 + int((i / max(1, pending)) * 60)
        update_pct(pct, f"ê±´ë„ˆëœ€ â€¢ {fname} (ì €í’ˆì§ˆ/ì¤‘ë³µ)")
        if should_stop():
            update_msg("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ â€” í˜„ì¬ê¹Œì§€ ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        continue

    # ----- [08.6] INDEX APPEND ------------------------------------------
    update_msg(f"ì¸ë±ìŠ¤ ìƒì„± â€¢ {fname} ({done_cnt + i}/{total})")
    try:
        VectorStoreIndex.from_documents(
            kept,
            storage_context=storage_context,
            show_progress=False,
            transformations=[splitter],
        )
        storage_context.persist(persist_dir=persist_dir)     # ë¶€ë¶„ ì €ì¥
        _mark_done(cp, fid, fmeta)                           # íŒŒì¼ ì™„ë£Œ ê¸°ë¡
        save_checkpoint_copy_to_persist(cp, persist_dir)     # persist_dirì—ë„ ë™ê¸°í™”
    except Exception as e:
        st.error(f"ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {fname}")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°"):
            st.exception(e)
        st.stop()

    # ----- [08.7] PROGRESS UPDATE ---------------------------------------
    pct = 30 + int((i / max(1, pending)) * 60)
    update_pct(pct, f"ì™„ë£Œ â€¢ {fname}")

    # ----- [08.8] STOP CHECK (AFTER SAVE) --------------------------------
    if should_stop():
        update_msg("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ â€” í˜„ì¬ íŒŒì¼ê¹Œì§€ ì €ì¥ ì™„ë£Œ, ì¢…ë£Œí•©ë‹ˆë‹¤.")
        break


    # ===== [09] FINALIZE & RETURN ============================================
    update_pct(95, "ë‘ë‡Œ ì €ì¥ ì¤‘")
    try:
        storage_context.persist(persist_dir=persist_dir)
    except Exception as e:
        st.error("ì¸ë±ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°", expanded=True):
            st.exception(e)
        st.stop()

    update_pct(100, "ì™„ë£Œ")
    return load_index_from_storage(storage_context)
