# src/rag_engine.py â€” RAG ìœ í‹¸(ì„ë² ë”© 1íšŒ + LLM 2ê°œ) + ì·¨ì†Œ/ì¬ê°œ + Google Docs Export ì§€ì›
#  - tqdm ì½˜ì†” ì§„í–‰ë°” ì–µì œ(TQDM_DISABLE)
#  - Google Docs/Sheets/SlidesëŠ” Drive "export"ë¡œ í…ìŠ¤íŠ¸/CSV ë³€í™˜ í›„ ì¸ë±ì‹±
#  - ì²« ì‹¤í–‰/ê¹¨ì§„ ì €ì¥ì†Œ ìë™ ë³µêµ¬

from __future__ import annotations
import os, json, shutil, re, hashlib, io
from typing import Callable, Any, Mapping, Iterable

# ğŸ”‡ tqdm(ì½˜ì†” ì§„í–‰ë°”) ì–µì œ â€” Streamlit Cloud ë¡œê·¸ ìŠ¤íŒ¸/ì›Œì»¤ë¶€í•˜ ì™„í™”
os.environ.setdefault("TQDM_DISABLE", "1")

import streamlit as st
from src.config import settings

# --- [NEW] ì¶œì²˜ íŒŒì¼ëª… ì¶”ì¶œ ìœ í‹¸ -------------------------------
def _source_names_from_nodes(nodes):
    """
    LlamaIndex ì‘ë‹µì˜ source_nodesì—ì„œ ì•ˆì „í•˜ê²Œ íŒŒì¼ëª…ì„ ë½‘ì•„ë‚¸ë‹¤.
    1) metadataì˜ ë‹¤ì–‘í•œ í‚¤ ì‹œë„
    2) file_idë§Œ ìˆì„ ê²½ìš°, ë¡œì»¬ Drive ë§¤ë‹ˆí˜ìŠ¤íŠ¸(settings.MANIFEST_PATH)ë¥¼ í™œìš©
    """
    import json, os
    names = set()

    # 1) ë¡œì»¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë¡œë“œ(ìˆìœ¼ë©´)
    manifest = {}
    try:
        if os.path.exists(settings.MANIFEST_PATH):
            with open(settings.MANIFEST_PATH, "r", encoding="utf-8") as fp:
                manifest = json.load(fp)
    except Exception:
        manifest = {}

    # 2) ë…¸ë“œ ë©”íƒ€ íŒŒì‹±
    for n in (nodes or []):
        meta = getattr(n, "metadata", {}) or {}
        # í”íˆ ì“°ì´ëŠ” í›„ë³´ í‚¤ë“¤ì„ ì°¨ë¡€ë¡œ í™•ì¸
        for k in ("file_name", "filename", "file", "source", "file_path", "document"):
            v = meta.get(k)
            if isinstance(v, str) and v.strip():
                names.add(v.strip())
                break
        else:
            # ìœ„ í‚¤ë“¤ì— ì‹¤íŒ¨í–ˆë‹¤ë©´ file_idë¥˜ë¡œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë§¤í•‘
            fid = meta.get("file_id") or meta.get("id") or meta.get("drive_file_id")
            if isinstance(fid, str) and fid in manifest:
                v = manifest[fid].get("name")
                if isinstance(v, str) and v.strip():
                    names.add(v.strip())

    return ", ".join(sorted(names)) if names else "ì¶œì²˜ ì •ë³´ ì—†ìŒ"

# ---------------------------------------------------------------

# (ì„ íƒ) llama_index ë¡œê·¸ ì–µì œ â€” ê³¼ë„í•œ ë””ë²„ê·¸ ì¶œë ¥ ë°©ì§€
import logging
logging.getLogger("llama_index").setLevel(logging.WARNING)

# Google API
from googleapiclient.http import MediaIoBaseDownload
from googleapiclient.errors import HttpError

# llama_index
from llama_index.core import Settings
from llama_index.core.storage.storage_context import StorageContext
from llama_index.core import load_index_from_storage, VectorStoreIndex
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.schema import Document

# ì·¨ì†Œ ì‹ í˜¸ìš© ì˜ˆì™¸
class CancelledError(Exception):
    """ì‚¬ìš©ìê°€ 'ì·¨ì†Œ' ë²„íŠ¼ì„ ëˆŒëŸ¬ ì‹¤í–‰ì„ ì¤‘ë‹¨í–ˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."""
    pass

# =============================================================================
# 0) ê³µí†µ: ì„œë¹„ìŠ¤ê³„ì • JSON ì •ê·œí™” + Drive ì„œë¹„ìŠ¤
# =============================================================================

def _normalize_sa(raw_sa: Any | None) -> Mapping[str, Any] | None:
    """ì„œë¹„ìŠ¤ê³„ì • JSONì„ dictë¡œ ì •ê·œí™”(+private_key ê°œí–‰ ë³´ì •)"""
    if raw_sa is None:
        return None
    if isinstance(raw_sa, Mapping):
        return dict(raw_sa)

    if isinstance(raw_sa, str):
        s = raw_sa.strip()
        if not s:
            return None
        # 1) ì •ìƒ JSON ì‹œë„
        try:
            return json.loads(s)
        except Exception:
            pass
        # 2) private_key ê°œí–‰ ë³´ì •
        try:
            m = re.search(r'"private_key"\s*:\s*"(?P<key>.*?)"', s, re.DOTALL)
            if m:
                key = m.group("key")
                key_fixed = key.replace("\r\n", "\n").replace("\n", "\\n")
                s_fixed = s[:m.start("key")] + key_fixed + s[m.end("key"):]
                return json.loads(s_fixed)
        except Exception:
            pass
    return None

def _build_drive_service(creds_dict):
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    scopes = [
        "https://www.googleapis.com/auth/drive.readonly",
        "https://www.googleapis.com/auth/drive.file",  # exportì—ë„ í•„ìš”
    ]
    creds = service_account.Credentials.from_service_account_info(creds_dict, scopes=scopes)
    return build("drive", "v3", credentials=creds, cache_discovery=False)

# =============================================================================
# 1) ì„ë² ë”©/LLM ì´ˆê¸°í™” (ì„ë² ë”© 1íšŒ, LLMì€ í•„ìš” ìˆ˜ë§Œí¼)
# =============================================================================

def set_embed_provider(provider: str, api_key: str, embed_model: str):
    """ì„ë² ë”© ê³µê¸‰ìë§Œ ì§€ì •í•´ Settings.embed_model ì„¤ì •"""
    p = (provider or "google").lower()
    try:
        if p == "openai":
            from llama_index.embeddings.openai import OpenAIEmbedding as _EMB
            Settings.embed_model = _EMB(model=embed_model, api_key=api_key)
        else:
            from llama_index.embeddings.google_genai import GoogleGenAIEmbedding as _EMB
            Settings.embed_model = _EMB(model_name=embed_model, api_key=api_key)
        # ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸
        _ = Settings.embed_model.get_text_embedding("ping")
    except Exception as e:
        st.error("ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨ â€” í‚¤/ëª¨ë¸/íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°", expanded=True):
            st.exception(e)
        st.stop()

def make_llm(provider: str, api_key: str, llm_model: str, temperature: float = 0.0):
    """ê³µê¸‰ìì˜ LLM ì¸ìŠ¤í„´ìŠ¤ë§Œ ë§Œë“¤ì–´ ë°˜í™˜(SettingsëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ)"""
    p = (provider or "google").lower()
    try:
        if p == "openai":
            from llama_index.llms.openai import OpenAI as _LLM
            return _LLM(model=llm_model, api_key=api_key, temperature=temperature)
        else:
            from llama_index.llms.google_genai import GoogleGenAI as _LLM
            return _LLM(model=llm_model, api_key=api_key, temperature=temperature)
    except Exception as e:
        st.error(f"{provider} LLM ì´ˆê¸°í™” ì‹¤íŒ¨ â€” í‚¤/ëª¨ë¸ì„ í™•ì¸í•˜ì„¸ìš”.")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°", expanded=True):
            st.exception(e)
        st.stop()

# =============================================================================
# 2) Drive í…ŒìŠ¤íŠ¸/ë¯¸ë¦¬ë³´ê¸°
# =============================================================================

def smoke_test_drive() -> tuple[bool, str]:
    folder_id = settings.GDRIVE_FOLDER_ID
    if not str(folder_id).strip():
        return (False, "GDRIVE_FOLDER_IDê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. Secretsì— ê°’ì„ ì¶”ê°€í•˜ì„¸ìš”.")

    sa = _normalize_sa(settings.GDRIVE_SERVICE_ACCOUNT_JSON)
    if not sa:
        return (False, "GOOGLE_SERVICE_ACCOUNT_JSON(ì„œë¹„ìŠ¤ê³„ì •) íŒŒì‹± ì‹¤íŒ¨")

    try:
        svc = _build_drive_service(sa)
        svc.files().get(fileId=folder_id, fields="id").execute()
        return (True, "Google Drive ì—°ê²° OK")
    except Exception as e:
        return (False, f"Drive ì—°ê²° ì ê²€ ì‹¤íŒ¨: {e}")

def preview_drive_files(max_items: int = 10) -> tuple[bool, str, list[dict]]:
    folder_id = settings.GDRIVE_FOLDER_ID
    sa = _normalize_sa(settings.GDRIVE_SERVICE_ACCOUNT_JSON)
    if not sa or not str(folder_id).strip():
        return (False, "ì„œë¹„ìŠ¤ê³„ì •/í´ë” ID ì„¤ì •ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.", [])

    try:
        svc = _build_drive_service(sa)
        q = f"'{folder_id}' in parents and trashed=false"
        fields = "files(id,name,mimeType,modifiedTime), nextPageToken"
        resp = svc.files().list(
            q=q,
            orderBy="modifiedTime desc",
            pageSize=max_items,
            fields=fields,
            supportsAllDrives=True,
            includeItemsFromAllDrives=True,
        ).execute()
        files = resp.get("files", [])
        rows = [{
            "name": f.get("name"),
            "link": f"https://drive.google.com/file/d/{f.get('id')}/view",
            "mime": f.get("mimeType"),
            "modified": f.get("modifiedTime"),
        } for f in files]
        return (True, f"{len(rows)}ê°œ íŒŒì¼", rows)
    except Exception as e:
        return (False, f"ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}", [])

# =============================================================================
# 3) ë§¤ë‹ˆí˜ìŠ¤íŠ¸/ì²´í¬í¬ì¸íŠ¸ + ì¸ë±ì‹±(Resume ì§€ì›) + Google Docs Export
# =============================================================================

def _fetch_drive_manifest(creds_dict, folder_id: str, is_cancelled: Callable[[], bool] | None = None) -> dict:
    svc = _build_drive_service(creds_dict)
    files = []
    page_token = None
    q = f"'{folder_id}' in parents and trashed=false"
    fields = "nextPageToken, files(id,name,mimeType,modifiedTime,md5Checksum,size)"
    while True:
        if is_cancelled and is_cancelled():
            raise CancelledError("ì‚¬ìš©ì ì·¨ì†Œ(ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì¡°íšŒ ì¤‘)")
        resp = svc.files().list(
            q=q, fields=fields, pageToken=page_token,
            pageSize=1000, supportsAllDrives=True, includeItemsFromAllDrives=True
        ).execute()
        files.extend(resp.get("files", []))
        page_token = resp.get("nextPageToken")
        if not page_token:
            break
    return {
        f["id"]: {
            "name": f.get("name"),
            "mimeType": f.get("mimeType"),
            "modifiedTime": f.get("modifiedTime"),
            "md5": f.get("md5Checksum"),
            "size": f.get("size"),
        } for f in files
    }

def _manifest_hash(m: dict) -> str:
    """ì›ê²© ìŠ¤ëƒ…ìƒ·ì„ í•´ì‹œí•˜ì—¬ ì²´í¬í¬ì¸íŠ¸ íƒ€ê¹ƒ IDë¡œ ì‚¬ìš©"""
    s = json.dumps(m, ensure_ascii=False, sort_keys=True)
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def _load_local_manifest(path: str) -> dict | None:
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as fp:
                return json.load(fp)
        except Exception:
            return None
    return None

def _save_local_manifest(path: str, m: dict) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as fp:
        json.dump(m, fp, ensure_ascii=False, indent=2, sort_keys=True)

def _manifests_differ(local: dict | None, remote: dict) -> bool:
    if local is None:
        return True
    if set(local.keys()) != set(remote.keys()):
        return True
    for fid, r in remote.items():
        l = local.get(fid, {})
        if l.get("md5") and r.get("md5"):
            if l["md5"] != r["md5"]:
                return True
        if l.get("modifiedTime") != r.get("modifiedTime"):
            return True
    return False

def _ckpt_path(persist_dir: str) -> str:
    return os.path.join(persist_dir, "build_checkpoint.json")

def _load_ckpt(persist_dir: str) -> dict | None:
    path = _ckpt_path(persist_dir)
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as fp:
                return json.load(fp)
        except Exception:
            return None
    return None

def _save_ckpt(persist_dir: str, data: dict) -> None:
    os.makedirs(persist_dir, exist_ok=True)
    with open(_ckpt_path(persist_dir), "w", encoding="utf-8") as fp:
        json.dump(data, fp, ensure_ascii=False, indent=2, sort_keys=True)

def _clear_ckpt(persist_dir: str) -> None:
    path = _ckpt_path(persist_dir)
    if os.path.exists(path):
        try:
            os.remove(path)
        except Exception:
            pass

@st.cache_resource(show_spinner=False)
def _load_index_from_disk(persist_dir: str):
    """ì €ì¥ëœ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œ. ì‹¤íŒ¨ ì‹œ ê¹¨ë—í•œ ì €ì¥ì†Œë¡œ ìë™ ì´ˆê¸°í™”."""
    try:
        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
        return load_index_from_storage(storage_context)
    except Exception:
        return _ensure_index_initialized(persist_dir)

def _ensure_index_initialized(persist_dir: str):
    """ë¹ˆ ì¸ë±ìŠ¤ë¥¼ ë©”ëª¨ë¦¬ì— ë§Œë“¤ê³ , ì§€ì • ê²½ë¡œë¡œ ìµœì´ˆ persist(í•„ìˆ˜ íŒŒì¼ ìƒì„±)."""
    os.makedirs(persist_dir, exist_ok=True)
    storage_context = StorageContext.from_defaults()
    index = VectorStoreIndex.from_documents([], storage_context=storage_context)
    index.storage_context.persist(persist_dir=persist_dir)
    return index

def _iter_drive_file_ids(manifest: dict) -> Iterable[str]:
    items = []
    for fid, meta in manifest.items():
        items.append((meta.get("modifiedTime") or "", fid))
    items.sort(reverse=True)
    for _, fid in items:
        yield fid

# === Google Docs/Sheets/Slides Export â†’ í…ìŠ¤íŠ¸/CSV =================================

_GOOGLE_APPS = "application/vnd.google-apps."

def _export_text_via_drive(svc, file_id: str, mime_type: str) -> tuple[str | None, str]:
    """
    Google Docs/Sheets/Slidesë¥¼ í…ìŠ¤íŠ¸/CSVë¡œ export í›„ ë¬¸ìì—´ ë°˜í™˜.
    ë¦¬í„´: (text_or_none, used_mime)
    """
    export_map = {
        _GOOGLE_APPS + "document": "text/plain",      # Docs â†’ txt
        _GOOGLE_APPS + "spreadsheet": "text/csv",     # Sheets â†’ csv
        _GOOGLE_APPS + "presentation": "text/plain",  # Slides â†’ txt (ê°€ëŠ¥í•œ ê²½ìš°)
    }
    target = export_map.get(mime_type)
    if not target:
        return (None, "")

    try:
        req = svc.files().export_media(fileId=file_id, mimeType=target)
        buf = io.BytesIO()
        downloader = MediaIoBaseDownload(buf, req)
        done = False
        while not done:
            _, done = downloader.next_chunk()
        content = buf.getvalue().decode("utf-8", errors="ignore")
        return (content, target)
    except HttpError as e:
        # Slidesì—ì„œ text/plain ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆìŒ â†’ íƒ€ê²Ÿì„ ì¡°ê¸ˆ ë°”ê¿” ì‹œë„ (ìµœí›„ ìˆ˜ë‹¨)
        if mime_type.endswith("presentation"):
            try:
                alt = "text/csv"
                req = svc.files().export_media(fileId=file_id, mimeType=alt)
                buf = io.BytesIO()
                downloader = MediaIoBaseDownload(buf, req)
                done = False
                while not done:
                    _, done = downloader.next_chunk()
                content = buf.getvalue().decode("utf-8", errors="ignore")
                return (content, alt)
            except Exception:
                pass
        # ì‹¤íŒ¨í•˜ë©´ None ë°˜í™˜
        return (None, "")
    except Exception:
        return (None, "")

def _build_or_resume_with_progress(update_pct: Callable[[int, str | None], None],
                                   update_msg: Callable[[str], None],
                                   gdrive_folder_id: str,
                                   gcp_creds: Mapping[str, Any],
                                   persist_dir: str,
                                   manifest: dict,
                                   max_docs: int | None = None,
                                   is_cancelled: Callable[[], bool] | None = None):
    """
    â–¶ Resume ì§€ì› ë¹Œë“œ
      - ì²´í¬í¬ì¸íŠ¸ì— ê¸°ë¡ëœ íŒŒì¼ì€ ê±´ë„ˆë›°ê³ , ë‚˜ë¨¸ì§€ íŒŒì¼ë§Œ ê³„ì† ì¸ë±ì‹±
      - ê° íŒŒì¼ ì²˜ë¦¬ í›„ persist + ì²´í¬í¬ì¸íŠ¸ ê°±ì‹ 
      - Google Docs/Sheets/SlidesëŠ” exportë¡œ í…ìŠ¤íŠ¸/CSV ì¶”ì¶œ
    """
    from llama_index.readers.google import GoogleDriveReader

    # 0) ë¦¬ë”/ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    update_pct(10, "Drive ë¦¬ë” ì´ˆê¸°í™”")
    try:
        try:
            loader = GoogleDriveReader(service_account_key=gcp_creds)   # ì‹ í˜•
        except TypeError:
            loader = GoogleDriveReader(gcp_creds_dict=gcp_creds)        # êµ¬í˜•
        svc = _build_drive_service(gcp_creds)
    except Exception as e:
        st.error("Google Drive ë¦¬ë” ì´ˆê¸°í™” ì‹¤íŒ¨")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°", expanded=True):
            st.exception(e)
        st.stop()

    # 1) ì¸ë±ìŠ¤ ì´ˆê¸°í™”/ë¡œë“œ (ì—†ìœ¼ë©´ ìƒì„± í›„ persist)
    try:
        if os.path.exists(persist_dir):
            index = _load_index_from_disk(persist_dir)
        else:
            index = _ensure_index_initialized(persist_dir)
    except Exception as e:
        st.error("ì¸ë±ìŠ¤ ì €ì¥ì†Œ ì´ˆê¸°í™” ì‹¤íŒ¨")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°", expanded=True):
            st.exception(e)
        st.stop()

    # 2) ì²´í¬í¬ì¸íŠ¸ ì¤€ë¹„
    target_hash = _manifest_hash(manifest)
    ckpt = _load_ckpt(persist_dir) or {}
    done_ids: set[str] = set(ckpt.get("done_ids", [])) if ckpt.get("target_hash") == target_hash else set()
    if ckpt.get("target_hash") != target_hash:
        ckpt = {"target_hash": target_hash, "done_ids": []}
        _save_ckpt(persist_dir, ckpt)

    # 3) ì²˜ë¦¬í•  ID ëª©ë¡
    all_ids = list(_iter_drive_file_ids(manifest))
    if max_docs:
        all_ids = all_ids[:max_docs]
    todo_ids = [fid for fid in all_ids if fid not in done_ids]
    total = len(all_ids)

    update_msg(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: ì™„ë£Œ {len(done_ids)}/{total}ê°œ â€” ì¬ê°œ ì¤€ë¹„")

    splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=128)

    # 4) ë¬¸ì„œë³„ ì²˜ë¦¬ ë£¨í”„
    for fid in todo_ids:
        if is_cancelled and is_cancelled():
            raise CancelledError("ì‚¬ìš©ì ì·¨ì†Œ(ë¬¸ì„œ ì²˜ë¦¬ ì¤‘)")

        meta = manifest.get(fid, {})
        fname = meta.get("name", fid)
        mime  = meta.get("mimeType", "")
        update_msg(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘: {fname}")

        docs: list[Document] = []

        # (A) Google Docs/Sheets/Slides â†’ export ë¡œ í…ìŠ¤íŠ¸/CSV ì¶”ì¶œ
        if mime.startswith(_GOOGLE_APPS):
            text, used = _export_text_via_drive(svc, fid, mime)
            if text:
                docs = [Document(text=text, metadata={"file_name": fname, "file_id": fid, "mimeType": mime, "exported_as": used})]
            else:
                st.warning(f"Export ì‹¤íŒ¨(ê±´ë„ˆëœ€): {fname} ({mime})")
        # (B) ê·¸ ì™¸ ë°”ì´ë„ˆë¦¬/ì¼ë°˜ íŒŒì¼ â†’ LlamaIndex ë¦¬ë”ë¡œ ì‹œë„
        else:
            try:
                docs = loader.load_data(file_ids=[fid])
            except Exception as e:
                st.warning(f"ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨(ê±´ë„ˆëœ€): {fname} â€” {e}")

        if not docs:
            continue

        # ë…¸ë“œí™” â†’ ì¸ë±ìŠ¤ ì‚½ì… â†’ persist â†’ ì²´í¬í¬ì¸íŠ¸ ê°±ì‹ 
        try:
            nodes = splitter.get_nodes_from_documents(docs)
            index.insert_nodes(nodes)
            index.storage_context.persist(persist_dir=persist_dir)  # ë¶€ë¶„ ì €ì¥
            done_ids.add(fid)
            _save_ckpt(persist_dir, {"target_hash": target_hash, "done_ids": sorted(done_ids)})
        except Exception as e:
            st.warning(f"ì¸ë±ì‹± ì‹¤íŒ¨({fname}): {e}")
            continue

        # ì§„í–‰ë¥ (30~90 êµ¬ê°„ ë§¤í•‘)
        cur_done = len(done_ids)
        pct = 30 + int(60 * (cur_done / total)) if total > 0 else 90
        update_pct(pct, f"ì§„í–‰ {cur_done}/{total} â€” {fname}")

    # 5) ì™„ë£Œ ì •ë¦¬
    update_pct(95, "ì •ë¦¬/ê²€ì¦â€¦")
    try:
        index.storage_context.persist(persist_dir=persist_dir)
    except Exception as e:
        st.warning(f"ìµœì¢… ì €ì¥ ê²½ê³ : {e}")

    update_pct(100, "ì™„ë£Œ")
    return index

def get_or_build_index(update_pct: Callable[[int, str | None], None],
                       update_msg: Callable[[str], None],
                       gdrive_folder_id: str,
                       raw_sa: Any | None,
                       persist_dir: str,
                       manifest_path: str,
                       max_docs: int | None = None,
                       is_cancelled: Callable[[], bool] | None = None):
    """ë³€ê²½ ì—†ìœ¼ë©´ ë¡œë”©, ë³€ê²½ ìˆìœ¼ë©´ â–¶ 'Resume ì²´í¬í¬ì¸íŠ¸' ìš°ì„  ì‹œë„ â†’ ì—†ìœ¼ë©´ ìƒˆ ë¹Œë“œ"""
    gcp_creds = _normalize_sa(raw_sa)
    if not gcp_creds:
        st.error("ì„œë¹„ìŠ¤ê³„ì • JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        st.stop()

    update_pct(5, "ë“œë¼ì´ë¸Œ ë³€ê²½ í™•ì¸ ì¤‘â€¦")
    remote = _fetch_drive_manifest(gcp_creds, gdrive_folder_id, is_cancelled=is_cancelled)
    local = _load_local_manifest(manifest_path)
    target_hash = _manifest_hash(remote)

    # 1) ë³€ê²½ ì—†ìœ¼ë©´ ë¹ ë¥¸ ë¡œë”©
    if os.path.exists(persist_dir) and not _manifests_differ(local, remote):
        update_pct(25, "ë³€ê²½ ì—†ìŒ â†’ ì €ì¥ëœ ë‘ë‡Œ ë¡œë”©")
        idx = _load_index_from_disk(persist_dir)
        update_pct(100, "ì™„ë£Œ!")
        return idx

    # 2) ë³€ê²½ì´ ìˆì§€ë§Œ, ê°™ì€ ìŠ¤ëƒ…ìƒ·(target_hash)ìœ¼ë¡œ ì§„í–‰ ì¤‘ ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ 'ì¬ê°œ'
    ckpt = _load_ckpt(persist_dir)
    if ckpt and ckpt.get("target_hash") == target_hash and os.path.exists(persist_dir):
        update_pct(20, "ë³€ê²½ ê°ì§€ â†’ ë¯¸ì™„ë£Œ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬ â†’ ì¬ê°œ")
        idx = _build_or_resume_with_progress(
            update_pct, update_msg, gdrive_folder_id, gcp_creds, persist_dir, remote,
            max_docs=max_docs, is_cancelled=is_cancelled
        )
        _save_local_manifest(manifest_path, remote)
        update_pct(100, "ì™„ë£Œ!")
        return idx

    # 3) ìƒˆë¡œ ë¹Œë“œ(ê¸°ì¡´ ì €ì¥ì†Œ/ì²´í¬í¬ì¸íŠ¸ ì œê±°)
    if os.path.exists(persist_dir):
        shutil.rmtree(persist_dir)
    _clear_ckpt(persist_dir)

    update_pct(25, "ë³€ê²½ ê°ì§€ â†’ ìƒˆ ì¸ë±ìŠ¤ ìƒì„±")
    idx = _build_or_resume_with_progress(
        update_pct, update_msg, gdrive_folder_id, gcp_creds, persist_dir, remote,
        max_docs=max_docs, is_cancelled=is_cancelled
    )

    _save_local_manifest(manifest_path, remote)
    update_pct(100, "ì™„ë£Œ!")
    return idx

# =============================================================================
# 4) QA ìœ í‹¸
# =============================================================================

def get_text_answer(query_engine, question: str, system_prompt: str) -> str:
    """ì„ íƒëœ í˜ë¥´ì†Œë‚˜ ì§€ì¹¨ + ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í•©ì³ ì¿¼ë¦¬í•˜ê³ , ì¶œì²˜ íŒŒì¼ëª…ì„ í•¨ê»˜ ë°˜í™˜."""
    try:
        full_query = (
            f"{system_prompt}\n\n"
            "[ì§€ì‹œì‚¬í•­] ë°˜ë“œì‹œ ì—…ë¡œë“œëœ ê°•ì˜ ìë£Œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ê³ , "
            "ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…í™•íˆ ë°í˜€ë¼.\n\n"
            f"[í•™ìƒì˜ ì§ˆë¬¸]\n{question}"
        )
        response = query_engine.query(full_query)
        answer_text = str(response)

        # [ê°œì„ ] ë‹¤ì–‘í•œ ë©”íƒ€ í‚¤ + Drive ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ í™œìš©í•´ íŒŒì¼ëª… ë³µêµ¬
        nodes = getattr(response, "source_nodes", []) or []
        source_files = _source_names_from_nodes(nodes)

        return f"{answer_text}\n\n---\n*ì°¸ê³  ìë£Œ: {source_files}*"

    except Exception as e:
        return f"í…ìŠ¤íŠ¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# === LLM ë¬´ê²€ìƒ‰(ì§ì ‘ ì™„ì„±) ìœ í‹¸ ======================================
def llm_complete(llm, prompt: str, temperature: float = 0.0) -> str:
    """
    RAG ê²€ìƒ‰ ì—†ì´ ìˆœìˆ˜ LLMìœ¼ë¡œë§Œ ê²°ê³¼ë¥¼ ìƒì„±.
    llama-index LLM ë˜í¼ í˜¸í™˜ (complete().text ë˜ëŠ” predict())
    """
    try:
        resp = llm.complete(prompt)
        # CompletionResponse(text=...) í˜•íƒœ
        return getattr(resp, "text", str(resp))
    except AttributeError:
        # ì¼ë¶€ êµ¬í˜„ì²´ëŠ” predictë§Œ ì œê³µí•  ìˆ˜ ìˆìŒ
        return llm.predict(prompt)
