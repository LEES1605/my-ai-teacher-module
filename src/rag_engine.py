# src/rag_engine.py
from __future__ import annotations
import os, json, shutil, io, zipfile, hashlib, re
from typing import Callable, Any, Mapping, List, Tuple, Dict
from datetime import datetime
try:
    from zoneinfo import ZoneInfo  # py>=3.9
except Exception:
    ZoneInfo = None  # í´ë°±

import streamlit as st
from src.config import settings, APP_DATA_DIR, QUALITY_REPORT_PATH

# ====== ë°±ì—… íŒŒì¼ëª… ê·œì¹™(ë‚ ì§œ í¬í•¨) ===========================================
INDEX_BACKUP_PREFIX = "ai_brain_cache"  # ê²°ê³¼: ai_brain_cache-YYYYMMDD-HHMMSS.zip

def _now_kst_str() -> str:
    try:
        if ZoneInfo:
            return datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y%m%d-%H%M%S")
    except Exception:
        pass
    return datetime.utcnow().strftime("%Y%m%d-%H%M%S")

def _build_backup_filename(prefix: str = INDEX_BACKUP_PREFIX) -> str:
    return f"{prefix}-{_now_kst_str()}.zip"

# ====== ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ========================================================
CHECKPOINT_PATH = str((APP_DATA_DIR / "index_checkpoint.json").resolve())

# ====== LLM/Embedding ì„¤ì • ====================================================
def init_llama_settings(api_key: str, llm_model: str, embed_model: str, temperature: float = 0.0):
    from llama_index.core import Settings
    from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
    from llama_index.llms.google_genai import GoogleGenAI
    Settings.llm = GoogleGenAI(model=llm_model, api_key=api_key, temperature=temperature)
    Settings.embed_model = GoogleGenAIEmbedding(model_name=embed_model, api_key=api_key)
    try:
        _ = Settings.embed_model.get_text_embedding("ping")
    except Exception as e:
        st.error("ì„ë² ë”© ëª¨ë¸ ì ê²€ ì‹¤íŒ¨ â€” API í‚¤/ëª¨ë¸ëª…/ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°", expanded=True):
            st.exception(e)
        st.stop()

# ====== ë¡œì»¬ ì €ì¥ë³¸ ë¡œë”© =======================================================
@st.cache_resource(show_spinner=False)
def _load_index_from_disk(persist_dir: str):
    from llama_index.core import StorageContext, load_index_from_storage
    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
    index = load_index_from_storage(storage_context)
    return index

# ====== Google Drive helpers ==================================================
def _build_drive_service(creds_dict: Mapping[str, Any], write: bool = False):
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    scopes = ["https://www.googleapis.com/auth/drive.readonly"]
    if write:
        scopes = ["https://www.googleapis.com/auth/drive"]
    creds = service_account.Credentials.from_service_account_info(dict(creds_dict), scopes=scopes)
    return build("drive", "v3", credentials=creds, cache_discovery=False)

def _fetch_drive_manifest(creds_dict: Mapping[str, Any], folder_id: str) -> dict:
    svc = _build_drive_service(creds_dict, write=False)
    files = []
    page_token = None
    q = f"'{folder_id}' in parents and trashed=false"
    fields = "nextPageToken, files(id,name,mimeType,modifiedTime,md5Checksum,size)"
    while True:
        resp = svc.files().list(
            q=q, fields=fields, pageToken=page_token,
            pageSize=1000, supportsAllDrives=True, includeItemsFromAllDrives=True
        ).execute()
        files.extend(resp.get("files", []))
        page_token = resp.get("nextPageToken")
        if not page_token:
            break
    return {
        f["id"]: {
            "name": f.get("name"),
            "mimeType": f.get("mimeType"),
            "modifiedTime": f.get("modifiedTime"),
            "md5": f.get("md5Checksum"),
            "size": f.get("size"),
        } for f in files
    }

# ====== ì„œë¹„ìŠ¤ê³„ì • ì •ê·œí™”/ê²€ì¦ =================================================
def _normalize_sa(raw_sa: Any | None) -> Mapping[str, Any] | None:
    if raw_sa is None:
        return None
    if isinstance(raw_sa, Mapping):
        d = dict(raw_sa)
    elif isinstance(raw_sa, str) and raw_sa.strip():
        try:
            d = json.loads(raw_sa)
        except Exception:
            return None
    else:
        return None
    for k in ("service_account", "serviceAccount"):
        if isinstance(d.get(k), Mapping):
            d = dict(d[k])
    pk = d.get("private_key")
    if isinstance(pk, str) and "\\n" in pk and "\n" not in pk:
        d["private_key"] = pk.replace("\\n", "\n")
    return d

def _validate_sa(creds: Mapping[str, Any] | None) -> Mapping[str, Any]:
    if not creds:
        st.error("GDRIVE ì„œë¹„ìŠ¤ê³„ì • ìê²©ì¦ëª…ì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. secretsì— JSONì„ ë„£ì–´ì£¼ì„¸ìš”.")
        st.stop()
    required = {"type","project_id","private_key_id","private_key","client_email","client_id","token_uri"}
    missing = [k for k in required if k not in creds]
    if missing:
        st.error("ì„œë¹„ìŠ¤ê³„ì • JSONì— í•„ìˆ˜ í‚¤ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: " + ", ".join(missing))
        with st.expander("ì§„ë‹¨ ì •ë³´(ë³´ìœ  í‚¤ ëª©ë¡)"):
            st.write(sorted(list(creds.keys())))
        st.stop()
    return creds

# ====== ZIP ë°±ì—…/ë³µì› + ë³´ê´€ì •ì±… =============================================
def _zip_dir(src_dir: str, zip_path: str) -> None:
    os.makedirs(os.path.dirname(zip_path), exist_ok=True)
    with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for root, _, files in os.walk(src_dir):
            for name in files:
                full = os.path.join(root, name)
                rel = os.path.relpath(full, src_dir)
                zf.write(full, rel)

def _list_backups(svc, folder_id: str, prefix: str = INDEX_BACKUP_PREFIX) -> List[dict]:
    q = (
        f"'{folder_id}' in parents and trashed=false and "
        f"mimeType='application/zip' and name contains '{prefix}-'"
    )
    resp = svc.files().list(
        q=q, orderBy="modifiedTime desc", pageSize=100,
        fields="files(id,name,modifiedTime,size,md5Checksum)"
    ).execute()
    return resp.get("files", [])

def prune_old_backups(creds: Mapping[str, Any], folder_id: str, keep: int = 5,
                      prefix: str = INDEX_BACKUP_PREFIX) -> List[Tuple[str, str]]:
    if keep <= 0:
        keep = 1
    svc = _build_drive_service(creds, write=True)
    items = _list_backups(svc, folder_id, prefix)
    to_delete = items[keep:] if len(items) > keep else []
    deleted: List[Tuple[str, str]] = []
    for it in to_delete:
        try:
            svc.files().delete(fileId=it["id"]).execute()
            deleted.append((it["id"], it.get("name", "")))
        except Exception:
            pass
    return deleted

def export_brain_to_drive(creds: Mapping[str, Any], persist_dir: str, dest_folder_id: str,
                          filename: str | None = None) -> Tuple[str, str]:
    if not os.path.exists(persist_dir):
        raise FileNotFoundError("persist_dirê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë‘ë‡Œë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    fname = filename or _build_backup_filename()
    tmp_zip = os.path.join("/tmp", fname)
    _zip_dir(persist_dir, tmp_zip)

    svc = _build_drive_service(creds, write=True)
    from googleapiclient.http import MediaFileUpload
    meta = {"name": fname, "parents": [dest_folder_id]}
    media = MediaFileUpload(tmp_zip, mimetype="application/zip", resumable=True)
    created = svc.files().create(body=meta, media_body=media, fields="id,name,parents").execute()
    return created["id"], created["name"]

def import_brain_from_drive(creds: Mapping[str, Any], persist_dir: str, src_folder_id: str,
                            prefix: str = INDEX_BACKUP_PREFIX) -> bool:
    svc = _build_drive_service(creds, write=True)
    items = _list_backups(svc, src_folder_id, prefix)
    if not items:
        return False
    file_id = items[0]["id"]
    from googleapiclient.http import MediaIoBaseDownload
    req = svc.files().get_media(fileId=file_id)
    tmp_zip = os.path.join("/tmp", items[0]["name"])
    with io.FileIO(tmp_zip, "wb") as fh:
        downloader = MediaIoBaseDownload(fh, req)
        done = False
        while not done:
            status, done = downloader.next_chunk()

    if os.path.exists(persist_dir):
        shutil.rmtree(persist_dir)
    os.makedirs(persist_dir, exist_ok=True)
    with zipfile.ZipFile(tmp_zip, "r") as zf:
        zf.extractall(persist_dir)
    return True

def try_restore_index_from_drive(creds: Mapping[str, Any], persist_dir: str, folder_id: str) -> bool:
    if os.path.exists(persist_dir):
        return True
    try:
        return import_brain_from_drive(creds, persist_dir, folder_id, INDEX_BACKUP_PREFIX)
    except Exception:
        return False

# ====== ì²´í¬í¬ì¸íŠ¸ ìœ í‹¸ =======================================================
def _load_checkpoint(path: str = CHECKPOINT_PATH) -> Dict[str, Dict]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _save_checkpoint(data: Dict[str, Dict], path: str = CHECKPOINT_PATH) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, sort_keys=True)

def _mark_done(cp: Dict[str, Dict], file_id: str, entry: Dict) -> None:
    cp[file_id] = {
        "md5": entry.get("md5"),
        "modifiedTime": entry.get("modifiedTime"),
        "name": entry.get("name"),
        "ts": _now_kst_str(),
    }
    _save_checkpoint(cp)

def clear_checkpoint(path: str = CHECKPOINT_PATH) -> None:
    try:
        if os.path.exists(path):
            os.remove(path)
    except Exception:
        pass

# ====== ì €ì¥ë³¸ ì¡´ì¬ ì²´í¬ & ì•ˆì „í•œ StorageContext ìƒì„± =========================
def _has_persisted_index(persist_dir: str) -> bool:
    names = ("docstore.json", "index_store.json", "vector_store.json")
    return any(os.path.exists(os.path.join(persist_dir, n)) for n in names)

def _make_storage_context(persist_dir: str):
    from llama_index.core import StorageContext
    try:
        if _has_persisted_index(persist_dir):
            return StorageContext.from_defaults(persist_dir=persist_dir)
    except Exception:
        pass
    return StorageContext.from_defaults()

# ====== í’ˆì§ˆ ìœ í‹¸(ì „ì²˜ë¦¬Â·ì¤‘ë³µÂ·ë¦¬í¬íŠ¸) =========================================
_ws_re = re.compile(r"[ \t\f\v]+")
def _clean_text(s: str) -> str:
    s = s.replace("\u00a0", " ").replace("\r", "\n")
    s = re.sub(r"\n{3,}", "\n\n", s)
    s = _ws_re.sub(" ", s)
    return s.strip()

def _sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()

def _get_opt() -> Dict[str, Any]:
    return {
        "chunk_size": int(st.session_state.get("opt_chunk_size", settings.CHUNK_SIZE)),
        "chunk_overlap": int(st.session_state.get("opt_chunk_overlap", settings.CHUNK_OVERLAP)),
        "min_chars": int(st.session_state.get("opt_min_chars", settings.MIN_CHARS_PER_DOC)),
        "dedup": bool(st.session_state.get("opt_dedup", settings.DEDUP_BY_TEXT_HASH)),
        "skip_low_text": bool(st.session_state.get("opt_skip_low_text", settings.SKIP_LOW_TEXT_DOCS)),
        "pre_summarize": bool(st.session_state.get("opt_pre_summarize", settings.PRE_SUMMARIZE_DOCS)),
    }

def _maybe_summarize_docs(docs: List[Any]) -> None:
    if not docs or not _get_opt()["pre_summarize"]:
        return
    try:
        from llama_index.core import Settings
        for d in docs:
            if "doc_summary" in getattr(d, "metadata", {}):
                continue
            text = (getattr(d, "text", "") or "")[:4000]
            if not text:
                continue
            prompt = (
                "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ êµì‚¬ ì‹œê°ì—ì„œ 5ì¤„ ì´ë‚´ í•µì‹¬ bulletë¡œ ìš”ì•½í•˜ë¼.\n"
                "êµì¬ ë‹¨ì›/ê°œë…/ì˜ˆë¬¸/í•µì‹¬ ê·œì¹™ì„ ê°„ë‹¨íˆ í‘œì‹œí•˜ë¼.\n\n"
                f"[ë¬¸ì„œ ë‚´ìš©]\n{text}"
            )
            try:
                resp = Settings.llm.complete(prompt)
                summary = getattr(resp, "text", None) or str(resp)
                d.metadata["doc_summary"] = summary.strip()
            except Exception:
                pass
    except Exception:
        pass

def _preprocess_docs(docs: List[Any], seen_hashes: set, min_chars: int, dedup: bool) -> Tuple[List[Any], Dict[str, Any]]:
    kept: List[Any] = []
    stats = {"input_docs": len(docs), "kept": 0, "skipped_low_text": 0, "skipped_dup": 0, "total_chars": 0}
    for d in docs:
        t = _clean_text(getattr(d, "text", "") or "")
        if len(t) < min_chars:
            stats["skipped_low_text"] += 1
            continue
        h = _sha1(t)
        if dedup and h in seen_hashes:
            stats["skipped_dup"] += 1
            continue
        d.text = t
        d.metadata = dict(getattr(d, "metadata", {}) or {})
        d.metadata["text_hash"] = h
        kept.append(d)
        seen_hashes.add(h)
        stats["kept"] += 1
        stats["total_chars"] += len(t)
    return kept, stats

def _load_quality_report(path: str = QUALITY_REPORT_PATH) -> Dict[str, Any]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {"summary": {}, "files": {}}

def _save_quality_report(data: Dict[str, Any], path: str = QUALITY_REPORT_PATH) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2, sort_keys=True)

# ====== ì¸ë±ìŠ¤ ìƒì„±(ì²´í¬í¬ì¸íŠ¸ + ìµœì í™” + ì¤‘ì§€ ì§€ì›) ==========================
def _build_index_with_checkpoint(update_pct: Callable[[int, str | None], None],
                                 update_msg: Callable[[str], None],
                                 gdrive_folder_id: str,
                                 gcp_creds: Mapping[str, Any],
                                 persist_dir: str,
                                 remote_manifest: Dict[str, Dict],
                                 should_stop: Callable[[], bool] | None = None):
    """
    íŒŒì¼ID ë‹¨ìœ„ ì²˜ë¦¬ â†’ ê° íŒŒì¼ ì™„ì£¼ í›„ ì €ì¥ & ì²´í¬í¬ì¸íŠ¸ ê¸°ë¡.
    ì „ì²˜ë¦¬(ì •ë¦¬/ì €í’ˆì§ˆ í•„í„°/ì¤‘ë³µ ì œê±°) â†’ SentenceSplitter ì²­í‚¹ â†’ ì¸ë±ìŠ¤ ëˆ„ì .
    ì¤‘ì§€ ë²„íŠ¼(should_stop=True) ê°ì§€ ì‹œ 'í˜„ì¬ íŒŒì¼ê¹Œì§€' ì €ì¥ í›„ ì•ˆì „ ì¢…ë£Œ.
    """
    from llama_index.core import VectorStoreIndex, load_index_from_storage
    from llama_index.core.node_parser import SentenceSplitter
    from llama_index.readers.google import GoogleDriveReader

    if should_stop is None:
        should_stop = lambda: False  # ê¸°ë³¸: ì¤‘ì§€ ì—†ìŒ

    opt = _get_opt()

    update_pct(15, "Drive ë¦¬ë” ì´ˆê¸°í™”")
    loader = GoogleDriveReader(service_account_key=gcp_creds)

    cp = _load_checkpoint()
    todo_ids: List[str] = []
    for fid, meta in remote_manifest.items():
        done = cp.get(fid)
        if done and done.get("md5") and meta.get("md5") and done["md5"] == meta["md5"]:
            continue
        todo_ids.append(fid)

    total = len(remote_manifest)
    pending = len(todo_ids)
    done_cnt = total - pending
    update_pct(30, f"ë¬¸ì„œ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ â€¢ ì „ì²´ {total}ê°œ, ì´ë²ˆì— ì²˜ë¦¬ {pending}ê°œ")

    os.makedirs(persist_dir, exist_ok=True)
    storage_context = _make_storage_context(persist_dir)
    try:
        _ = load_index_from_storage(storage_context)
    except Exception:
        pass

    # í’ˆì§ˆ ë¦¬í¬íŠ¸
    qrep = _load_quality_report()
    qrep.setdefault("summary", {}).setdefault("total_docs", total)
    for k in ("processed_docs","kept_docs","skipped_low_text","skipped_dup","total_chars"):
        qrep["summary"].setdefault(k, 0)
    qrep.setdefault("files", {})
    seen_hashes = set(h for h in qrep.get("files", {}).values() if isinstance(h, dict) and "text_hash" in h)

    if pending == 0:
        update_pct(95, "ë³€ê²½ ì—†ìŒ â†’ ì €ì¥ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©")
        try:
            return load_index_from_storage(storage_context)
        except Exception:
            st.error("ì €ì¥ëœ ë‘ë‡Œê°€ ì—†ëŠ”ë° ë³€ê²½ë„ ì—†ë‹¤ê³  ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ˆê¸°í™” í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            st.stop()

    splitter = SentenceSplitter(chunk_size=opt["chunk_size"], chunk_overlap=opt["chunk_overlap"])

    for i, fid in enumerate(todo_ids, start=1):
        # ì¤‘ì§€ ìš”ì²­ì´ 'ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°€ê¸° ì „'ì— ë“¤ì–´ì˜¨ ê²½ìš°: ë°”ë¡œ ì¢…ë£Œ
        if should_stop():
            update_msg("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ â€” í˜„ì¬ê¹Œì§€ ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        meta = remote_manifest.get(fid, {})
        fname = meta.get("name") or fid
        update_msg(f"ì „ì²˜ë¦¬ â€¢ {fname} ({done_cnt + i}/{total})")

        # 1) íŒŒì¼ ë¡œë“œ
        try:
            docs = loader.load_data(file_ids=[fid])
        except TypeError:
            st.error("GoogleDriveReader ë²„ì „ì´ ì˜¤ë˜ë˜ì–´ file_ids ì˜µì…˜ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. requirements ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            st.stop()

        # 2) ì „ì²˜ë¦¬/í•„í„°/ì¤‘ë³µ
        kept, stats = _preprocess_docs(
            docs, seen_hashes,
            min_chars=opt["min_chars"], dedup=opt["dedup"]
        )
        _maybe_summarize_docs(kept)

        # 3) í’ˆì§ˆ ë¦¬í¬íŠ¸ ê¸°ë¡(íŒŒì¼ ë‹¨ìœ„)
        qrep["files"][fid] = {
            "name": fname,
            "md5": meta.get("md5"),
            "modifiedTime": meta.get("modifiedTime"),
            "kept": stats["kept"],
            "skipped_low_text": stats["skipped_low_text"],
            "skipped_dup": stats["skipped_dup"],
            "total_chars": stats["total_chars"],
        }
        qs = qrep["summary"]
        qs["processed_docs"] += 1
        qs["kept_docs"] += stats["kept"]
        qs["skipped_low_text"] += stats["skipped_low_text"]
        qs["skipped_dup"] += stats["skipped_dup"]
        qs["total_chars"] += stats["total_chars"]
        _save_quality_report(qrep)

        if stats["kept"] == 0:
            # í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ì¤‘ë³µë§Œ â†’ ì™„ë£Œ ì²´í¬ë§Œ í•˜ê³  ë‹¤ìŒ íŒŒì¼
            _mark_done(cp, fid, meta)
            pct = 30 + int((i / max(1, pending)) * 60)
            update_pct(pct, f"ê±´ë„ˆëœ€ â€¢ {fname} (ì €í’ˆì§ˆ/ì¤‘ë³µ)")
            # ì¤‘ì§€ ìš”ì²­ì´ ì´ ì‹œì ì— ë“¤ì–´ì™”ìœ¼ë©´ ì—¬ê¸°ì„œ ì¢…ë£Œ
            if should_stop():
                update_msg("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ â€” í˜„ì¬ê¹Œì§€ ì €ì¥ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            continue

        # 4) ì¸ë±ìŠ¤ì— ëˆ„ì  ì¶”ê°€(ì²­í‚¹ ì ìš©)
        update_msg(f"ì¸ë±ìŠ¤ ìƒì„± â€¢ {fname} ({done_cnt + i}/{total})")
        try:
            VectorStoreIndex.from_documents(
                kept, storage_context=storage_context, show_progress=False,
                transformations=[splitter]
            )
            storage_context.persist(persist_dir=persist_dir)  # ë¶€ë¶„ ì§„í–‰ ì €ì¥
            _mark_done(cp, fid, meta)  # íŒŒì¼ 'ì™„ë£Œ'ë¡œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë¡
        except Exception as e:
            st.error(f"ì¸ë±ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {fname}")
            with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°"):
                st.exception(e)
            st.stop()

        pct = 30 + int((i / max(1, pending)) * 60)
        update_pct(pct, f"ì™„ë£Œ â€¢ {fname}")

        # 5) íŒŒì¼ ê²½ê³„ì—ì„œ ì¤‘ì§€ ìš”ì²­ í™•ì¸ â†’ ì•ˆì „ ì¢…ë£Œ
        if should_stop():
            update_msg("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ê°ì§€ â€” í˜„ì¬ íŒŒì¼ê¹Œì§€ ì €ì¥ ì™„ë£Œ, ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

    # ìµœì¢… ì €ì¥ ë° ì¸ë±ìŠ¤ ë°˜í™˜(ë¶€ë¶„ ì§„í–‰ì´ì–´ë„ ì•ˆì „)
    update_pct(95, "ë‘ë‡Œ ì €ì¥ ì¤‘")
    try:
        storage_context.persist(persist_dir=persist_dir)
    except Exception as e:
        st.error("ì¸ë±ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        with st.expander("ìì„¸í•œ ì˜¤ë¥˜ ë³´ê¸°", expanded=True):
            st.exception(e)
        st.stop()

    update_pct(100, "ì™„ë£Œ")
    from llama_index.core import load_index_from_storage
    return load_index_from_storage(storage_context)

# ====== ë³€ê²½ ê°ì§€ â†’ ì¸ë±ìŠ¤ ì¤€ë¹„(ì²´í¬í¬ì¸íŠ¸ í¬í•¨) ==============================
def get_or_build_index(update_pct: Callable[[int, str | None], None],
                       update_msg: Callable[[str], None],
                       gdrive_folder_id: str,
                       raw_sa: Any | None,
                       persist_dir: str,
                       manifest_path: str,
                       should_stop: Callable[[], bool] | None = None):
    """Drive ë³€ê²½ì„ ê°ì§€í•´ ì €ì¥ë³¸ì„ ì“°ê±°ë‚˜, ë³€ê²½ ì‹œì—ë§Œ ì¬ì¸ë±ì‹±(ì²´í¬í¬ì¸íŠ¸ & ì¤‘ì§€ ì§€ì›)."""
    update_pct(5, "ë“œë¼ì´ë¸Œ ë³€ê²½ í™•ì¸ ì¤‘â€¦")
    gcp_creds = _validate_sa(_normalize_sa(raw_sa))

    remote = _fetch_drive_manifest(gcp_creds, gdrive_folder_id)

    local = None
    if os.path.exists(manifest_path):
        try:
            with open(manifest_path, "r", encoding="utf-8") as fp:
                local = json.load(fp)
        except Exception:
            local = None

    def _manifests_differ(local_m: dict | None, remote_m: dict) -> bool:
        if local_m is None:
            return True
        if set(local_m.keys()) != set(remote_m.keys()):
            return True
        for fid, r in remote_m.items():
            l = local_m.get(fid, {})
            if l.get("md5") and r.get("md5"):
                if l["md5"] != r["md5"]:
                    return True
            if l.get("modifiedTime") != r.get("modifiedTime"):
                return True
        return False

    if os.path.exists(persist_dir) and not _manifests_differ(local, remote):
        update_pct(25, "ë³€ê²½ ì—†ìŒ â†’ ì €ì¥ëœ ë‘ë‡Œ ë¡œë”©")
        from llama_index.core import StorageContext, load_index_from_storage
        storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
        idx = load_index_from_storage(storage_context)
        update_pct(100, "ì™„ë£Œ!")
        return idx

    update_pct(40, "ë³€ê²½ ê°ì§€ â†’ ì „ì²˜ë¦¬/ì²­í‚¹/ì¸ë±ìŠ¤ ìƒì„± (ì²´í¬í¬ì¸íŠ¸)")
    idx = _build_index_with_checkpoint(
        update_pct, update_msg, gdrive_folder_id, gcp_creds, persist_dir, remote,
        should_stop=should_stop
    )

    # ìƒˆ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥ & ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬(ì™„ì£¼í–ˆì„ ë•Œë§Œ)
    if not (should_stop and should_stop()):
        os.makedirs(os.path.dirname(manifest_path), exist_ok=True)
        with open(manifest_path, "w", encoding="utf-8") as fp:
            json.dump(remote, fp, ensure_ascii=False, indent=2, sort_keys=True)
        # ëª¨ë“  íŒŒì¼ ì™„ë£Œí–ˆì„ ë•Œë§Œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¹„ì›€
        # (ì¤‘ì§€í•œ ê²½ìš°ì—ëŠ” ë‚¨ê²¨ë‘ì–´ ì¬ê°œ ì§€ì ìœ¼ë¡œ ì‚¬ìš©)
        if os.path.exists(CHECKPOINT_PATH):
            try:
                with open(CHECKPOINT_PATH, "r", encoding="utf-8") as f:
                    cp = json.load(f)
                # ëª¨ë‘ ì™„ë£Œì¸ì§€ ë¹ ë¥´ê²Œ í™•ì¸
                all_done = set(cp.keys()) == set(remote.keys())
                if all_done:
                    os.remove(CHECKPOINT_PATH)
            except Exception:
                pass

    update_pct(100, "ì™„ë£Œ!")
    return idx

# ====== QA ìœ í‹¸ ===============================================================
def get_text_answer(query_engine, question: str, system_prompt: str) -> str:
    try:
        full_query = (
            f"{system_prompt}\n\n"
            "[ì§€ì‹œì‚¬í•­] ë°˜ë“œì‹œ ì—…ë¡œë“œëœ ê°•ì˜ ìë£Œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ê³ , "
            "ê·¼ê±°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…í™•íˆ ë°í˜€ë¼.\n\n"
            f"[í•™ìƒì˜ ì§ˆë¬¸]\n{question}"
        )
        response = query_engine.query(full_query)
        answer_text = str(response)
        try:
            files = [n.metadata.get("file_name", "ì•Œ ìˆ˜ ì—†ìŒ") for n in getattr(response, "source_nodes", [])]
            source_files = ", ".join(sorted(list(set(files)))) if files else "ì¶œì²˜ ì •ë³´ ì—†ìŒ"
        except Exception:
            source_files = "ì¶œì²˜ ì •ë³´ ì—†ìŒ"
        return f"{answer_text}\n\n---\n*ì°¸ê³  ìë£Œ: {source_files}*"
    except Exception as e:
        return f"í…ìŠ¤íŠ¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
