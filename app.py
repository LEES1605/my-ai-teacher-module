# app.py â€” ê·¸ë£¹í† ë¡  + ì¦ë¶„ ì¸ë±ì‹±(Resume) + ì¤€ë¹„ ì·¨ì†Œ/ì¬ê°œ + ë¡œê·¸ ìƒíƒœ ê³ ì •í‘œì‹œ
import os, time, uuid, re, json
import pandas as pd
import streamlit as st

# ê¸°ë³¸ UI
from src.ui import load_css, render_header
# Drive ë¡œê·¸ ìœ í‹¸
from src.drive_log import save_chatlog_markdown, get_chatlog_folder_id
# í”„ë¡¬í”„íŠ¸/í˜ë¥´ì†Œë‚˜
from src.prompts import (
    EXPLAINER_PROMPT, ANALYST_PROMPT, READER_PROMPT,
    GEMINI_TONE, CHATGPT_TONE, REVIEW_DIRECTIVE,
)
# ì„¤ì •
from src.config import settings
# JSONL ì €ì¥ ëª¨ë“ˆ
from src import chat_store

# ========== ëŸ°íƒ€ì„ ì•ˆì •í™” ==========
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["STREAMLIT_SERVER_ENABLE_WEBSOCKET_COMPRESSION"] = "false"

# ========== í˜ì´ì§€ ==========
st.set_page_config(page_title="ë‚˜ì˜ AI ì˜ì–´ êµì‚¬", layout="wide", initial_sidebar_state="collapsed")
load_css(); render_header()

# ========== ì„¸ì…˜ ==========
ss = st.session_state
ss.setdefault("session_id", uuid.uuid4().hex[:12])
ss.setdefault("messages", [])
ss.setdefault("auto_save_chatlog", True)
ss.setdefault("save_logs", True)
ss.setdefault("prep_both_running", False)
ss.setdefault("prep_both_done", ("qe_google" in ss) or ("qe_openai" in ss))
ss.setdefault("p_shared", 0)
ss.setdefault("prep_cancel_requested", False)
ss.setdefault("session_terminated", False)
# ë¡œê·¸ ìƒíƒœ(ì§€ì† í‘œì‹œ)
ss.setdefault("log_status", None)   # "ok" | "err" | None
ss.setdefault("log_error", "")

# ì‚¬ì´ë“œë°”: ìë™ ì €ì¥ í† ê¸€ + ë¡œê·¸ ìƒíƒœ ë°•ìŠ¤
with st.sidebar:
    ss.auto_save_chatlog = st.toggle("ëŒ€í™” ìë™ ì €ì¥(Drive)", value=ss.auto_save_chatlog)
    log_box = st.container()
def _render_log_status():
    log_box.empty()
    if ss.log_status == "ok":
        log_box.success("ğŸ’¾ Drive ì €ì¥ ì™„ë£Œ (chat_log/)")
    elif ss.log_status == "err":
        log_box.error(f"âš ï¸ Drive ì €ì¥ ì‹¤íŒ¨: {ss.log_error}")

st.info("âœ… ì¸ë±ì‹±ì€ 1ë²ˆë§Œ ìˆ˜í–‰í•˜ê³ , ì €ì¥ëœ ì¸ë±ìŠ¤ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤. (ìŠ¤í… ì‹¤í–‰Â·ì·¨ì†Œ/ì¬ê°œ ì§€ì›)")

# --- rag_engine ì„í¬íŠ¸ ê°€ë“œ (ì •í™•í•œ ì˜¤ë¥˜ í‘œì‹œ) ---
def _import_rag_engine_or_die():
    import traceback
    try:
        import src.rag_engine as re_mod
        required = [
            "set_embed_provider","make_llm","get_or_build_index","get_text_answer","CancelledError",
            "start_index_builder","resume_index_builder","cancel_index_builder","get_index_progress"
        ]
        missing = [n for n in required if not hasattr(re_mod, n)]
        if missing:
            st.error("rag_engineê°€ ìµœì‹ ì´ ì•„ë‹™ë‹ˆë‹¤. ëˆ„ë½: " + ", ".join(missing))
            st.stop()
        return re_mod
    except Exception as e:
        st.error(f"rag_engine ì„í¬íŠ¸ ì˜¤ë¥˜: {e}")
        st.code(traceback.format_exc()); st.stop()
re_mod = _import_rag_engine_or_die()

# ===== Google Drive ì—°ê²° í…ŒìŠ¤íŠ¸ =====
st.markdown("## ğŸ”— Google Drive ì—°ê²° í…ŒìŠ¤íŠ¸")
st.caption("ì„œë¹„ìŠ¤ê³„ì •ì— í´ë” â€˜ì“°ê¸°(Writer)â€™ ê¶Œí•œì´ ìˆì–´ì•¼ ëŒ€í™” ë¡œê·¸ ì €ì¥ì´ ë©ë‹ˆë‹¤.")
col1, col2 = st.columns([0.65, 0.35])

with col1:
    if st.button("í´ë” íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ìµœì‹  10ê°œ)", use_container_width=True):
        ok, msg, rows = re_mod.preview_drive_files(max_items=10)
        if ok and rows:
            df = pd.DataFrame(rows)
            df["type"] = df["mime"].str.replace("application/vnd.google-apps.", "", regex=False)
            df = df.rename(columns={"modified": "modified_at"})[["name","link","type","modified_at"]]
            st.dataframe(
                df, use_container_width=True, height=360,
                column_config={
                    "name": st.column_config.TextColumn("íŒŒì¼ëª…"),
                    "link": st.column_config.LinkColumn("open", display_text="ì—´ê¸°"),
                    "type": st.column_config.TextColumn("ìœ í˜•"),
                    "modified_at": st.column_config.TextColumn("ìˆ˜ì •ì‹œê°"),
                }, hide_index=True,
            )
        elif ok:
            st.warning("í´ë”ì— íŒŒì¼ì´ ì—†ê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error(msg)

with col2:
    ok, msg = re_mod.smoke_test_drive()
    if ok:
        st.success(msg)
    else:
        st.warning(msg)

# ===== ì¸ë±ì‹± ë³´ê³ ì„œ(ìŠ¤í‚µ íŒŒì¼) =====
rep = st.session_state.get("indexing_report")
if rep:
    with st.expander("ğŸ§¾ ì¸ë±ì‹± ë³´ê³ ì„œ(ìŠ¤í‚µëœ íŒŒì¼ ë³´ê¸°)", expanded=False):
        st.write(
            f"ì´ íŒŒì¼(ë§¤ë‹ˆí˜ìŠ¤íŠ¸): {rep.get('total_manifest')} Â· "
            f"ë¡œë”©ëœ ë¬¸ì„œ: {rep.get('loaded_docs')} Â· "
            f"ìŠ¤í‚µ: {rep.get('skipped_count')}"
        )
        if rep.get("skipped"):
            st.dataframe(pd.DataFrame(rep["skipped"]), use_container_width=True, hide_index=True)
        else:
            st.caption("ìŠ¤í‚µëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ ğŸ‰")

# ===== ë‘ë‡Œ ì¤€ë¹„(ìŠ¤í… ì‹¤í–‰/ì·¨ì†Œ/ì¬ê°œ) =====
st.markdown("----")
st.subheader("ğŸ§  ë‘ë‡Œ ì¤€ë¹„ â€” ì¸ë±ìŠ¤ 1íšŒ + Gemini/ChatGPT")

def _render_progress(slot_bar, slot_msg, pct: int, msg: str | None = None):
    p = max(0, min(100, int(pct)))
    slot_bar.markdown(
        f"""<div class="gp-wrap"><div class="gp-fill" style="width:{p}%"></div>
        <div class="gp-label">{p}%</div></div>""", unsafe_allow_html=True)
    if msg is not None: slot_msg.markdown(f"<div class='gp-msg'>{msg}</div>", unsafe_allow_html=True)

def _bump_max(key: str, pct: int) -> int:
    now = int(pct); prev = int(ss.get(key, 0))
    if now < prev: now = prev
    ss[key] = now; return now

with st.expander("âš™ï¸ ì˜µì…˜", expanded=False):
    fast = st.checkbox("âš¡ ë¹ ë¥¸ ì¤€ë¹„ (ì²˜ìŒ Nê°œ ë¬¸ì„œë§Œ ì¸ë±ì‹±)", value=True,
                       disabled=ss.prep_both_running or ss.prep_both_done)
    max_docs = st.number_input("N (ë¹ ë¥¸ ëª¨ë“œì¼ ë•Œë§Œ)", min_value=5, max_value=500, value=40, step=5,
                               disabled=ss.prep_both_running or ss.prep_both_done)
    ss.save_logs = st.checkbox("ğŸ’¾ JSONL ë¡œê·¸ ì €ì¥", value=ss.save_logs, disabled=False)

st.markdown("### ğŸš€ ì¸ë±ì‹± 1ë²ˆ + ë‘ LLM ì¤€ë¹„")
c_g, c_o = st.columns(2)
with c_g: st.caption("Gemini ì§„í–‰"); g_bar = st.empty(); g_msg = st.empty()
with c_o: st.caption("ChatGPT ì§„í–‰"); o_bar = st.empty(); o_msg = st.empty()

def _is_cancelled() -> bool:
    return bool(ss.get("prep_cancel_requested", False))

def run_prepare_both_step():
    # A. ì„ë² ë”© ì„¤ì •
    embed_provider = "openai"
    embed_api = (settings.OPENAI_API_KEY.get_secret_value()
                 if getattr(settings,"OPENAI_API_KEY",None) else "")
    embed_model = getattr(settings, "OPENAI_EMBED_MODEL", "text-embedding-3-small")
    if not embed_api:
        embed_provider = "google"
        embed_api = settings.GEMINI_API_KEY.get_secret_value()
        embed_model = getattr(settings, "EMBED_MODEL", "text-embedding-004")

    try:
        if _is_cancelled(): raise re_mod.CancelledError("ì‚¬ìš©ì ì·¨ì†Œ")
        p = _bump_max("p_shared", 5)
        _render_progress(g_bar, g_msg, p, f"ì„ë² ë”© ì„¤ì •({embed_provider})")
        _render_progress(o_bar, o_msg, p, f"ì„ë² ë”© ì„¤ì •({embed_provider})")
        re_mod.set_embed_provider(embed_provider, embed_api, embed_model)
    except re_mod.CancelledError:
        ss.prep_both_running=False; ss.prep_cancel_requested=False
        _render_progress(g_bar,g_msg,ss.p_shared,"ì‚¬ìš©ì ì·¨ì†Œ")
        _render_progress(o_bar,o_msg,ss.p_shared,"ì‚¬ìš©ì ì·¨ì†Œ"); st.stop()
    except Exception as e:
        _render_progress(g_bar,g_msg,100,f"ì„ë² ë”© ì‹¤íŒ¨: {e}")
        _render_progress(o_bar,o_msg,100,f"ì„ë² ë”© ì‹¤íŒ¨: {e}")
        ss.prep_both_running=False; st.stop()

    # B. ì¸ë±ìŠ¤ ì¤€ë¹„(Resume/ì·¨ì†Œ ì§€ì›)
    persist_dir = f"{getattr(settings,'PERSIST_DIR','/tmp/my_ai_teacher/storage_gdrive')}_shared"
    re_mod.start_index_builder(
        update_pct=lambda pct,msg=None: (
            _render_progress(g_bar,g_msg,_bump_max('p_shared',pct),msg),
            _render_progress(o_bar,o_msg,ss.p_shared,msg)
        ),
        update_msg=lambda m: (
            _render_progress(g_bar,g_msg,ss.p_shared,m),
            _render_progress(o_bar,o_msg,ss.p_shared,m)
        ),
        gdrive_folder_id=settings.GDRIVE_FOLDER_ID,
        raw_sa=settings.GDRIVE_SERVICE_ACCOUNT_JSON,
        persist_dir=persist_dir,
        manifest_path=getattr(settings, "MANIFEST_PATH", "/tmp/my_ai_teacher/drive_manifest.json"),
        max_docs=(max_docs if fast else None),
        is_cancelled=_is_cancelled,
    )

    # C. ìŠ¤í… ë£¨í”„
    while True:
        if _is_cancelled():
            re_mod.cancel_index_builder()
            ss.prep_both_running=False; ss.prep_cancel_requested=False
            _render_progress(g_bar,g_msg,ss.p_shared,"ì‚¬ìš©ì ì·¨ì†Œ")
            _render_progress(o_bar,o_msg,ss.p_shared,"ì‚¬ìš©ì ì·¨ì†Œ")
            st.stop()
        state = re_mod.resume_index_builder()
        for b in state["bursts"]:
            pct, msg = b.get("pct", ss.p_shared), b.get("msg")
            _render_progress(g_bar,g_msg,_bump_max("p_shared",pct),msg)
            _render_progress(o_bar,o_msg,ss.p_shared,msg)
        if state.get("done"): break
        time.sleep(0.2)

    index = state["index"]

    # D. LLM ë‘ ê°œ ì¤€ë¹„
    try:
        g_llm = re_mod.make_llm("google", settings.GEMINI_API_KEY.get_secret_value(),
                                getattr(settings, "LLM_MODEL","gemini-1.5-pro"),
                                float(ss.get("temperature",0.0)))
        ss["llm_google"] = g_llm
        ss["qe_google"] = index.as_query_engine(
            llm=g_llm,
            response_mode=ss.get("response_mode", getattr(settings,"RESPONSE_MODE","compact")),
            similarity_top_k=int(ss.get("similarity_top_k", getattr(settings,"SIMILARITY_TOP_K",5))),
        ); _render_progress(g_bar,g_msg,100,"ì™„ë£Œ!")
    except Exception as e:
        _render_progress(g_bar,g_msg,100,f"Gemini ì¤€ë¹„ ì‹¤íŒ¨: {e}")

    try:
        if getattr(settings,"OPENAI_API_KEY",None) and settings.OPENAI_API_KEY.get_secret_value():
            o_llm = re_mod.make_llm("openai", settings.OPENAI_API_KEY.get_secret_value(),
                                    getattr(settings,"OPENAI_LLM_MODEL","gpt-4o-mini"),
                                    float(ss.get("temperature",0.0)))
            ss["llm_openai"] = o_llm
            ss["qe_openai"] = index.as_query_engine(
                llm=o_llm,
                response_mode=ss.get("response_mode", getattr(settings,"RESPONSE_MODE","compact")),
                similarity_top_k=int(ss.get("similarity_top_k", getattr(settings,"SIMILARITY_TOP_K",5))),
            ); _render_progress(o_bar,o_msg,100,"ì™„ë£Œ!")
        else:
            _render_progress(o_bar,o_msg,100,"í‚¤ ëˆ„ë½ â€” OPENAI_API_KEY í•„ìš”")
    except Exception as e:
        _render_progress(o_bar,o_msg,100,f"ChatGPT ì¤€ë¹„ ì‹¤íŒ¨: {e}")

    ss.prep_both_running=False; ss.prep_both_done=True

left, right = st.columns([0.7,0.3])
with left:
    clicked = st.button("ğŸš€ í•œ ë²ˆì— ì¤€ë¹„í•˜ê¸°", key="prepare_both",
                        use_container_width=True, disabled=ss.prep_both_running or ss.prep_both_done)
with right:
    cancel_clicked = st.button("â›” ì¤€ë¹„ ì·¨ì†Œ", key="cancel_prepare",
                               use_container_width=True, type="secondary",
                               disabled=not ss.prep_both_running)

if cancel_clicked and ss.prep_both_running:
    ss.prep_cancel_requested=True; st.rerun()
if clicked and not (ss.prep_both_running or ss.prep_both_done):
    ss.p_shared=0; ss.prep_cancel_requested=False; ss.prep_both_running=True
    run_prepare_both_step(); st.experimental_rerun()  # í•œë²ˆ ë” ì•ˆì • ë¦¬í”„ë ˆì‹œ

st.caption("ì¤€ë¹„ ë²„íŠ¼ì„ ë‹¤ì‹œ í™œì„±í™”í•˜ë ¤ë©´ ì•„ë˜ ì¬ì„¤ì • ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
if st.button("ğŸ”§ ì¬ì„¤ì •(ë²„íŠ¼ ë‹¤ì‹œ í™œì„±í™”)", disabled=not ss.prep_both_done):
    ss.prep_both_done=False; ss.p_shared=0; st.rerun()

# ===== ëŒ€í™” UI =====
st.markdown("---"); st.subheader("ğŸ’¬ ê·¸ë£¹í† ë¡  (í•™ìƒ â†’ Gemini â†’ ChatGPT)")

ready_google = "qe_google" in ss
ready_openai = "qe_openai" in ss
if ss.session_terminated:
    st.warning("ì„¸ì…˜ì´ ì¢…ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”."); st.stop()
if not ready_google:
    st.info("ë¨¼ì € **[ğŸš€ í•œ ë²ˆì— ì¤€ë¹„í•˜ê¸°]** ë¥¼ í´ë¦­í•´ ë‘ë‡Œë¥¼ ì¤€ë¹„í•˜ì„¸ìš”. (OpenAI í‚¤ê°€ ì—†ìœ¼ë©´ Geminië§Œ ì‘ë‹µ)")
    st.stop()

# ê¸°ì¡´ ë©”ì‹œì§€ ë Œë”
for m in ss.messages:
    with st.chat_message(m["role"]): st.markdown(m["content"])

def _strip_sources(text: str) -> str:
    return re.sub(r"\n+---\n\*ì°¸ê³  ìë£Œ:.*$", "", text, flags=re.DOTALL)

def _build_context_for_models(messages: list[dict], limit_pairs: int = 2, max_chars: int = 2000) -> str:
    pairs, buf_user = [], None
    for m in reversed(messages):
        role, content = m.get("role"), str(m.get("content","")).strip()
        if role == "assistant":
            content = re.sub(r"^\*\*ğŸ¤– .*?\*\*\s*\n+","",content).strip()
            if buf_user is not None:
                pairs.append((buf_user, content)); buf_user=None
                if len(pairs) >= limit_pairs: break
        elif role == "user" and buf_user is None:
            buf_user = content
    pairs=list(reversed(pairs))
    blocks=[f"[í•™ìƒ]\n{u}\n\n[êµì‚¬]\n{a}" for u,a in pairs]
    ctx="\n\n---\n\n".join(blocks).strip()
    return ctx[-max_chars:] if len(ctx)>max_chars else ctx

# JSONL ì €ì¥(ì§€ì† ìƒíƒœ í‘œì‹œ)
def _log_try(items):
    if not ss.save_logs: return
    try:
        parent_id = (getattr(settings,"CHATLOG_FOLDER_ID",None) or settings.GDRIVE_FOLDER_ID)
        sa = settings.GDRIVE_SERVICE_ACCOUNT_JSON
        if isinstance(sa,str):
            try: sa=json.loads(sa)
            except Exception: pass
        sub_id = get_chatlog_folder_id(parent_folder_id=parent_id, sa_json=sa)
        chat_store.append_jsonl(folder_id=sub_id, sa_json=sa, items=items)
        ss.log_status="ok"; ss.log_error=""
    except Exception as e:
        ss.log_status="err"; ss.log_error=str(e)
    _render_log_status()

# ===== ì…ë ¥ ì²˜ë¦¬ =====
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ë¶„ì„/ìš”ì•½í•  ë¬¸ì¥ì´ë‚˜ ê¸€ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")
if user_input:
    ss.messages.append({"role":"user","content":user_input})
    with st.chat_message("user"): st.markdown(user_input)
    _log_try([chat_store.make_entry(ss.session_id, "user", "user", user_input, "group", model="user")])

    # 1) Gemini (ì¹œì ˆí•œ ì¹œêµ¬)
    with st.spinner("ğŸ¤– Gemini(ì°©í•˜ê³  ë˜‘ë˜‘í•œ ì¹œêµ¬)ê°€ ë¨¼ì € ë‹µí•©ë‹ˆë‹¤â€¦"):
        prev_ctx = _build_context_for_models(ss.messages[:-1], limit_pairs=2, max_chars=2000)
        gemini_query = (f"[ì´ì „ ëŒ€í™”]\n{prev_ctx}\n\n" if prev_ctx else "") + f"[í•™ìƒ ì§ˆë¬¸]\n{user_input}"
        gemini_system = EXPLAINER_PROMPT + "\n" + GEMINI_TONE + """
[íŒ€í”Œë ˆì´ ì¶œë ¥í˜•ì‹]
- ë³¸ ë‹µë³€
- ë„˜ê²¨ë°›ì„_í¬ì¸íŠ¸: ë‹¤ìŒ í™”ìì—ê²Œ ë„˜ê¸°ê³  ì‹¶ì€ í•µì‹¬ 2~3ì¤„
"""
        ans_g = re_mod.get_text_answer(ss["qe_google"], gemini_query, gemini_system)
    content_g = f"**ğŸ¤– Gemini**\n\n{ans_g}"
    ss.messages.append({"role":"assistant","content":content_g})
    with st.chat_message("assistant"): st.markdown(content_g)
    _log_try([chat_store.make_entry(ss.session_id,"assistant","Gemini",content_g,"group",
                                    model=getattr(settings,"LLM_MODEL","gemini"))])

    # 2) ChatGPT (ìœ ë¨¸ëŸ¬ìŠ¤í•œ ì¹œêµ¬)
    if ready_openai:
        from src.rag_engine import llm_complete
        prev_ctx = _build_context_for_models(ss.messages, limit_pairs=2, max_chars=2000)
        augmented = (
            (f"[ì´ì „ ëŒ€í™”]\n{prev_ctx}\n\n" if prev_ctx else "") +
            f"[í•™ìƒ ì§ˆë¬¸]\n{user_input}\n\n"
            f"[ë™ë£Œì˜ 1ì°¨ ë‹µë³€(Gemini)]\n{_strip_sources(ans_g)}\n\n"
            f"[ë‹¹ì‹ ì˜ ì‘ì—…]\nìœ„ ê¸°ì¤€(REVIEW_DIRECTIVE)ì— ë”°ë¼ ë³´ì™„/ê²€ì¦í•˜ë¼."
        )
        chatgpt_system = EXPLAINER_PROMPT + "\n" + CHATGPT_TONE + "\n" + REVIEW_DIRECTIVE
        with st.spinner("ğŸ¤ ChatGPT(ìœ ë¨¸ëŸ¬ìŠ¤í•œ ì¹œêµ¬)ê°€ ë³´ì™„/ê²€ì¦ ì¤‘â€¦"):
            ans_o = llm_complete(ss.get("llm_openai"), chatgpt_system + "\n\n" + augmented)
        content_o = f"**ğŸ¤– ChatGPT**\n\n{ans_o}"
        ss.messages.append({"role":"assistant","content":content_o})
        with st.chat_message("assistant"): st.markdown(content_o)
        _log_try([chat_store.make_entry(ss.session_id,"assistant","ChatGPT",content_o,"group",
                                        model=getattr(settings,"OPENAI_LLM_MODEL","gpt-4o-mini"))])
    else:
        with st.chat_message("assistant"):
            st.info("ChatGPT í‚¤ê°€ ì—†ì–´ Geminië§Œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤. OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ë©´ ë³´ì™„/ê²€ì¦ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

    # Markdown ëŒ€í™” ë¡œê·¸ ì €ì¥(ì§€ì† ìƒíƒœ)
    if ss.auto_save_chatlog and ss.messages:
        try:
            parent_id = (getattr(settings,"CHATLOG_FOLDER_ID",None) or settings.GDRIVE_FOLDER_ID)
            sa = settings.GDRIVE_SERVICE_ACCOUNT_JSON
            if isinstance(sa,str):
                try: sa=json.loads(sa)
                except Exception: pass
            save_chatlog_markdown(ss.session_id, ss.messages, parent_folder_id=parent_id, sa_json=sa)
            ss.log_status="ok"; ss.log_error=""
        except Exception as e:
            ss.log_status="err"; ss.log_error=str(e)
        _render_log_status()
    st.rerun()
