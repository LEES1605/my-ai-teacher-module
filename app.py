# app.py
# ===== TOP OF FILE ============================================================
# 0) Streamlit í˜ì´ì§€ ì„¤ì •ì€ ë°˜ë“œì‹œ ìµœìƒë‹¨ì—ì„œ!
import os
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"
os.environ["STREAMLIT_RUN_ON_SAVE"] = "false"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["STREAMLIT_SERVER_ENABLE_WEBSOCKET_COMPRESSION"] = "false"

import time
import streamlit as st

from src.config import settings, APP_DATA_DIR, PERSIST_DIR, MANIFEST_PATH
from src.ui import load_css, safe_render_header, ensure_progress_css, render_progress_bar, render_stepper
from src.prompts import EXPLAINER_PROMPT, ANALYST_PROMPT, READER_PROMPT
from src.rag_engine import (
    get_or_build_index,
    init_llama_settings,
    get_text_answer,
    _load_index_from_disk,   # â† ì €ì¥ëœ ë‘ë‡Œë¥¼ ì¦‰ì‹œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•œ ë‚´ë¶€ í—¬í¼
)
from src.auth import admin_login_flow

# 2) í˜ì´ì§€ ë©”íƒ€
st.set_page_config(page_title="ë‚˜ì˜ AI ì˜ì–´ êµì‚¬", layout="wide", initial_sidebar_state="collapsed")

# 3) ì „ì—­ CSS + (ì„ íƒ) ë°°ê²½
load_css("assets/style.css", use_bg=settings.USE_BG_IMAGE, bg_path="assets/background_book.png")
ensure_progress_css()

# 4) í—¤ë”
safe_render_header()

# 5) ìƒë‹¨ ìš°ì¸¡ ê´€ë¦¬ì ë²„íŠ¼
_, _, c3 = st.columns([0.8, 0.1, 0.1])
with c3:
    if st.button("ğŸ› ï¸", key="admin_icon_top_bar"):
        st.session_state.admin_mode = True

# 6) ê³ ê¸‰/RAG íŠœë‹ íŒ¨ë„(ê´€ë¦¬ìë§Œ ë³´ì´ê²Œ)
if admin_login_flow(settings.ADMIN_PASSWORD or ""):
    with st.expander("âš™ï¸ ê³ ê¸‰ RAG/LLM ì„¤ì •", expanded=False):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.session_state.setdefault("similarity_top_k", settings.SIMILARITY_TOP_K)
            k = st.slider("similarity_top_k", 1, 12, st.session_state["similarity_top_k"])
        with col2:
            st.session_state.setdefault("temperature", 0.0)
            temp = st.slider("LLM temperature", 0.0, 1.0, float(st.session_state["temperature"]), 0.05)
        with col3:
            st.session_state.setdefault("response_mode", settings.RESPONSE_MODE)
            mode_sel = st.selectbox(
                "response_mode",
                ["compact", "refine", "tree_summarize"],
                index=["compact", "refine", "tree_summarize"].index(st.session_state["response_mode"]),
            )
        if st.button("ì ìš©"):
            st.session_state["similarity_top_k"] = k
            st.session_state["temperature"] = temp
            st.session_state["response_mode"] = mode_sel
            st.success("RAG/LLM ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ì¸ë±ìŠ¤/ì—”ì§„ì€ ë‹¤ìŒ ì¿¼ë¦¬ë¶€í„° ë°˜ì˜)")

    with st.expander("ğŸ› ï¸ ê´€ë¦¬ì ë„êµ¬", expanded=False):
        if st.button("ğŸ“¥ ê°•ì˜ ìë£Œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ê¸° (ë‘ë‡Œ ì´ˆê¸°í™”)"):
            import shutil
            if os.path.exists(PERSIST_DIR):
                shutil.rmtree(PERSIST_DIR)
            if "query_engine" in st.session_state:
                del st.session_state["query_engine"]
            st.success("ë‘ë‡Œ íŒŒì¼ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ì‹œ ì¤€ë¹„í•˜ì„¸ìš”.")

# 7) ë©”ì¸ ì›Œí¬í”Œë¡œìš°
def _auto_attach_saved_brain() -> bool:
    """
    ì´ë¯¸ ì €ì¥ëœ ë‘ë‡Œê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ì—°ê²°.
    ì„±ê³µ ì‹œ True ë°˜í™˜(ì±„íŒ… UIë¡œ ì§„ì…), ì‹¤íŒ¨/ì—†ìŒì´ë©´ False.
    """
    # ì €ì¥ëœ ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤
    if not os.path.exists(PERSIST_DIR):
        return False
    try:
        with st.spinner("ğŸ”„ ì´ì „ì— ì¤€ë¹„ëœ ë‘ë‡Œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
            # LLM/ì„ë² ë”© ì—”ì§„ì€ ì¿¼ë¦¬ ì—”ì§„ ìƒì„± ì „ì— ì¤€ë¹„í•´ì•¼ í•¨
            init_llama_settings(
                api_key=settings.GEMINI_API_KEY.get_secret_value(),
                llm_model=settings.LLM_MODEL,
                embed_model=settings.EMBED_MODEL,
                temperature=float(st.session_state.get("temperature", 0.0)),
            )
            index = _load_index_from_disk(PERSIST_DIR)  # ìºì‹œ ë¦¬ì†ŒìŠ¤ë¼ ë¹ ë¦„
            st.session_state.query_engine = index.as_query_engine(
                response_mode=st.session_state.get("response_mode", settings.RESPONSE_MODE),
                similarity_top_k=int(st.session_state.get("similarity_top_k", settings.SIMILARITY_TOP_K)),
            )
        st.success("ì´ì „ì— ì¤€ë¹„ëœ ë‘ë‡Œì— ìë™ ì—°ê²°í–ˆìŠµë‹ˆë‹¤.")
        st.rerun()
        return True
    except Exception:
        # ìë™ ì—°ê²° ì‹¤íŒ¨í•´ë„ ì‚¬ìš©ìì—ê²Œ ë²„íŠ¼ì„ ë³´ì—¬ì£¼ê³  ìˆ˜ë™ ì¤€ë¹„ë¡œ ì§„í–‰
        st.warning("ì €ì¥ëœ ë‘ë‡Œ ìë™ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ë‹¤ì‹œ ì¤€ë¹„í•´ ì£¼ì„¸ìš”.")
        return False

def main():
    # âŠ ì„¸ì…˜ì— ì—”ì§„ì´ ì—†ê³ , ì €ì¥ë³¸ì´ ìˆìœ¼ë©´ ìë™ ì—°ê²° ì‹œë„
    if "query_engine" not in st.session_state:
        if _auto_attach_saved_brain():
            st.stop()

    # â‹ ì—¬ì „íˆ ì—”ì§„ì´ ì—†ìœ¼ë©´ ë²„íŠ¼ í‘œì‹œ â†’ ìˆ˜ë™ ì¤€ë¹„
    if "query_engine" not in st.session_state:
        st.info("AI êµì‚¬ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”. ì²˜ìŒì—ëŠ” í•™ìŠµëŸ‰ì— ë”°ë¼ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        if st.button("ğŸ§  AI ë‘ë‡Œ ì¤€ë¹„ ì‹œì‘í•˜ê¸°"):
            # â”€â”€ ì§„í–‰ í‘œì‹œìš© ìŠ¬ë¡¯ë“¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            stepper_slot = st.empty()
            bar_slot = st.empty()
            msg_slot = st.empty()

            # â”€â”€ ìŠ¤í… ì •ì˜ & ìƒíƒœ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            steps = [
                ("check", "ë“œë¼ì´ë¸Œ ë³€ê²½ í™•ì¸"),
                ("init", "Drive ë¦¬ë” ì´ˆê¸°í™”"),
                ("list", "ë¬¸ì„œ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘"),
                ("index", "ì¸ë±ìŠ¤ ìƒì„±"),
                ("save", "ë‘ë‡Œ ì €ì¥"),
            ]
            st.session_state["_step_status"] = {k: "pending" for k, _ in steps}
            st.session_state["_step_curr"] = None

            def _set_active(key: str):
                prev = st.session_state.get("_step_curr")
                if prev and st.session_state["_step_status"].get(prev) == "active":
                    st.session_state["_step_status"][prev] = "done"
                st.session_state["_step_status"][key] = "active"
                st.session_state["_step_curr"] = key
                render_stepper(stepper_slot, steps, st.session_state["_step_status"], sticky=True)

            def _set_done_all():
                for k, _ in steps:
                    st.session_state["_step_status"][k] = "done"
                render_stepper(stepper_slot, steps, st.session_state["_step_status"], sticky=True)

            # ìµœì´ˆ í‘œì‹œ
            _set_active("check")
            render_progress_bar(bar_slot, 0)
            msg_slot.markdown("<div class='gp-msg'>ë‘ë‡Œ ì¤€ë¹„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤â€¦</div>", unsafe_allow_html=True)

            # update ì½œë°±: ì§„í–‰ë°” + ë©”ì‹œì§€ + ìŠ¤í…í¼ ë™ê¸°í™”
            st.session_state["_gp_pct"] = 0
            def update_pct(pct: int, msg: str | None = None):
                st.session_state["_gp_pct"] = max(0, min(100, int(pct)))
                render_progress_bar(bar_slot, st.session_state["_gp_pct"])
                if msg is not None:
                    update_msg(msg)

            def update_msg(msg: str):
                text = msg or ""
                if "ë³€ê²½ í™•ì¸" in text:
                    _set_active("check")
                elif "ë¦¬ë” ì´ˆê¸°í™”" in text:
                    _set_active("init")
                elif "ëª©ë¡ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘" in text:
                    _set_active("list")
                elif "ì¸ë±ìŠ¤ ìƒì„±" in text:
                    _set_active("index")
                elif "ì €ì¥ ì¤‘" in text:
                    _set_active("save")
                elif "ì™„ë£Œ" in text:
                    _set_done_all()
                msg_slot.markdown(f"<div class='gp-msg'>{text}</div>", unsafe_allow_html=True)

            # 1) LLM/Embedding ì„¤ì •(ì˜¨ë””ë§¨ë“œ)
            init_llama_settings(
                api_key=settings.GEMINI_API_KEY.get_secret_value(),
                llm_model=settings.LLM_MODEL,
                embed_model=settings.EMBED_MODEL,
                temperature=float(st.session_state.get("temperature", 0.0)),
            )

            # 2) ì¸ë±ìŠ¤ ì¤€ë¹„(ë³€ê²½ ê°ì§€ â†’ í•„ìš” ì‹œ ì¬ë¹Œë“œ)
            index = get_or_build_index(
                update_pct=update_pct,
                update_msg=update_msg,
                gdrive_folder_id=settings.GDRIVE_FOLDER_ID,
                raw_sa=settings.GDRIVE_SERVICE_ACCOUNT_JSON,
                persist_dir=PERSIST_DIR,
                manifest_path=MANIFEST_PATH,
            )

            # 3) ì§ˆì˜ ì—”ì§„ ìƒì„±
            st.session_state.query_engine = index.as_query_engine(
                response_mode=st.session_state.get("response_mode", settings.RESPONSE_MODE),
                similarity_top_k=int(st.session_state.get("similarity_top_k", settings.SIMILARITY_TOP_K)),
            )
            update_pct(100, "ì™„ë£Œ!")
            time.sleep(0.5)
            stepper_slot.empty(); bar_slot.empty(); msg_slot.empty()
            st.rerun()

        st.stop()

    # === ì±„íŒ… UI ===
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    st.markdown("---")

    mode = st.radio(
        "**ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•œê°€ìš”?**",
        ["ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…", "ğŸ” êµ¬ë¬¸ ë¶„ì„", "ğŸ“š ë…í•´ ë° ìš”ì•½"],
        horizontal=True,
        key="mode_select",
    )

    prompt = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ë¶„ì„/ìš”ì•½í•  ë¬¸ì¥ì´ë‚˜ ê¸€ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")

    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("AI ì„ ìƒë‹˜ì´ ë‹µë³€ì„ ìƒê°í•˜ê³  ìˆì–´ìš”..."):
            if mode == "ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…":
                selected_prompt = EXPLAINER_PROMPT
            elif mode == "ğŸ” êµ¬ë¬¸ ë¶„ì„":
                selected_prompt = ANALYST_PROMPT
            else:
                selected_prompt = READER_PROMPT

            answer = get_text_answer(st.session_state.query_engine, prompt, selected_prompt)

        st.session_state.messages.append({"role": "assistant", "content": answer})
        st.rerun()

if __name__ == "__main__":
    main()
# ===== END OF FILE ============================================================
