# app.py â€” í•œ ë²ˆì— ë‘ ì—”ì§„(ğŸ§ Gemini / ğŸ§ ChatGPT) ì¤€ë¹„ + ê°ì ì§„í–‰ë°” + ì„ íƒ ë‹µë³€

import streamlit as st
import pandas as pd
import time

# ===== í˜ì´ì§€ ì„¤ì • ============================================================
st.set_page_config(page_title="ë‚˜ì˜ AI ì˜ì–´ êµì‚¬", layout="wide", initial_sidebar_state="collapsed")

# ===== ê¸°ë³¸ UI/ìŠ¤íƒ€ì¼ =========================================================
from src.ui import load_css, render_header
load_css()
render_header()

st.info("âœ… í•œ ë²ˆì˜ í´ë¦­ìœ¼ë¡œ Gemini/ChatGPT ë‘ ì—”ì§„ì„ ëª¨ë‘ ì¤€ë¹„í•˜ê³ , ê°ì ì§„í–‰ ìƒí™©ì„ ë”°ë¡œ ë³¼ ìˆ˜ ìˆì–´ìš”.")

# ===== Google Drive ì—°ê²° í…ŒìŠ¤íŠ¸ ===============================================
# ì„í¬íŠ¸ ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì˜¤ë¥˜ ë³´ì—¬ì£¼ê¸°
try:
    from src.rag_engine import smoke_test_drive, preview_drive_files
except Exception:
    st.error("`src.rag_engine` ì„í¬íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    import traceback, os
    st.write("íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:", os.path.exists("src/rag_engine.py"))
    with st.expander("ì„í¬íŠ¸ ìŠ¤íƒ(ì›ì¸)", expanded=True):
        st.code(traceback.format_exc())
    st.stop()

st.markdown("## ğŸ”— Google Drive ì—°ê²° í…ŒìŠ¤íŠ¸")
st.caption("ë²„íŠ¼ì„ ëˆŒëŸ¬ Drive í´ë” ì—°ê²°ì´ ì •ìƒì¸ì§€ í™•ì¸í•˜ì„¸ìš”. (ì„œë¹„ìŠ¤ê³„ì •ì— Viewer ì´ìƒ ê³µìœ  í•„ìš”)")

col1, col2 = st.columns([0.65, 0.35])
with col1:
    if st.button("í´ë” íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ìµœì‹  10ê°œ)", use_container_width=True):
        ok, msg, rows = preview_drive_files(max_items=10)
        if ok and rows:
            df = pd.DataFrame(rows)
            df["type"] = df["mime"].str.replace("application/vnd.google-apps.", "", regex=False)
            df = df.rename(columns={"modified": "modified_at"})
            df = df[["name", "link", "type", "modified_at"]]
            st.dataframe(
                df,
                use_container_width=True,
                height=360,
                column_config={
                    "name": st.column_config.TextColumn("íŒŒì¼ëª…"),
                    "link": st.column_config.LinkColumn("open", display_text="ì—´ê¸°"),
                    "type": st.column_config.TextColumn("ìœ í˜•"),
                    "modified_at": st.column_config.TextColumn("ìˆ˜ì •ì‹œê°"),
                },
                hide_index=True,
            )
        elif ok:
            st.warning("í´ë”ì— íŒŒì¼ì´ ì—†ê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error(msg)
with col2:
    ok, msg = smoke_test_drive()
    st.success(msg) if ok else st.warning(msg)

# ===== ë‘ë‡Œ ì¤€ë¹„ (ì‹œë®¬ë ˆì´ì…˜) ==================================================
st.markdown("----")
st.subheader("ğŸ§  ë‘ë‡Œ ì¤€ë¹„ (ì‹œë®¬ë ˆì´ì…˜)")
if st.button("ë‘ë‡Œ ì¤€ë¹„ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘"):
    bar_slot = st.empty(); msg_slot = st.empty()
    def render_progress(pct: int, msg: str | None = None):
        p = max(0, min(100, int(pct)))
        bar_slot.markdown(f"""
<div class="gp-wrap"><div class="gp-fill" style="width:{p}%"></div><div class="gp-label">{p}%</div></div>
""", unsafe_allow_html=True)
        if msg: msg_slot.markdown(f"<div class='gp-msg'>{msg}</div>", unsafe_allow_html=True)
    render_progress(5, "ì‹œì‘â€¦"); time.sleep(0.2)
    render_progress(25, "ë¹„ë°€í‚¤ ì ê²€â€¦")
    missing = [k for k in ("GEMINI_API_KEY", "GDRIVE_FOLDER_ID") if not str(st.secrets.get(k, "")).strip()]
    if missing: render_progress(100, "ì‹¤íŒ¨"); st.error("í•„ìˆ˜ Secrets ì—†ìŒ: " + ", ".join(missing)); st.stop()
    render_progress(60, "í™˜ê²½ ì¤€ë¹„â€¦"); time.sleep(0.2)
    render_progress(90, "ë§ˆë¬´ë¦¬â€¦"); time.sleep(0.2)
    render_progress(100, "ì™„ë£Œ!"); time.sleep(0.2)
    bar_slot.empty(); msg_slot.empty()
    st.success("ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ â€” UI/ì§„í–‰ íë¦„ ì •ìƒì…ë‹ˆë‹¤.")

# ===== ë‘ë‡Œ ì¤€ë¹„ (ì‹¤ì „) â€” ë²„íŠ¼ í•˜ë‚˜ë¡œ ë‘ ì—”ì§„ ë™ì‹œì— ì§„í–‰ ======================
st.markdown("----")
st.subheader("ğŸ§  ë‘ë‡Œ ì¤€ë¹„ (ì‹¤ì „) â€” ğŸš€ í•œ ë²ˆì— Gemini & ChatGPT")

from src.config import settings
# LLM/Index ìœ í‹¸ ì„í¬íŠ¸ (ì˜¤ë¥˜ ì‹œ ìƒì„¸ í‘œì‹œ)
try:
    from src.rag_engine import init_llama_settings, get_or_build_index, get_text_answer
except Exception:
    st.error("`src.rag_engine` ì„í¬íŠ¸(LLM/RAG) ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    import traceback
    with st.expander("ì„í¬íŠ¸ ìŠ¤íƒ(ì›ì¸)", expanded=True):
        st.code(traceback.format_exc())
    st.stop()

def _render_progress(slot_bar, slot_msg, pct: int, msg: str | None = None):
    p = max(0, min(100, int(pct)))
    slot_bar.markdown(f"""
<div class="gp-wrap"><div class="gp-fill" style="width:{p}%"></div><div class="gp-label">{p}%</div></div>
""", unsafe_allow_html=True)
    if msg is not None:
        slot_msg.markdown(f"<div class='gp-msg'>{msg}</div>", unsafe_allow_html=True)

DEFAULTS = {
    "google": {"llm": "gemini-1.5-pro", "embed": "text-embedding-004"},
    "openai": {"llm": "gpt-4o-mini",     "embed": "text-embedding-3-small"},
}

def _build_one(provider: str, bar_slot, msg_slot):
    """í•œ ê³µê¸‰ìì— ëŒ€í•´ ì§„í–‰ë°”/ë©”ì‹œì§€ë¥¼ í•´ë‹¹ ìŠ¬ë¡¯ì—ë§Œ ê·¸ë¦¬ë©° ë‘ë‡Œë¥¼ ì¤€ë¹„."""
    provider = provider.lower()
    _render_progress(bar_slot, msg_slot, 0, f"{provider.title()} ë‘ë‡Œ ì¤€ë¹„ ì‹œì‘â€¦")

    # 1) ëª¨ë¸/í‚¤/ê²½ë¡œ ê²°ì •
    if provider == "google":
        api_key = settings.GEMINI_API_KEY.get_secret_value()
        llm_model = DEFAULTS["google"]["llm"] if "gemini" not in getattr(settings, "LLM_MODEL", "") else settings.LLM_MODEL
        embed_model = DEFAULTS["google"]["embed"] if "embedding" not in getattr(settings, "EMBED_MODEL", "") else settings.EMBED_MODEL
        persist_dir = f"{getattr(settings, 'PERSIST_DIR', '/tmp/my_ai_teacher/storage_gdrive')}_google"
    else:
        api_key = getattr(settings, "OPENAI_API_KEY", None).get_secret_value() if hasattr(settings, "OPENAI_API_KEY") else ""
        llm_model = getattr(settings, "OPENAI_LLM_MODEL", DEFAULTS["openai"]["llm"])
        embed_model = getattr(settings, "OPENAI_EMBED_MODEL", DEFAULTS["openai"]["embed"])
        persist_dir = f"{getattr(settings, 'PERSIST_DIR', '/tmp/my_ai_teacher/storage_gdrive')}_openai"

    if not api_key:
        _render_progress(bar_slot, msg_slot, 100, "í‚¤ ëˆ„ë½ â€” secrets.toml í™•ì¸")
        return None

    # 2) LLM/ì„ë² ë”© ì´ˆê¸°í™” (ì„ë² ë”©ì€ Settingsì— ì„¤ì •, LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜)
    try:
        llm = init_llama_settings(
            provider=provider,
            api_key=api_key,
            llm_model=llm_model,
            embed_model=embed_model,
            temperature=float(st.session_state.get("temperature", 0.0)),
        )
    except Exception as e:
        _render_progress(bar_slot, msg_slot, 100, f"LLM/ì„ë² ë”© ì„¤ì • ì˜¤ë¥˜: {e}")
        return None

    # 3) ì¸ë±ìŠ¤ ë¡œë”©/ë¹Œë“œ (í•´ë‹¹ ì§„í–‰ë°”ë§Œ ê°±ì‹ )
    try:
        progress = {"pct": 0}
        def update_pct(pct: int, m: str | None = None):
            progress["pct"] = int(pct); _render_progress(bar_slot, msg_slot, progress["pct"], m)
        def update_msg(m: str):
            _render_progress(bar_slot, msg_slot, progress["pct"], m)

        index = get_or_build_index(
            update_pct=update_pct,
            update_msg=update_msg,
            gdrive_folder_id=settings.GDRIVE_FOLDER_ID,
            raw_sa=settings.GDRIVE_SERVICE_ACCOUNT_JSON,
            persist_dir=persist_dir,
            manifest_path=getattr(settings, "MANIFEST_PATH", "/tmp/my_ai_teacher/drive_manifest.json"),
        )
    except Exception as e:
        _render_progress(bar_slot, msg_slot, 100, f"ì¸ë±ìŠ¤ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        return None

    # 4) QueryEngine ìƒì„±(ì´ ê³µê¸‰ìì˜ LLMì„ ëª…ì‹œ ì£¼ì…)
    qe = index.as_query_engine(
        llm=llm,
        response_mode=st.session_state.get("response_mode", getattr(settings, "RESPONSE_MODE", "compact")),
        similarity_top_k=int(st.session_state.get("similarity_top_k", getattr(settings, "SIMILARITY_TOP_K", 5))),
    )

    # 5) ì„¸ì…˜ì— ì €ì¥ + ì™„ë£Œ í‘œì‹œ
    key = "qe_google" if provider == "google" else "qe_openai"
    st.session_state[key] = qe
    _render_progress(bar_slot, msg_slot, 100, "ì™„ë£Œ!")
    return qe

# === â–¶ ë²„íŠ¼ í•˜ë‚˜ë¡œ ë‘ ì—”ì§„ ë™ì‹œ ì¤€ë¹„(ìˆœì°¨ ì‹¤í–‰, ê°ì ì§„í–‰ë°” ë³„ë„ í‘œì‹œ) ==========
st.markdown("### ğŸš€ ë‘ ì—”ì§„ í•œêº¼ë²ˆì— ì¤€ë¹„")
c_g, c_o = st.columns(2)
with c_g: st.caption("Gemini ì§„í–‰"); g_bar = st.empty(); g_msg = st.empty()
with c_o: st.caption("ChatGPT ì§„í–‰"); o_bar = st.empty(); o_msg = st.empty()

if st.button("ğŸš€ ë‘ ì—”ì§„ í•œêº¼ë²ˆì— ì¤€ë¹„", use_container_width=True):
    # ì‹œì‘ ìƒíƒœ í‘œì‹œ
    _render_progress(g_bar, g_msg, 0, "ëŒ€ê¸° ì¤‘â€¦")
    _render_progress(o_bar, o_msg, 0, "ëŒ€ê¸° ì¤‘â€¦")

    # ìˆœì°¨ ì‹¤í–‰(ì•ˆì •/ìì› ë³´í˜¸ ëª©ì ) â€” ê°ì ë°”/ë©”ì‹œì§€ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨
    _build_one("google", g_bar, g_msg)
    _build_one("openai", o_bar, o_msg)

    # ëë‚œ ë’¤ ë¦¬ëŸ°(ì±„íŒ… UI ê°±ì‹ )
    time.sleep(0.2)
    st.rerun()

# ===== ëŒ€í™” UI â€” ë‹µë³€í•  AI ì„ íƒ í›„ ì§ˆë¬¸ ========================================
st.markdown("---")
st.subheader("ğŸ’¬ ëŒ€í™”")

# ì¤€ë¹„ ìƒíƒœ
ready_google = "qe_google" in st.session_state
ready_openai = "qe_openai" in st.session_state
if not (ready_google or ready_openai):
    st.info("ë¨¼ì € ìœ„ì˜ **[ğŸš€ ë‘ ì—”ì§„ í•œêº¼ë²ˆì— ì¤€ë¹„]**ë¥¼ í´ë¦­í•´ ì£¼ì„¸ìš”. (OpenAI í‚¤ê°€ ì—†ìœ¼ë©´ Geminië§Œ ì¤€ë¹„ë©ë‹ˆë‹¤)")
    st.stop()

# ëŒ€í™” ê¸°ë¡
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ë©”ì‹œì§€ ë Œë”
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ì–´ë–¤ AIë¡œ ë‹µë³€ ë°›ì„ì§€ ì„ íƒ
choices = []
if ready_google: choices.append("Gemini")
if ready_openai: choices.append("ChatGPT")
answer_with = st.radio("ë‹µë³€í•  AI ì„ íƒ", choices, horizontal=True, index=0)

# í”„ë¡¬í”„íŠ¸ë“¤
from src.prompts import EXPLAINER_PROMPT, ANALYST_PROMPT, READER_PROMPT
mode = st.radio("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…", "ğŸ” êµ¬ë¬¸ ë¶„ì„", "ğŸ“š ë…í•´ ë° ìš”ì•½"], horizontal=True, key="mode_select")

# ì…ë ¥ì°½
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ë¶„ì„/ìš”ì•½í•  ë¬¸ì¥ì´ë‚˜ ê¸€ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"): st.markdown(user_input)

    # í”„ë¡¬í”„íŠ¸ ì„ íƒ
    system_prompt = EXPLAINER_PROMPT if mode == "ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…" else (ANALYST_PROMPT if mode == "ğŸ” êµ¬ë¬¸ ë¶„ì„" else READER_PROMPT)

    # ì„ íƒëœ ì—”ì§„
    qe = st.session_state.get("qe_google" if answer_with == "Gemini" else "qe_openai")
    if qe is None:
        st.warning(f"{answer_with} ë‘ë‡Œê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ì–´ìš”. ìœ„ì—ì„œ ë¨¼ì € ì¤€ë¹„ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    else:
        with st.spinner(f"{answer_with}ê°€ ë‹µë³€ì„ ìƒê°í•˜ê³  ìˆì–´ìš”..."):
            answer = get_text_answer(qe, user_input, system_prompt)

        # ì–´ëŠ AIì¸ì§€ ë¼ë²¨ë§í•˜ì—¬ ì¶œë ¥
        label = "ğŸ¤– Gemini" if answer_with == "Gemini" else "ğŸ¤– ChatGPT"
        content = f"**{label}**\n\n{answer}"
        st.session_state.messages.append({"role": "assistant", "content": content})
        with st.chat_message("assistant"): st.markdown(content)

    st.rerun()
