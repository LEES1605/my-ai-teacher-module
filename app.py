# app.py â€” ìŠ¤í… ì¸ë±ì‹±(ì¤‘ê°„ì·¨ì†Œ/ì¬ê°œ) + ê·¸ë£¹í† ë¡  + ê´€ë¦¬ì ì—…ë¡œë“œ(OAuth ì—…ë¡œë“œ)

import os, time, uuid, re, json
import pandas as pd
import streamlit as st

# ê¸°ë³¸ UI/ì„¤ì •
from src.ui import load_css, render_header
from src.config import settings
from src.prompts import EXPLAINER_PROMPT, ANALYST_PROMPT, READER_PROMPT

# RAG/ì¸ë±ì‹± ìœ í‹¸
from src.rag_engine import (
    set_embed_provider, make_llm, get_text_answer, CancelledError,
    start_index_builder, resume_index_builder, cancel_index_builder,
    smoke_test_drive, preview_drive_files,
)

# ë¡œê·¸ ì €ì¥
from src import chat_store
from src.drive_log import save_chatlog_markdown_oauth, get_chatlog_folder_id

# OAuth
from src.google_oauth import start_oauth, finish_oauth_if_redirected, is_signed_in, build_drive_service, get_user_email, sign_out

# ===== ì•ˆì •í™” í™˜ê²½ ë³€ìˆ˜ =====
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["STREAMLIT_SERVER_ENABLE_WEBSOCKET_COMPRESSION"] = "false"

# ===== í˜ì´ì§€ ì„¤ì •(ìµœì´ˆ 1íšŒ) + OAuth ë¦¬ë‹¤ì´ë ‰íŠ¸ ì™„ë£Œ ì²˜ë¦¬ =====
st.set_page_config(page_title="ë‚˜ì˜ AI ì˜ì–´ êµì‚¬", layout="wide", initial_sidebar_state="collapsed")
finish_oauth_if_redirected()

# ===== ì„¸ì…˜ ìƒíƒœ =====
ss = st.session_state
ss.setdefault("session_id", uuid.uuid4().hex[:12])
ss.setdefault("messages", [])
ss.setdefault("auto_save_chatlog", True)     # OAuth Markdown ì €ì¥
ss.setdefault("save_logs", False)            # ì„œë¹„ìŠ¤ê³„ì • JSONL ì €ì¥(ê³µìœ ë“œë¼ì´ë¸Œ Writer ê¶Œì¥)
ss.setdefault("prep_both_running", False)
ss.setdefault("prep_both_done", ("qe_google" in ss) or ("qe_openai" in ss))
ss.setdefault("prep_cancel_requested", False)
ss.setdefault("index_job", None)

# ===== í—¤ë” =====
load_css(); render_header()
st.info("âœ… prepared í´ë”(ìµœì í™” ìë£Œ)ë§Œ ì¦ë¶„ ì¸ë±ì‹±í•©ë‹ˆë‹¤. ë³€ê²½ ì—†ìœ¼ë©´ ì €ì¥ë³¸ì„ ì¦‰ì‹œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. (ì¤‘ê°„ ì·¨ì†Œ/ì¬ê°œ ì§€ì›)")

# ===== ìƒë‹¨ íƒ­: [ë‘ë‡Œ ì¤€ë¹„] / [ê´€ë¦¬ì ì—…ë¡œë“œ] =====
tab_run, tab_admin = st.tabs(["ğŸ§  ë‘ë‡Œ ì¤€ë¹„ & ëŒ€í™”", "ğŸ› ï¸ ê´€ë¦¬ì ì—…ë¡œë“œ"])

with tab_run:
    # ===== ì‚¬ì´ë“œë°”: ìë™ì €ì¥ + OAuth ë¡œê·¸ì¸ =====
    with st.sidebar:
        ss.auto_save_chatlog = st.toggle("ëŒ€í™” ìë™ ì €ì¥ (OAuth/ë‚´ ë“œë¼ì´ë¸Œ, Markdown)", value=ss.auto_save_chatlog)
        ss.save_logs = st.toggle("ëŒ€í™” JSONL ì €ì¥ (ì„œë¹„ìŠ¤ê³„ì •/chat_log/)", value=ss.save_logs,
                                 help="ê³µìœ ë“œë¼ì´ë¸Œ Writer ê¶Œí•œ í•„ìš”. ì¿¼í„° ë¬¸ì œ ì‹œ ë„ê¸° ê¶Œì¥.")
        st.markdown("---")
        st.markdown("### Google ë¡œê·¸ì¸ (ë‚´ ë“œë¼ì´ë¸Œ ì €ì¥)")
        if not is_signed_in():
            if st.button("ğŸ” Googleë¡œ ë¡œê·¸ì¸"):
                url = start_oauth()
                st.markdown(f"[ì—¬ê¸°ë¥¼ ëˆŒëŸ¬ ë¡œê·¸ì¸í•˜ì„¸ìš”]({url})")
        else:
            st.success(f"ë¡œê·¸ì¸ë¨: {get_user_email() or 'ì•Œ ìˆ˜ ì—†ìŒ'}")
            if st.button("ë¡œê·¸ì•„ì›ƒ"):
                sign_out(); st.rerun()

    # ===== Drive ì—°ê²° í…ŒìŠ¤íŠ¸ =====
    st.markdown("## ğŸ”— Google Drive ì—°ê²° í…ŒìŠ¤íŠ¸")
    st.caption("ì„œë¹„ìŠ¤ê³„ì •ì€ ì¸ë±ì‹±ì€ Readonlyì´ë©´ ì¶©ë¶„í•©ë‹ˆë‹¤. (JSONL ì €ì¥ì€ ê³µìœ ë“œë¼ì´ë¸Œ Writer ê¶Œí•œ í•„ìš”)")
    col1, col2 = st.columns([0.65, 0.35])
    with col1:
        if st.button("í´ë” íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ìµœì‹  10ê°œ)", use_container_width=True):
            ok, msg, rows = preview_drive_files(max_items=10)
            if ok and rows:
                df = pd.DataFrame(rows)
                df["type"] = df["mime"].str.replace("application/vnd.google-apps.", "", regex=False)
                df = df.rename(columns={"modified": "modified_at"})
                df = df[["name", "link", "type", "modified_at"]]
                st.dataframe(
                    df, use_container_width=True, height=360,
                    column_config={
                        "name": st.column_config.TextColumn("íŒŒì¼ëª…"),
                        "link": st.column_config.LinkColumn("open", display_text="ì—´ê¸°"),
                        "type": st.column_config.TextColumn("ìœ í˜•"),
                        "modified_at": st.column_config.TextColumn("ìˆ˜ì •ì‹œê°"),
                    }, hide_index=True
                )
            elif ok:
                st.warning("í´ë”ì— íŒŒì¼ì´ ì—†ê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.error(msg)
    with col2:
        ok, msg = smoke_test_drive()
        st.success(msg) if ok else st.warning(msg)

    # ===== ì¸ë±ì‹± ë³´ê³ ì„œ =====
    rep = st.session_state.get("indexing_report")
    if rep:
        with st.expander("ğŸ§¾ ì¸ë±ì‹± ë³´ê³ ì„œ (ìŠ¤í‚µëœ íŒŒì¼ ë³´ê¸°)", expanded=False):
            st.write(
                f"ì´ íŒŒì¼(ë§¤ë‹ˆí˜ìŠ¤íŠ¸): {rep.get('total_manifest')}, "
                f"ë¡œë”©ëœ ë¬¸ì„œ ìˆ˜: {rep.get('loaded_docs')}, "
                f"ìŠ¤í‚µ: {rep.get('skipped_count')}"
            )
            skipped = rep.get("skipped", [])
            if skipped:
                st.dataframe(pd.DataFrame(skipped), use_container_width=True, hide_index=True)
            else:
                st.caption("ìŠ¤í‚µëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ ğŸ‰")

    # ===== ë‘ë‡Œ ì¤€ë¹„ (ìŠ¤í…/ì·¨ì†Œ/ì¬ê°œ) =====
    st.markdown("---")
    st.subheader("ğŸ§  ë‘ë‡Œ ì¤€ë¹„ â€” ì €ì¥ë³¸ ë¡œë“œ â†” ë³€ê²½ ì‹œ ì¦ë¶„ ì¸ë±ì‹± (ì¤‘ê°„ ì·¨ì†Œ/ì¬ê°œ)")

    c_g, c_o = st.columns(2)
    with c_g:
        st.caption("Gemini ì§„í–‰"); g_bar = st.empty(); g_msg = st.empty()
    with c_o:
        st.caption("ChatGPT ì§„í–‰"); o_bar = st.empty(); o_msg = st.empty()

    def _render_progress(slot_bar, slot_msg, pct: int, msg: str | None = None):
        p = max(0, min(100, int(pct)))
        slot_bar.markdown(f"""
<div class="gp-wrap"><div class="gp-fill" style="width:{p}%"></div><div class="gp-label">{p}%</div></div>
""", unsafe_allow_html=True)
        if msg is not None:
            slot_msg.markdown(f"<div class='gp-msg'>{msg}</div>", unsafe_allow_html=True)

    def _is_cancelled() -> bool:
        return bool(ss.get("prep_cancel_requested", False))

    def run_prepare_both_step():
        # 1) ì„ë² ë”© ê³µê¸‰ì ì„ íƒ/ì„¤ì •
        embed_provider = "openai"
        embed_api = (getattr(settings, "OPENAI_API_KEY", None).get_secret_value()
                     if getattr(settings, "OPENAI_API_KEY", None) else "")
        embed_model = getattr(settings, "OPENAI_EMBED_MODEL", "text-embedding-3-small")
        if not embed_api:
            embed_provider = "google"
            embed_api = settings.GEMINI_API_KEY.get_secret_value()
            embed_model = getattr(settings, "EMBED_MODEL", "text-embedding-004")
        try:
            _render_progress(g_bar, g_msg, 3, f"ì„ë² ë”© ì„¤ì •({embed_provider})")
            _render_progress(o_bar, o_msg, 3, f"ì„ë² ë”© ì„¤ì •({embed_provider})")
            set_embed_provider(embed_provider, embed_api, embed_model)
        except Exception as e:
            _render_progress(g_bar, g_msg, 100, f"ì„ë² ë”© ì‹¤íŒ¨: {e}")
            _render_progress(o_bar, o_msg, 100, f"ì„ë² ë”© ì‹¤íŒ¨: {e}")
            ss.prep_both_running = False
            return

        # 2) ì¸ë±ìŠ¤ ìŠ¤í… ì§„í–‰
        def upd(p, m=None):
            _render_progress(g_bar, g_msg, p, m); _render_progress(o_bar, o_msg, p, m)
        def umsg(m):
            _render_progress(g_bar, g_msg, ss.get("p_shared", 0), m)
            _render_progress(o_bar, o_msg, ss.get("p_shared", 0), m)

        job = ss.get("index_job")
        persist_dir = f"{getattr(settings,'PERSIST_DIR','/tmp/my_ai_teacher/storage_gdrive')}_shared"

        if job is None:
            res = start_index_builder(
                update_pct=upd, update_msg=umsg,
                gdrive_folder_id=settings.GDRIVE_FOLDER_ID,
                raw_sa=settings.GDRIVE_SERVICE_ACCOUNT_JSON,
                persist_dir=persist_dir,
                manifest_path=getattr(settings, "MANIFEST_PATH", "/tmp/my_ai_teacher/drive_manifest.json"),
                max_docs=None,    # ğŸ”¹ë¹ ë¥¸ëª¨ë“œ ì œê±° â†’ ì „ì²´
                is_cancelled=_is_cancelled,
            )
            status = res.get("status")
            if status == "done":
                index = res["index"]
            elif status == "running":
                ss.index_job = res["job"]
                _render_progress(g_bar, g_msg, res.get("pct", 8), res.get("msg", "ì§„í–‰ ì¤‘â€¦"))
                _render_progress(o_bar, o_msg, res.get("pct", 8), res.get("msg", "ì§„í–‰ ì¤‘â€¦"))
                time.sleep(0.2); st.rerun(); return
            else:
                _render_progress(g_bar, g_msg, 100, "ì¸ë±ì‹± ì‹œì‘ ì‹¤íŒ¨")
                _render_progress(o_bar, o_msg, 100, "ì¸ë±ì‹± ì‹œì‘ ì‹¤íŒ¨")
                ss.prep_both_running = False; return
        else:
            res = resume_index_builder(job=job, update_pct=upd, update_msg=umsg,
                                       is_cancelled=_is_cancelled, batch_size=6)
            status = res.get("status")
            if status == "running":
                _render_progress(g_bar, g_msg, res.get("pct", 8), res.get("msg", "ì§„í–‰ ì¤‘â€¦"))
                _render_progress(o_bar, o_msg, res.get("pct", 8), res.get("msg", "ì§„í–‰ ì¤‘â€¦"))
                time.sleep(0.15); st.rerun(); return
            elif status == "cancelled":
                ss.prep_both_running = False; ss.prep_cancel_requested = False; ss.index_job = None
                _render_progress(g_bar, g_msg, ss.get("p_shared", 0), "ì‚¬ìš©ì ì·¨ì†Œ")
                _render_progress(o_bar, o_msg, ss.get("p_shared", 0), "ì‚¬ìš©ì ì·¨ì†Œ")
                return
            elif status == "done":
                index = res["index"]
                ss.index_job = None
            else:
                _render_progress(g_bar, g_msg, 100, "ì¸ë±ì‹± ì‹¤íŒ¨")
                _render_progress(o_bar, o_msg, 100, "ì¸ë±ì‹± ì‹¤íŒ¨")
                ss.prep_both_running = False; return

        # 3) LLM 2ê°œ ì¤€ë¹„ + QE ìƒì„±
        try:
            g_llm = make_llm("google", settings.GEMINI_API_KEY.get_secret_value(),
                             getattr(settings, "LLM_MODEL", "gemini-1.5-pro"),
                             float(ss.get("temperature", 0.0)))
            ss["llm_google"] = g_llm
            ss["qe_google"] = index.as_query_engine(
                llm=g_llm,
                response_mode=ss.get("response_mode", getattr(settings, "RESPONSE_MODE", "compact")),
                similarity_top_k=int(ss.get("similarity_top_k", getattr(settings, "SIMILARITY_TOP_K", 5))),
            )
            _render_progress(g_bar, g_msg, 100, "ì™„ë£Œ!")
        except Exception as e:
            _render_progress(g_bar, g_msg, 100, f"Gemini ì¤€ë¹„ ì‹¤íŒ¨: {e}")

        try:
            if getattr(settings, "OPENAI_API_KEY", None) and settings.OPENAI_API_KEY.get_secret_value():
                o_llm = make_llm("openai", settings.OPENAI_API_KEY.get_secret_value(),
                                 getattr(settings, "OPENAI_LLM_MODEL", "gpt-4o-mini"),
                                 float(ss.get("temperature", 0.0)))
                ss["llm_openai"] = o_llm
                ss["qe_openai"] = index.as_query_engine(
                    llm=o_llm,
                    response_mode=ss.get("response_mode", getattr(settings, "RESPONSE_MODE", "compact")),
                    similarity_top_k=int(ss.get("similarity_top_k", getattr(settings, "SIMILARITY_TOP_K", 5))),
                )
                _render_progress(o_bar, o_msg, 100, "ì™„ë£Œ!")
            else:
                _render_progress(o_bar, o_msg, 100, "í‚¤ ëˆ„ë½ â€” OPENAI_API_KEY í•„ìš”")
        except Exception as e:
            _render_progress(o_bar, o_msg, 100, f"ChatGPT ì¤€ë¹„ ì‹¤íŒ¨: {e}")

        ss.prep_both_running = False
        ss.prep_both_done = True
        time.sleep(0.2); st.rerun()

    # ì‹¤í–‰/ì·¨ì†Œ ë²„íŠ¼
    left, right = st.columns([0.7, 0.3])
    with left:
        clicked = st.button("ğŸš€ í•œ ë²ˆì— ì¤€ë¹„í•˜ê¸°", key="prepare_both", use_container_width=True,
                            disabled=ss.prep_both_running or ss.prep_both_done)
    with right:
        cancel_clicked = st.button("â›” ì¤€ë¹„ ì·¨ì†Œ", key="cancel_prepare", use_container_width=True, type="secondary",
                                   disabled=not ss.prep_both_running)

    if cancel_clicked and ss.prep_both_running:
        ss.prep_cancel_requested = True
        if ss.get("index_job"):
            cancel_index_builder(ss.index_job)
        st.rerun()

    if clicked and not (ss.prep_both_running or ss.prep_both_done):
        ss.prep_cancel_requested = False
        ss.prep_both_running = True
        ss.index_job = None
        st.rerun()

    if ss.prep_both_running:
        run_prepare_both_step()

    st.caption("ì¤€ë¹„ ë²„íŠ¼ì„ ë‹¤ì‹œ í™œì„±í™”í•˜ë ¤ë©´ ì•„ë˜ ì¬ì„¤ì • ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
    if st.button("ğŸ”§ ì¬ì„¤ì •(ë²„íŠ¼ ë‹¤ì‹œ í™œì„±í™”)", disabled=not ss.prep_both_done):
        ss.prep_both_done = False
        st.rerun()

    # ===== ëŒ€í™” UI (ê·¸ë£¹í† ë¡ ) =====
    st.markdown("---")
    st.subheader("ğŸ’¬ ê·¸ë£¹í† ë¡  â€” í•™ìƒ â†” ğŸ¤–Gemini(ì¹œì ˆ/ê¼¼ê¼¼) â†” ğŸ¤–ChatGPT(ìœ ë¨¸ëŸ¬ìŠ¤/ë³´ì™„)")

    ready_google = "qe_google" in ss
    ready_openai = "qe_openai" in ss

    if not ready_google:
        st.info("ë¨¼ì € **[ğŸš€ í•œ ë²ˆì— ì¤€ë¹„í•˜ê¸°]** ë¥¼ í´ë¦­í•´ ë‘ë‡Œë¥¼ ì¤€ë¹„í•˜ì„¸ìš”. (OpenAI í‚¤ê°€ ì—†ìœ¼ë©´ Geminië§Œ ì‘ë‹µ)")
        st.stop()

    # ë Œë” ì´ì „ ë©”ì‹œì§€
    for m in ss.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    def _strip_sources(text: str) -> str:
        return re.sub(r"\n+---\n\*ì°¸ê³  ìë£Œ:.*$", "", text, flags=re.DOTALL)

    def _build_context_for_models(messages: list[dict], limit_pairs: int = 2, max_chars: int = 2000) -> str:
        pairs, buf_user = [], None
        for m in reversed(messages):
            role, content = m.get("role"), str(m.get("content", "")).strip()
            if role == "assistant":
                content = re.sub(r"^\*\*ğŸ¤– .*?\*\*\s*\n+", "", content).strip()
                if buf_user is not None:
                    pairs.append((buf_user, content)); buf_user = None
                    if len(pairs) >= limit_pairs: break
            elif role == "user" and buf_user is None:
                buf_user = content
        pairs = list(reversed(pairs))
        blocks = [f"[í•™ìƒ]\n{u}\n\n[êµì‚¬]\n{a}" for u, a in pairs]
        ctx = "\n\n---\n\n".join(blocks).strip()
        return ctx[-max_chars:] if len(ctx) > max_chars else ctx

    # í˜ë¥´ì†Œë‚˜ í•©ì„±
    def _persona():
        mode = st.session_state.get("mode_select", "ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…")
        base = EXPLAINER_PROMPT if mode == "ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…" else (ANALYST_PROMPT if mode == "ğŸ” êµ¬ë¬¸ ë¶„ì„" else READER_PROMPT)
        common = (
            "ì—­í• : í•™ìƒì˜ ì˜ì–´ ì‹¤ë ¥ì„ ë•ëŠ” AI êµì‚¬.\n"
            "ê·œì¹™: ê·¼ê±°ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ ê·¸ ì‚¬ì‹¤ì„ ëª…í™•íˆ ë°íŒë‹¤. ì˜ˆì‹œ ë¬¸ì¥ì€ ì§§ê³  ì ì§„ì ìœ¼ë¡œ.\n"
        )
        return base + "\n" + common

    GEMINI_STYLE = (
        "ë‹¹ì‹ ì€ ì°©í•˜ê³  ë˜‘ë˜‘í•œ ì¹œêµ¬ ê°™ì€ êµì‚¬ì…ë‹ˆë‹¤. ì§€ë‚˜ì¹˜ê²Œ ì–´ë µê²Œ ë§í•˜ì§€ ë§ê³ , "
        "ì¹­ì°¬ê³¼ ê²©ë ¤ë¥¼ ê³ë“¤ì—¬ ì°¨ë¶„íˆ ì•ˆë‚´í•˜ì„¸ìš”. í•µì‹¬ ê·œì¹™ì€ ì •í™•ì„±ì…ë‹ˆë‹¤."
    )
    CHATGPT_REVIEW_STYLE = (
        "ë‹¹ì‹ ì€ ìœ ë¨¸ëŸ¬ìŠ¤í•˜ì§€ë§Œ ì •í™•í•œ ë™ë£Œ êµì‚¬ì…ë‹ˆë‹¤. ë™ë£Œ(Gemini)ì˜ ë‹µì„ ì½ê³  "
        "ë¹ ì§„ ë¶€ë¶„ì„ ë³´ì™„/êµì •í•˜ê³ , ë§ˆì§€ë§‰ì— <ìµœì¢… ì •ë¦¬>ë¥¼ ì œì‹œí•˜ì„¸ìš”. ê³¼í•œ ë†ë‹´ì€ í”¼í•˜ê³ , "
        "ì§§ê³  ëª…ë£Œí•œ ìœ ë¨¸ í•œë‘ ì¤„ë§Œ í—ˆìš©ë©ë‹ˆë‹¤."
    )

    mode = st.radio("í•™ìŠµ ëª¨ë“œ", ["ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…", "ğŸ” êµ¬ë¬¸ ë¶„ì„", "ğŸ“š ë…í•´ ë° ìš”ì•½"],
                    horizontal=True, key="mode_select")

    # ì„œë¹„ìŠ¤ê³„ì • JSONL ë¡œê·¸ ì €ì¥
    def _log_try(items):
        if not ss.save_logs:
            return
        try:
            parent_id = (getattr(settings, "CHATLOG_FOLDER_ID", None) or settings.GDRIVE_FOLDER_ID)
            sa = settings.GDRIVE_SERVICE_ACCOUNT_JSON
            if isinstance(sa, str):
                try: sa = json.loads(sa)
                except Exception: pass
            sub_id = get_chatlog_folder_id(parent_folder_id=parent_id, sa_json=sa)
            chat_store.append_jsonl(folder_id=sub_id, sa_json=sa, items=items)
            st.toast("ëŒ€í™” JSONL ì €ì¥ ì™„ë£Œ", icon="ğŸ’¾")
        except Exception as e:
            st.warning(f"ëŒ€í™” JSONL ì €ì¥ ì‹¤íŒ¨: {e}")

    # ì…ë ¥ì°½
    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ë¶„ì„/ìš”ì•½í•  ë¬¸ì¥ì´ë‚˜ ê¸€ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")
    if user_input:
        ss.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)

        _log_try([chat_store.make_entry(ss.session_id, "user", "user", user_input, mode, model="user")])

        # 1) Gemini 1ì°¨ (RAG)
        with st.spinner("ğŸ¤– Gemini ì„ ìƒë‹˜ì´ ë¨¼ì € ë‹µë³€í•©ë‹ˆë‹¤â€¦"):
            prev_ctx = _build_context_for_models(ss.messages[:-1], limit_pairs=2, max_chars=2000)
            gemini_query = (f"[ì´ì „ ëŒ€í™”]\n{prev_ctx}\n\n" if prev_ctx else "") + f"[í•™ìƒ ì§ˆë¬¸]\n{user_input}"
            ans_g = get_text_answer(ss["qe_google"], gemini_query, _persona() + "\n" + GEMINI_STYLE)

        content_g = f"**ğŸ¤– Gemini**\n\n{ans_g}"
        ss.messages.append({"role": "assistant", "content": content_g})
        with st.chat_message("assistant"): st.markdown(content_g)

        _log_try([chat_store.make_entry(
            ss.session_id, "assistant", "Gemini", content_g, mode,
            model=getattr(settings, "LLM_MODEL", "gemini")
        )])

        # 2) ChatGPT ë³´ì™„(ë™ì¼ ì¸ë±ìŠ¤ ì‚¬ìš© RAG)
        if "qe_openai" in ss:
            prev_ctx_all = _build_context_for_models(ss.messages, limit_pairs=2, max_chars=2000)
            augmented = (
                (f"[ì´ì „ ëŒ€í™”]\n{prev_ctx_all}\n\n" if prev_ctx_all else "") +
                f"[í•™ìƒ ì§ˆë¬¸]\n{user_input}\n\n"
                f"[ë™ë£Œì˜ 1ì°¨ ë‹µë³€(Gemini)]\n{_strip_sources(ans_g)}\n\n"
                f"[ë‹¹ì‹ ì˜ ì‘ì—…]\nìœ„ ê¸°ì¤€ìœ¼ë¡œë§Œ ë³´ì™„/ê²€ì¦í•˜ë¼."
            )
            with st.spinner("ğŸ¤ ChatGPT ì„ ìƒë‹˜ì´ ë³´ì™„/ê²€ì¦ ì¤‘â€¦"):
                ans_o = get_text_answer(ss["qe_openai"], augmented, _persona() + "\n" + CHATGPT_REVIEW_STYLE)
            content_o = f"**ğŸ¤– ChatGPT**\n\n{ans_o}"
            ss.messages.append({"role": "assistant", "content": content_o})
            with st.chat_message("assistant"): st.markdown(content_o)
        else:
            with st.chat_message("assistant"):
                st.info("ChatGPT í‚¤ê°€ ì—†ì–´ Geminië§Œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤. OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ë©´ ë³´ì™„/ê²€ì¦ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

        # OAuth Markdown ìë™ ì €ì¥
        if ss.auto_save_chatlog and ss.messages:
            try:
                if is_signed_in():
                    svc = build_drive_service()
                    parent_id = (st.secrets.get("OAUTH_CHAT_PARENT_ID") or "").strip() or None
                    _fid = save_chatlog_markdown_oauth(ss.session_id, ss.messages, svc, parent_id)
                    st.toast("ë‚´ ë“œë¼ì´ë¸Œì— ëŒ€í™” ì €ì¥ ì™„ë£Œ âœ…", icon="ğŸ’¾")
                else:
                    st.info("êµ¬ê¸€ ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸í•˜ë©´ ëŒ€í™”ê°€ **ë‚´ ë“œë¼ì´ë¸Œ**ì— ì €ì¥ë©ë‹ˆë‹¤.")
            except Exception as e:
                st.warning(f"OAuth ì €ì¥ ì‹¤íŒ¨: {e}")

        st.rerun()

with tab_admin:
    st.subheader("ğŸ› ï¸ ê´€ë¦¬ì ì—…ë¡œë“œ â€” PDF/TXTë¥¼ RAG-Ready Markdownìœ¼ë¡œ ë³€í™˜í•˜ì—¬ prepared í´ë”ì— ì—…ë¡œë“œ")
    st.caption("â€¢ ê¸¸ì´ ì œí•œì„ ì¤„ì´ê³  ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´, MD ìµœì í™”ë³¸ë§Œ ì¸ë±ì‹±í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤.")
    uploaded = st.file_uploader("íŒŒì¼ ì„ íƒ (PDF ë˜ëŠ” TXT)", type=["pdf", "txt"], accept_multiple_files=False)

    # ê°„ë‹¨ ë³€í™˜ê¸°: PDF â†’ í…ìŠ¤íŠ¸ (pypdf), TXT â†’ ê·¸ëŒ€ë¡œ, ê³µí†µ ë§ˆí¬ë‹¤ìš´ í¬ë§·
    def _pdf_to_text(file_bytes: bytes) -> str:
        try:
            import pypdf
            from io import BytesIO
            reader = pypdf.PdfReader(BytesIO(file_bytes))
            texts = []
            for page in reader.pages:
                texts.append(page.extract_text() or "")
            return "\n".join(texts)
        except Exception as e:
            return f"[PDF ì¶”ì¶œ ì‹¤íŒ¨] {e}"

    def _to_markdown(raw_text: str, title: str, tag: str) -> str:
        md = []
        md.append(f"# {title}")
        md.append("")
        md.append(f"> source_tag: {tag}")
        md.append("> type: prepared")
        md.append("")
        md.append("## TL;DR")
        md.append("- (í•µì‹¬ ê·œì¹™ì„ 2~4ì¤„ë¡œ ìš”ì•½)")
        md.append("")
        md.append("## í•µì‹¬ ê·œì¹™")
        md.append("- ê·œì¹™1")
        md.append("- ê·œì¹™2")
        md.append("")
        md.append("## ì˜ˆë¬¸ (EN â†’ KO)")
        md.append("- Example 1 â€” ì„¤ëª…")
        md.append("- Example 2 â€” ì„¤ëª…")
        md.append("")
        md.append("## ë³¸ë¬¸")
        md.append(raw_text.strip())
        return "\n".join(md)

    if uploaded is not None:
        base = os.path.splitext(uploaded.name)[0]
        title = st.text_input("ë¬¸ì„œ ì œëª©", value=base)
        tag = st.text_input("ë¬¸ë²• íƒœê·¸(ì˜ˆ: í˜•ìš©ì‚¬ì ˆ, ê´€ê³„ëŒ€ëª…ì‚¬ ë“±)", value="")
        if st.button("âœ… ë³€í™˜ & ì—…ë¡œë“œ(ì¤€ë¹„ í´ë”)", use_container_width=True):
            raw = uploaded.read()
            if uploaded.type == "application/pdf":
                text = _pdf_to_text(raw)
            else:
                text = raw.decode("utf-8", errors="ignore")
            md = _to_markdown(text, title=title or base, tag=tag)

            # OAuthë¡œ prepared í´ë” ì—…ë¡œë“œ
            try:
                if not is_signed_in():
                    st.error("ë¨¼ì € ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ Google ë¡œê·¸ì¸ í•´ì£¼ì„¸ìš”.")
                else:
                    svc = build_drive_service()
                    # prepared í´ë” ë³´ì¥ (my-ai-teacher-data/prepared)
                    from googleapiclient.http import MediaInMemoryUpload
                    def _ensure_folder(service, name: str, parent_id: str | None):
                        q = f"name='{name}' and mimeType='application/vnd.google-apps.folder' and trashed=false"
                        if parent_id: q += f" and '{parent_id}' in parents"
                        res = service.files().list(q=q, fields="files(id,name)", includeItemsFromAllDrives=True, supportsAllDrives=True).execute()
                        files = res.get("files", [])
                        if files: return files[0]["id"]
                        meta = {"name": name, "mimeType": "application/vnd.google-apps.folder"}
                        if parent_id: meta["parents"] = [parent_id]
                        created = service.files().create(body=meta, fields="id", supportsAllDrives=True).execute()
                        return created["id"]

                    parent_id = (st.secrets.get("OAUTH_PREPARED_PARENT_ID") or "").strip() or None
                    root = _ensure_folder(svc, "my-ai-teacher-data", parent_id)
                    prepared = _ensure_folder(svc, "prepared", root)

                    media = MediaInMemoryUpload(md.encode("utf-8"), mimetype="text/markdown", resumable=False)
                    meta = {"name": f"{title or base}.md", "parents": [prepared]}
                    created = svc.files().create(body=meta, media_body=media, fields="id,webViewLink", supportsAllDrives=True).execute()
                    st.success(f"ì—…ë¡œë“œ ì™„ë£Œ! íŒŒì¼ ID: {created['id']}")
                    st.caption("âš ï¸ ì¸ë±ì‹± ëŒ€ìƒ í´ë”(GDRIVE_FOLDER_ID)ê°€ prepared í´ë”ì˜ IDì¸ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”.")
            except Exception as e:
                st.error(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
