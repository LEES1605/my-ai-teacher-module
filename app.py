# app.py â€” ê·¸ë£¹í† ë¡ (í•­ìƒ ON) + ì¸ë±ì‹± ì¬ê°œ(Resume) + ì¤€ë¹„ ì·¨ì†Œ/ì¢…ë£Œ
#          + Google Drive ëŒ€í™” ë¡œê·¸(.jsonl / Markdown) ì €ì¥

# ===== Imports (osë¥¼ ë¨¼ì € ê°€ì ¸ì™€ì•¼ os.environ ì‚¬ìš© ê°€ëŠ¥) =====
import os
import time
import uuid
import re
import pandas as pd
import streamlit as st

# Google Drive Markdown ë¡œê·¸ ì €ì¥ ìœ í‹¸
from src.drive_log import save_chatlog_markdown, get_chatlog_folder_id

# ê¸°ë³¸ UI
from src.ui import load_css, render_header

# ===== í™˜ê²½ ë³€ìˆ˜ ì„¤ì •: ëŸ°íƒ€ì„ ì•ˆì •í™” =====
os.environ["STREAMLIT_SERVER_FILE_WATCHER_TYPE"] = "none"
# os.environ["STREAMLIT_RUN_ON_SAVE"] = "false"  # Streamlit 1.48+ íì§€
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["STREAMLIT_SERVER_ENABLE_WEBSOCKET_COMPRESSION"] = "false"

# ===== Streamlit í˜ì´ì§€ ì„¤ì • (ì²« í˜¸ì¶œë§Œ í—ˆìš©) =====
st.set_page_config(
    page_title="ë‚˜ì˜ AI ì˜ì–´ êµì‚¬",
    layout="wide",
    initial_sidebar_state="collapsed",
)

# ===== ì„¸ì…˜ ìƒíƒœ =====
ss = st.session_state
ss.setdefault("session_id", uuid.uuid4().hex[:12])   # ëŒ€í™” ì„¸ì…˜ ID
ss.setdefault("messages", [])                        # ì±„íŒ… ë©”ì‹œì§€ ë Œë”ìš©
ss.setdefault("chat_history", [])                    # (ì˜ˆë¹„) ë³„ë„ íˆìŠ¤í† ë¦¬
ss.setdefault("auto_save_chatlog", True)             # Markdown ìë™ ì €ì¥
ss.setdefault("prep_both_running", False)
ss.setdefault("prep_both_done", ("qe_google" in ss) or ("qe_openai" in ss))
ss.setdefault("p_shared", 0)
ss.setdefault("prep_cancel_requested", False)
ss.setdefault("session_terminated", False)
ss.setdefault("save_logs", True)                     # JSONL ì €ì¥ ON/OFF

# ===== ê¸°ë³¸ UI/ìŠ¤íƒ€ì¼ =====
load_css()
render_header()
st.info("âœ… ì¸ë±ì‹±ì€ 1ë²ˆë§Œ ìˆ˜í–‰í•˜ê³ , ê·¸ ì¸ë±ìŠ¤ë¡œ Gemini/ChatGPT ë‘ LLMì„ ì¤€ë¹„í•©ë‹ˆë‹¤. (ë¹ ë¥¸ ëª¨ë“œÂ·ë˜ëŒë¦¼ ë°©ì§€Â·ResumeÂ·í•­ìƒ ğŸ‘¥ ê·¸ë£¹í† ë¡ )")

# ===== ì‚¬ì´ë“œë°” =====
with st.sidebar:
    ss.auto_save_chatlog = st.toggle("ëŒ€í™” ìë™ ì €ì¥(Drive)", value=ss.auto_save_chatlog)

# ===== Google Drive ì—°ê²° í…ŒìŠ¤íŠ¸ =====
try:
    from src.rag_engine import smoke_test_drive, preview_drive_files
except Exception:
    st.error("`src.rag_engine` ì„í¬íŠ¸ ì‹¤íŒ¨")
    import traceback
    import os as _os
    st.write("íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:", _os.path.exists("src/rag_engine.py"))
    with st.expander("ì„í¬íŠ¸ ìŠ¤íƒ", expanded=True):
        st.code(traceback.format_exc())
    st.stop()

st.markdown("## ğŸ”— Google Drive ì—°ê²° í…ŒìŠ¤íŠ¸")
st.caption("ì„œë¹„ìŠ¤ê³„ì •ì— í´ë” â€˜ì“°ê¸°(Writer)â€™ ê¶Œí•œì´ ìˆì–´ì•¼ ëŒ€í™” ë¡œê·¸ ì €ì¥ì´ ë©ë‹ˆë‹¤.")

col1, col2 = st.columns([0.65, 0.35])
with col1:
    if st.button("í´ë” íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ìµœì‹  10ê°œ)", use_container_width=True):
        ok, msg, rows = preview_drive_files(max_items=10)
        if ok and rows:
            df = pd.DataFrame(rows)
            df["type"] = df["mime"].str.replace("application/vnd.google-apps.", "", regex=False)
            df = df.rename(columns={"modified": "modified_at"})
            df = df[["name", "link", "type", "modified_at"]]
            st.dataframe(
                df, use_container_width=True, height=360,
                column_config={
                    "name": st.column_config.TextColumn("íŒŒì¼ëª…"),
                    "link": st.column_config.LinkColumn("open", display_text="ì—´ê¸°"),
                    "type": st.column_config.TextColumn("ìœ í˜•"),
                    "modified_at": st.column_config.TextColumn("ìˆ˜ì •ì‹œê°"),
                }, hide_index=True
            )
        elif ok:
            st.warning("í´ë”ì— íŒŒì¼ì´ ì—†ê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error(msg)
with col2:
    ok, msg = smoke_test_drive()
    if ok:
        st.success(msg)
    else:
        st.warning(msg)

# ===== ë‘ë‡Œ ì¤€ë¹„ (ê³µí†µ ì¸ë±ìŠ¤ + LLM 2ê°œ) =====
st.markdown("----")
st.subheader("ğŸ§  ë‘ë‡Œ ì¤€ë¹„ â€” ì¸ë±ìŠ¤ 1íšŒ + Gemini/ChatGPT")

from src.config import settings
try:
    from src.rag_engine import (
        set_embed_provider, make_llm, get_or_build_index, get_text_answer, CancelledError
    )
except Exception:
    st.error("`src.rag_engine` ì„í¬íŠ¸(LLM/RAG) ì‹¤íŒ¨")
    import traceback
    with st.expander("ì„í¬íŠ¸ ìŠ¤íƒ", expanded=True):
        st.code(traceback.format_exc())
    st.stop()

# ==== (ì„ íƒ) ì¸ë±ì‹± ë³´ê³ ì„œ í‘œì‹œ ====
rep = st.session_state.get("indexing_report")
if rep:
    with st.expander("ğŸ§¾ ì¸ë±ì‹± ë³´ê³ ì„œ(ìŠ¤í‚µëœ íŒŒì¼ ë³´ê¸°)", expanded=False):
        st.write(f"ì´ íŒŒì¼(ë§¤ë‹ˆí˜ìŠ¤íŠ¸): {rep.get('total_manifest')}, "
                 f"ë¡œë”©ëœ ë¬¸ì„œ ìˆ˜: {rep.get('loaded_docs')}, "
                 f"ìŠ¤í‚µ: {rep.get('skipped_count')}")
        skipped = rep.get("skipped", [])
        if skipped:
            import pandas as _pd
            st.dataframe(_pd.DataFrame(skipped), use_container_width=True, hide_index=True)
        else:
            st.caption("ìŠ¤í‚µëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤ ğŸ‰")

# â–¶ ëŒ€í™” ë¡œê·¸ ì €ì¥ ëª¨ë“ˆ(JSONL)
from src import chat_store

# â–¶ í•™ìŠµ ëª¨ë“œ(í˜ë¥´ì†Œë‚˜)
from src.prompts import EXPLAINER_PROMPT, ANALYST_PROMPT, READER_PROMPT
mode = st.radio(
    "í•™ìŠµ ëª¨ë“œ", ["ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…", "ğŸ” êµ¬ë¬¸ ë¶„ì„", "ğŸ“š ë…í•´ ë° ìš”ì•½"],
    horizontal=True, key="mode_select"
)
def _persona():
    return EXPLAINER_PROMPT if mode == "ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…" else (ANALYST_PROMPT if mode == "ğŸ” êµ¬ë¬¸ ë¶„ì„" else READER_PROMPT)

# â–¶ ë¡œê·¸ ì €ì¥ ëŒ€ìƒ í´ë”
CHAT_FOLDER_ID = getattr(settings, "CHATLOG_FOLDER_ID", None) or settings.GDRIVE_FOLDER_ID

# â–¶ â€˜ì„¸ì…˜ ì¢…ë£Œâ€™ ë²„íŠ¼
with st.container():
    colx, coly = st.columns([0.72, 0.28])
    with coly:
        if st.button("â¹ ì„¸ì…˜ ì¢…ë£Œ", use_container_width=True, type="secondary"):
            ss.session_terminated = True
            ss.prep_both_running = False
            ss.prep_cancel_requested = False
            st.warning("ì„¸ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨(Ctrl/âŒ˜+Shift+R)ìœ¼ë¡œ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”.")
            st.stop()

# â–¶ ì§„í–‰ë¥  UI
def _render_progress(slot_bar, slot_msg, pct: int, msg: str | None = None):
    p = max(0, min(100, int(pct)))
    slot_bar.markdown(
        f"""
<div class="gp-wrap"><div class="gp-fill" style="width:{p}%"></div><div class="gp-label">{p}%</div></div>
""",
        unsafe_allow_html=True,
    )
    if msg is not None:
        slot_msg.markdown(f"<div class='gp-msg'>{msg}</div>", unsafe_allow_html=True)

def _bump_max(key: str, pct: int) -> int:
    now = int(pct)
    prev = int(ss.get(key, 0))
    if now < prev:
        now = prev
    ss[key] = now
    return now

# â–¶ ì˜µì…˜(ë¹ ë¥¸ ëª¨ë“œ)
with st.expander("âš™ï¸ ì˜µì…˜", expanded=False):
    fast = st.checkbox(
        "âš¡ ë¹ ë¥¸ ì¤€ë¹„ (ì²˜ìŒ Nê°œ ë¬¸ì„œë§Œ ì¸ë±ì‹±)", value=True,
        disabled=ss.prep_both_running or ss.prep_both_done
    )
    max_docs = st.number_input(
        "N (ë¹ ë¥¸ ëª¨ë“œì¼ ë•Œë§Œ)", min_value=5, max_value=500, value=40, step=5,
        disabled=ss.prep_both_running or ss.prep_both_done
    )
    ss.save_logs = st.checkbox(
        "ğŸ’¾ ëŒ€í™” ë¡œê·¸ë¥¼ Google Driveì— ì €ì¥í•˜ê¸°", value=ss.save_logs,
        help="Writer ê¶Œí•œ í•„ìš”. ì¼ìë³„ chat_log_YYYY-MM-DD.jsonl ë¡œ ì €ì¥ë©ë‹ˆë‹¤.",
        disabled=False
    )

st.markdown("### ğŸš€ ì¸ë±ì‹± 1ë²ˆ + ë‘ LLM ì¤€ë¹„")
c_g, c_o = st.columns(2)
with c_g:
    st.caption("Gemini ì§„í–‰")
    g_bar = st.empty()
    g_msg = st.empty()
with c_o:
    st.caption("ChatGPT ì§„í–‰")
    o_bar = st.empty()
    o_msg = st.empty()

def _is_cancelled() -> bool:
    return bool(ss.get("prep_cancel_requested", False))

def run_prepare_both():
    _render_progress(g_bar, g_msg, ss.p_shared, "ëŒ€ê¸° ì¤‘â€¦")
    _render_progress(o_bar, o_msg, ss.p_shared, "ëŒ€ê¸° ì¤‘â€¦")

    # 1) ì„ë² ë”© ê³µê¸‰ì ì„ íƒ
    embed_provider = "openai"
    embed_api = getattr(settings, "OPENAI_API_KEY", None).get_secret_value() if hasattr(settings, "OPENAI_API_KEY") else ""
    embed_model = getattr(settings, "OPENAI_EMBED_MODEL", "text-embedding-3-small")
    if not embed_api:
        embed_provider = "google"
        embed_api = settings.GEMINI_API_KEY.get_secret_value()
        embed_model = getattr(settings, "EMBED_MODEL", "text-embedding-004")
    persist_dir = f"{getattr(settings, 'PERSIST_DIR', '/tmp/my_ai_teacher/storage_gdrive')}_shared"

    # 2) ì„ë² ë”© ì„¤ì •
    try:
        if _is_cancelled():
            raise CancelledError("ì‚¬ìš©ì ì·¨ì†Œ")
        p = _bump_max("p_shared", 5)
        _render_progress(g_bar, g_msg, p, f"ì„ë² ë”© ì„¤ì •({embed_provider})")
        _render_progress(o_bar, o_msg, p, f"ì„ë² ë”© ì„¤ì •({embed_provider})")
        set_embed_provider(embed_provider, embed_api, embed_model)
    except CancelledError:
        ss.prep_both_running = False
        ss.prep_cancel_requested = False
        _render_progress(g_bar, g_msg, ss.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        _render_progress(o_bar, o_msg, ss.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        st.stop()
    except Exception as e:
        p = _bump_max("p_shared", 100)
        _render_progress(g_bar, g_msg, p, f"ì„ë² ë”© ì‹¤íŒ¨: {e}")
        _render_progress(o_bar, o_msg, p, f"ì„ë² ë”© ì‹¤íŒ¨: {e}")
        ss.prep_both_running = False
        st.stop()

    # 3) ì¸ë±ìŠ¤(Resume ì§€ì›)
    try:
        def upd(pct: int, msg: str | None = None):
            if _is_cancelled():
                raise CancelledError("ì‚¬ìš©ì ì·¨ì†Œ(ì§„í–‰ ì¤‘)")
            p = _bump_max("p_shared", pct)
            _render_progress(g_bar, g_msg, p, msg)
            _render_progress(o_bar, o_msg, p, msg)

        def umsg(m: str):
            p = ss.p_shared
            _render_progress(g_bar, g_msg, p, m)
            _render_progress(o_bar, o_msg, p, m)

        index = get_or_build_index(
            update_pct=upd, update_msg=umsg,
            gdrive_folder_id=settings.GDRIVE_FOLDER_ID,
            raw_sa=settings.GDRIVE_SERVICE_ACCOUNT_JSON,
            persist_dir=persist_dir,
            manifest_path=getattr(settings, "MANIFEST_PATH", "/tmp/my_ai_teacher/drive_manifest.json"),
            max_docs=(max_docs if fast else None),
            is_cancelled=_is_cancelled,   # ì¬ê°œ/ì·¨ì†Œ ì§€ì›
        )
    except CancelledError:
        ss.prep_both_running = False
        ss.prep_cancel_requested = False
        _render_progress(g_bar, g_msg, ss.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        _render_progress(o_bar, o_msg, ss.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        st.stop()
    except Exception as e:
        p = _bump_max("p_shared", 100)
        _render_progress(g_bar, g_msg, p, f"ì¸ë±ìŠ¤ ì‹¤íŒ¨: {e}")
        _render_progress(o_bar, o_msg, p, f"ì¸ë±ìŠ¤ ì‹¤íŒ¨: {e}")
        ss.prep_both_running = False
        st.stop()

    # 4) LLM ë‘ ê°œ ì¤€ë¹„ (llm ê°ì²´ë„ ë³´ê´€)
    try:
        g_llm = make_llm(
            "google",
            settings.GEMINI_API_KEY.get_secret_value(),
            getattr(settings, "LLM_MODEL", "gemini-1.5-pro"),
            float(ss.get("temperature", 0.0)),
        )
        ss["llm_google"] = g_llm
        ss["qe_google"] = index.as_query_engine(
            llm=g_llm,
            response_mode=ss.get("response_mode", getattr(settings, "RESPONSE_MODE", "compact")),
            similarity_top_k=int(ss.get("similarity_top_k", getattr(settings, "SIMILARITY_TOP_K", 5))),
        )
        _render_progress(g_bar, g_msg, 100, "ì™„ë£Œ!")
    except Exception as e:
        _render_progress(g_bar, g_msg, 100, f"Gemini ì¤€ë¹„ ì‹¤íŒ¨: {e}")

    try:
        if hasattr(settings, "OPENAI_API_KEY") and settings.OPENAI_API_KEY.get_secret_value():
            if _is_cancelled():
                raise CancelledError("ì‚¬ìš©ì ì·¨ì†Œ")
            o_llm = make_llm(
                "openai",
                settings.OPENAI_API_KEY.get_secret_value(),
                getattr(settings, "OPENAI_LLM_MODEL", "gpt-4o-mini"),
                float(ss.get("temperature", 0.0)),
            )
            ss["llm_openai"] = o_llm
            ss["qe_openai"] = index.as_query_engine(
                llm=o_llm,
                response_mode=ss.get("response_mode", getattr(settings, "RESPONSE_MODE", "compact")),
                similarity_top_k=int(ss.get("similarity_top_k", getattr(settings, "SIMILARITY_TOP_K", 5))),
            )
            _render_progress(o_bar, o_msg, 100, "ì™„ë£Œ!")
        else:
            _render_progress(o_bar, o_msg, 100, "í‚¤ ëˆ„ë½ â€” OPENAI_API_KEY í•„ìš”")
    except CancelledError:
        ss.prep_both_running = False
        ss.prep_cancel_requested = False
        _render_progress(o_bar, o_msg, ss.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        st.stop()
    except Exception as e:
        _render_progress(o_bar, o_msg, 100, f"ChatGPT ì¤€ë¹„ ì‹¤íŒ¨: {e}")

    ss.prep_both_running = False
    ss.prep_both_done = True
    time.sleep(0.2)
    st.rerun()

# â–¶ ì‹¤í–‰/ì·¨ì†Œ ë²„íŠ¼
left, right = st.columns([0.7, 0.3])
with left:
    clicked = st.button(
        "ğŸš€ í•œ ë²ˆì— ì¤€ë¹„í•˜ê¸°", key="prepare_both", use_container_width=True,
        disabled=ss.prep_both_running or ss.prep_both_done
    )
with right:
    cancel_clicked = st.button(
        "â›” ì¤€ë¹„ ì·¨ì†Œ", key="cancel_prepare", use_container_width=True, type="secondary",
        disabled=not ss.prep_both_running
    )

if cancel_clicked and ss.prep_both_running:
    ss.prep_cancel_requested = True
    st.rerun()

if clicked and not (ss.prep_both_running or ss.prep_both_done):
    ss.p_shared = 0
    ss.prep_cancel_requested = False
    ss.prep_both_running = True
    st.rerun()

if ss.prep_both_running:
    run_prepare_both()

st.caption("ì¤€ë¹„ ë²„íŠ¼ì„ ë‹¤ì‹œ í™œì„±í™”í•˜ë ¤ë©´ ì•„ë˜ ì¬ì„¤ì • ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
if st.button("ğŸ”§ ì¬ì„¤ì •(ë²„íŠ¼ ë‹¤ì‹œ í™œì„±í™”)", disabled=not ss.prep_both_done):
    ss.prep_both_done = False
    ss.p_shared = 0
    st.rerun()

# ===== ëŒ€í™” UI â€” í•­ìƒ ğŸ‘¥ ê·¸ë£¹í† ë¡  + ë¡œê·¸ ì €ì¥ =====
st.markdown("---")
st.subheader("ğŸ’¬ ê·¸ë£¹í† ë¡  ëŒ€í™” (ì‚¬ìš©ì â†’ Gemini 1ì°¨ â†’ ChatGPT ë³´ì™„/ê²€ì¦)")

ready_google = "qe_google" in ss
ready_openai = "qe_openai" in ss
if ss.session_terminated:
    st.warning("ì„¸ì…˜ì´ ì¢…ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤. ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”.")
    st.stop()
if not ready_google:
    st.info("ë¨¼ì € **[ğŸš€ í•œ ë²ˆì— ì¤€ë¹„í•˜ê¸°]** ë¥¼ í´ë¦­í•´ ë‘ë‡Œë¥¼ ì¤€ë¹„í•˜ì„¸ìš”. (OpenAI í‚¤ê°€ ì—†ìœ¼ë©´ Geminië§Œ ì‘ë‹µ)")
    st.stop()

# ì´ë¯¸ ìŒ“ì¸ ë©”ì‹œì§€ ë Œë”
for m in ss.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

def _strip_sources(text: str) -> str:
    # í•˜ë‹¨ ì°¸ê³ ìë£Œ ë¸”ë¡ ì œê±°
    return re.sub(r"\n+---\n\*ì°¸ê³  ìë£Œ:.*$", "", text, flags=re.DOTALL)

# === ìµœê·¼ ëŒ€í™” ë§¥ë½ êµ¬ì„±ê¸° =========================================
def _build_context_for_models(messages: list[dict], limit_pairs: int = 2, max_chars: int = 2000) -> str:
    """
    ìµœê·¼ user/assistant ìŒì„ limit_pairsê°œê¹Œì§€ ëª¨ì•„ ëª¨ë¸ì— ê±´ë„¤ì¤„ ë§¥ë½ ë¬¸ìì—´ ìƒì„±.
    ss.messages í¬ë§·: {"role": "user"|"assistant", "content": "..."}
    """
    pairs = []
    buf_user = None
    # ë’¤ì—ì„œ ì•ìœ¼ë¡œ í›‘ì–´ userâ†’assistant ìŒì„ ìˆ˜ì§‘
    for m in reversed(messages):
        role, content = m.get("role"), str(m.get("content", "")).strip()
        if role == "assistant":
            # í—¤ë”(**ğŸ¤– Gemini**) ì œê±°
            content = re.sub(r"^\*\*ğŸ¤– .*?\*\*\s*\n+", "", content).strip()
            if buf_user is not None:
                pairs.append((buf_user, content))
                buf_user = None
                if len(pairs) >= limit_pairs:
                    break
        elif role == "user":
            if buf_user is None:
                buf_user = content
    # ìµœì‹  â†’ ê³¼ê±° ìˆœì„œë¡œ ì •ë ¬
    pairs = list(reversed(pairs))
    blocks = []
    for u, a in pairs:
        blocks.append(f"[í•™ìƒ]\n{u}\n\n[êµì‚¬]\n{a}")
    ctx = "\n\n---\n\n".join(blocks).strip()
    # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
    if len(ctx) > max_chars:
        ctx = ctx[-max_chars:]
    return ctx
# ==================================================================

# ê³µí†µ: JSONL ë¡œê·¸ ì €ì¥(ì‹¤íŒ¨í•´ë„ ì•± ì¤‘ë‹¨ X) â€” chat_log/ ì„œë¸Œí´ë”ì— ì €ì¥
def _log_try(items):
    if not ss.save_logs:
        return
    try:
        # ìƒìœ„ ë°ì´í„° í´ë”(ID) â†’ chat_log/ ì„œë¸Œí´ë” IDë¡œ ë³€í™˜
        parent_id = (getattr(settings, "CHATLOG_FOLDER_ID", None) or settings.GDRIVE_FOLDER_ID)
        sub_id = get_chatlog_folder_id(
            parent_folder_id=parent_id,
            sa_json=settings.GDRIVE_SERVICE_ACCOUNT_JSON,
        )
        chat_store.append_jsonl(
            folder_id=sub_id,  # âœ… ì´ì œ ì„œë¸Œí´ë”ì— JSONL ì €ì¥
            sa_json=settings.GDRIVE_SERVICE_ACCOUNT_JSON,
            items=items,
        )
        st.toast("ëŒ€í™” ë¡œê·¸ ì €ì¥ ì™„ë£Œ", icon="ğŸ’¾")
    except Exception as e:
        st.caption(f"âš ï¸ ëŒ€í™” ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# ===== ì…ë ¥ì°½ & ì²˜ë¦¬ =====
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ë¶„ì„/ìš”ì•½í•  ë¬¸ì¥ì´ë‚˜ ê¸€ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")
if user_input:
    # 0) ì‚¬ìš©ì ë©”ì‹œì§€
    ss.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # JSONL ë¡œê·¸: ì‚¬ìš©ì
    _log_try([chat_store.make_entry(ss.session_id, "user", "user", user_input, mode, model="user")])

    # 1) Gemini 1ì°¨ (ìµœê·¼ ë§¥ë½ + í˜„ì¬ ì§ˆë¬¸)
    with st.spinner("ğŸ¤– Gemini ì„ ìƒë‹˜ì´ ë¨¼ì € ë‹µë³€í•©ë‹ˆë‹¤â€¦"):
        prev_ctx = _build_context_for_models(ss.messages[:-1], limit_pairs=2, max_chars=2000)
        gemini_query = (f"[ì´ì „ ëŒ€í™”]\n{prev_ctx}\n\n" if prev_ctx else "") + f"[í•™ìƒ ì§ˆë¬¸]\n{user_input}"
        ans_g = get_text_answer(ss["qe_google"], gemini_query, _persona())
    content_g = f"**ğŸ¤– Gemini**\n\n{ans_g}"
    ss.messages.append({"role": "assistant", "content": content_g})
    with st.chat_message("assistant"):
        st.markdown(content_g)

    # JSONL ë¡œê·¸: Gemini
    _log_try([chat_store.make_entry(
        ss.session_id, "assistant", "Gemini", content_g, mode,
        model=getattr(settings, "LLM_MODEL", "gemini")
    )])

    # 2) ChatGPT ë³´ì™„/ê²€ì¦ (RAG ì—†ì´, Gemini ë‹µë³€ì„ ì½ê³  ë³´ì™„)
    if ready_openai:
        from src.rag_engine import llm_complete

        review_directive = (
            "ì—­í• : ë‹¹ì‹ ì€ ë™ë£Œ AI ì˜ì–´êµì‚¬ì…ë‹ˆë‹¤.\n"
            "ëª©í‘œ: [ì´ì „ ëŒ€í™”], [í•™ìƒ ì§ˆë¬¸], [ë™ë£Œì˜ 1ì°¨ ë‹µë³€]ì„ ì½ê³ , ì‚¬ì‹¤ì˜¤ë¥˜/ë¹ ì§„ì /ëª¨í˜¸í•¨ì„ êµì •Â·ë³´ì™„í•©ë‹ˆë‹¤.\n"
            "ì§€ì¹¨:\n"
            "1) í•µì‹¬ë§Œ ê°„ê²°íˆ ì¬ì •ë¦¬\n"
            "2) í‹€ë¦° ë¶€ë¶„ì€ ê·¼ê±°ì™€ í•¨ê»˜ ë°”ë¡œì¡ê¸°\n"
            "3) ì´í•´ë¥¼ ë•ëŠ” ì˜ˆë¬¸ 2~3ê°œ ì¶”ê°€ (ê°€ëŠ¥í•˜ë©´ í•™ìŠµìì˜ ëª¨êµ­ì–´ ëŒ€ë¹„ í¬ì¸íŠ¸)\n"
            "4) ë§ˆì§€ë§‰ì— <ìµœì¢… ì •ë¦¬> ì„¹ì…˜ìœ¼ë¡œ í•œëˆˆ ìš”ì•½\n"
            "ê¸ˆì§€: ìƒˆë¡œìš´ ì™¸ë¶€ ê²€ìƒ‰/RAG. ì œê³µëœ ë‚´ìš©ê³¼ êµì‚¬ ì§€ì‹ë§Œ ì‚¬ìš©.\n"
        )

        prev_ctx = _build_context_for_models(ss.messages, limit_pairs=2, max_chars=2000)  # Gemini ë°©ê¸ˆ ë‹µ í¬í•¨
        augmented = (
            (f"[ì´ì „ ëŒ€í™”]\n{prev_ctx}\n\n" if prev_ctx else "") +
            f"[í•™ìƒ ì§ˆë¬¸]\n{user_input}\n\n"
            f"[ë™ë£Œì˜ 1ì°¨ ë‹µë³€(Gemini)]\n{_strip_sources(ans_g)}\n\n"
            f"[ë‹¹ì‹ ì˜ ì‘ì—…]\nìœ„ ê¸°ì¤€ìœ¼ë¡œë§Œ ë³´ì™„/ê²€ì¦í•˜ë¼."
        )

        with st.spinner("ğŸ¤ ChatGPT ì„ ìƒë‹˜ì´ ë³´ì™„/ê²€ì¦ ì¤‘â€¦"):
            ans_o = llm_complete(
                ss.get("llm_openai"),
                _persona() + "\n\n" + review_directive + "\n\n" + augmented
            )

        content_o = f"**ğŸ¤– ChatGPT**\n\n{ans_o}"
        ss.messages.append({"role": "assistant", "content": content_o})
        with st.chat_message("assistant"):
            st.markdown(content_o)
    else:
        with st.chat_message("assistant"):
            st.info("ChatGPT í‚¤ê°€ ì—†ì–´ Geminië§Œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤. OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ë©´ ë³´ì™„/ê²€ì¦ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

    # âœ… Drive Markdown ëŒ€í™” ë¡œê·¸ ìë™ ì €ì¥ (ê³µìœ ë“œë¼ì´ë¸Œì˜ ë°ì´í„° í´ë” ë‚´ chat_log/)
    if ss.auto_save_chatlog and ss.messages:
        try:
            save_chatlog_markdown(
                ss.session_id,
                ss.messages,
                parent_folder_id=(getattr(settings, "CHATLOG_FOLDER_ID", None) or settings.GDRIVE_FOLDER_ID),
            )
            st.toast("Driveì— ëŒ€í™” ì €ì¥ ì™„ë£Œ (chat_log/)", icon="ğŸ’¾")
        except Exception as e:
            st.caption(f"âš ï¸ Drive ì €ì¥ ì‹¤íŒ¨: {e}")

    st.rerun()
