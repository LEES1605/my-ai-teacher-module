# app.py â€” ì¸ë±ì‹± 1íšŒ + ë‘ LLM(Gemini/ChatGPT) ì¤€ë¹„ + ê°ì ì§„í–‰ë°”
#          â–¶ ì‹¤í–‰ ì¤‘/ì™„ë£Œ í›„ ë²„íŠ¼ ë¹„í™œì„±í™” + ì§„í–‰ë¥  ë‹¨ì¡°ì¦ê°€(ë˜ëŒë¦¼ ë°©ì§€)
#          â–¶ í•­ìƒ ğŸ‘¥ ê·¸ë£¹í† ë¡ : ì‚¬ìš©ì â†’ Gemini(1ì°¨) â†’ ChatGPT(ë³´ì™„/ê²€ì¦)
#          â–¶ 'â›” ì¤€ë¹„ ì·¨ì†Œ' ë²„íŠ¼ìœ¼ë¡œë§Œ ì¤‘ë‹¨ ê°€ëŠ¥(ì‹¤ìˆ˜ í´ë¦­ìœ¼ë¡œ ì¤‘ë‹¨ X)
#          â–¶ 'â¹ ì„¸ì…˜ ì¢…ë£Œ' ë²„íŠ¼ìœ¼ë¡œ ì•± ì‚¬ìš© ì¤‘ì—ë„ ì•ˆì „ ì¢…ë£Œ

import streamlit as st
import pandas as pd
import time
import re

# ===== í˜ì´ì§€ ì„¤ì • ============================================================
st.set_page_config(page_title="ë‚˜ì˜ AI ì˜ì–´ êµì‚¬", layout="wide", initial_sidebar_state="collapsed")

# ===== ê¸°ë³¸ UI/ìŠ¤íƒ€ì¼ =========================================================
from src.ui import load_css, render_header
load_css()
render_header()

st.info("âœ… ì¸ë±ì‹±ì€ 1ë²ˆë§Œ ìˆ˜í–‰í•˜ê³ , ê·¸ ì¸ë±ìŠ¤ë¡œ Gemini/ChatGPT ë‘ LLMì„ ì¤€ë¹„í•©ë‹ˆë‹¤. (ë¹ ë¥¸ ëª¨ë“œÂ·ì§„í–‰ë¥  ë˜ëŒë¦¼ ë°©ì§€Â·í•­ìƒ ğŸ‘¥ ê·¸ë£¹í† ë¡ )")

# ===== Google Drive ì—°ê²° í…ŒìŠ¤íŠ¸ ===============================================
try:
    from src.rag_engine import smoke_test_drive, preview_drive_files
except Exception:
    st.error("`src.rag_engine` ì„í¬íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    import traceback, os
    st.write("íŒŒì¼ ì¡´ì¬ ì—¬ë¶€:", os.path.exists("src/rag_engine.py"))
    with st.expander("ì„í¬íŠ¸ ìŠ¤íƒ(ì›ì¸)", expanded=True):
        st.code(traceback.format_exc())
    st.stop()

st.markdown("## ğŸ”— Google Drive ì—°ê²° í…ŒìŠ¤íŠ¸")
st.caption("ë²„íŠ¼ì„ ëˆŒëŸ¬ Drive í´ë” ì—°ê²°ì´ ì •ìƒì¸ì§€ í™•ì¸í•˜ì„¸ìš”. (ì„œë¹„ìŠ¤ê³„ì •ì— Viewer ì´ìƒ ê³µìœ  í•„ìš”)")

col1, col2 = st.columns([0.65, 0.35])
with col1:
    if st.button("í´ë” íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ìµœì‹  10ê°œ)", use_container_width=True):
        ok, msg, rows = preview_drive_files(max_items=10)
        if ok and rows:
            df = pd.DataFrame(rows)
            df["type"] = df["mime"].str.replace("application/vnd.google-apps.", "", regex=False)
            df = df.rename(columns={"modified": "modified_at"})
            df = df[["name", "link", "type", "modified_at"]]
            st.dataframe(
                df,
                use_container_width=True,
                height=360,
                column_config={
                    "name": st.column_config.TextColumn("íŒŒì¼ëª…"),
                    "link": st.column_config.LinkColumn("open", display_text="ì—´ê¸°"),
                    "type": st.column_config.TextColumn("ìœ í˜•"),
                    "modified_at": st.column_config.TextColumn("ìˆ˜ì •ì‹œê°"),
                },
                hide_index=True,
            )
        elif ok:
            st.warning("í´ë”ì— íŒŒì¼ì´ ì—†ê±°ë‚˜ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.error(msg)
with col2:
    ok, msg = smoke_test_drive()
    if ok:
        st.success(msg)
    else:
        st.warning(msg)

# ===== ë‘ë‡Œ ì¤€ë¹„ (ì‹¤ì „) â€” ê³µí†µ ì¸ë±ìŠ¤ + LLM 2ê°œ ===============================
st.markdown("----")
st.subheader("ğŸ§  ë‘ë‡Œ ì¤€ë¹„ â€” ì¸ë±ìŠ¤ 1íšŒ + Gemini/ChatGPT LLM")

from src.config import settings
try:
    from src.rag_engine import (
        set_embed_provider, make_llm, get_or_build_index, get_text_answer, CancelledError
    )
except Exception:
    st.error("`src.rag_engine` ì„í¬íŠ¸(LLM/RAG) ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    import traceback
    with st.expander("ì„í¬íŠ¸ ìŠ¤íƒ(ì›ì¸)", expanded=True):
        st.code(traceback.format_exc())
    st.stop()

# â–¶ ì„¸ì…˜ ìƒíƒœ ê¸°ë³¸ê°’
if "prep_both_running" not in st.session_state:
    st.session_state.prep_both_running = False
if "prep_both_done" not in st.session_state:
    st.session_state.prep_both_done = ("qe_google" in st.session_state) or ("qe_openai" in st.session_state)
if "p_shared" not in st.session_state:
    st.session_state.p_shared = 0  # ì§„í–‰ë¥  ìµœëŒ€ê°’(ë‹¨ì¡°ì¦ê°€)
if "prep_cancel_requested" not in st.session_state:
    st.session_state.prep_cancel_requested = False
if "session_terminated" not in st.session_state:
    st.session_state.session_terminated = False

# â–¶ 'ì„¸ì…˜ ì¢…ë£Œ' ë²„íŠ¼ (ì–¸ì œë“  ëˆ„ë¥´ë©´ ì„¸ì…˜ ì¢…ë£Œ)
with st.container():
    colx, coly = st.columns([0.75, 0.25])
    with coly:
        if st.button("â¹ ì„¸ì…˜ ì¢…ë£Œ", use_container_width=True, type="secondary"):
            st.session_state.session_terminated = True
            st.session_state.prep_both_running = False
            st.session_state.prep_cancel_requested = False
            st.warning("ì„¸ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ë©´ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.")
            st.stop()

# â–¶ ì§„í–‰ë¥  ë Œë”
def _render_progress(slot_bar, slot_msg, pct: int, msg: str | None = None):
    p = max(0, min(100, int(pct)))
    slot_bar.markdown(f"""
<div class="gp-wrap"><div class="gp-fill" style="width:{p}%"></div><div class="gp-label">{p}%</div></div>
""", unsafe_allow_html=True)
    if msg is not None:
        slot_msg.markdown(f"<div class='gp-msg'>{msg}</div>", unsafe_allow_html=True)

# â–¶ ì§„í–‰ë¥  ë‹¨ì¡°ì¦ê°€(ë˜ëŒë¦¼ ë°©ì§€)
def _bump_max(key: str, pct: int) -> int:
    now = int(pct)
    prev = int(st.session_state.get(key, 0))
    if now < prev:
        now = prev
    st.session_state[key] = now
    return now

# â–¶ ì˜µì…˜(ë¹ ë¥¸ ëª¨ë“œ) â€” ì‹¤í–‰ ì¤‘/ì™„ë£Œ ì‹œ ë¹„í™œì„±í™”
with st.expander("âš™ï¸ ì˜µì…˜", expanded=False):
    fast = st.checkbox("âš¡ ë¹ ë¥¸ ì¤€ë¹„ (ì²˜ìŒ Nê°œ ë¬¸ì„œë§Œ ì¸ë±ì‹±)", value=True,
                       disabled=st.session_state.prep_both_running or st.session_state.prep_both_done)
    max_docs = st.number_input("N (ë¹ ë¥¸ ëª¨ë“œì¼ ë•Œë§Œ ì ìš©)", min_value=5, max_value=500, value=40, step=5,
                               disabled=st.session_state.prep_both_running or st.session_state.prep_both_done)

st.markdown("### ğŸš€ ì¸ë±ì‹± 1ë²ˆ + ë‘ LLM ì¤€ë¹„")
c_g, c_o = st.columns(2)
with c_g: st.caption("Gemini ì§„í–‰"); g_bar = st.empty(); g_msg = st.empty()
with c_o: st.caption("ChatGPT ì§„í–‰"); o_bar = st.empty(); o_msg = st.empty()

def _is_cancelled() -> bool:
    """ì¤€ë¹„ ì¤‘ ì‚¬ìš©ìê°€ 'â›” ì¤€ë¹„ ì·¨ì†Œ'ë¥¼ ëˆŒë €ëŠ”ì§€ í™•ì¸"""
    return bool(st.session_state.get("prep_cancel_requested", False))

def run_prepare_both():
    """ê³µí†µ ì¸ë±ìŠ¤ 1íšŒ + ë‘ LLM ì¤€ë¹„. ì˜¤ì§ 'â›” ì¤€ë¹„ ì·¨ì†Œ'ë¡œë§Œ ì¤‘ë‹¨ ê°€ëŠ¥."""
    # 0) ì´ˆê¸° ë©”ì‹œì§€
    _render_progress(g_bar, g_msg, st.session_state.p_shared, "ëŒ€ê¸° ì¤‘â€¦")
    _render_progress(o_bar, o_msg, st.session_state.p_shared, "ëŒ€ê¸° ì¤‘â€¦")

    # ë‚´ë¶€ ì²´í¬ í•¨ìˆ˜
    def _check_cancel():
        if _is_cancelled():
            raise CancelledError("ì‚¬ìš©ì ì·¨ì†Œ")

    # 1) ì„ë² ë”© ê³µê¸‰ì ê²°ì •
    embed_provider = "openai"
    embed_api = getattr(settings, "OPENAI_API_KEY", None).get_secret_value() if hasattr(settings, "OPENAI_API_KEY") else ""
    embed_model = getattr(settings, "OPENAI_EMBED_MODEL", "text-embedding-3-small")
    if not embed_api:
        embed_provider = "google"
        embed_api = settings.GEMINI_API_KEY.get_secret_value()
        embed_model = getattr(settings, "EMBED_MODEL", "text-embedding-004")

    persist_dir = f"{getattr(settings, 'PERSIST_DIR', '/tmp/my_ai_teacher/storage_gdrive')}_shared"

    # 2) ì„ë² ë”© ì„¤ì • (ì–‘ìª½ ë°” ë™ì‹œ ê°±ì‹ )
    try:
        _check_cancel()
        p = _bump_max("p_shared", 5)
        _render_progress(g_bar, g_msg, p, f"ì„ë² ë”© ì„¤ì •({embed_provider})")
        _render_progress(o_bar, o_msg, p, f"ì„ë² ë”© ì„¤ì •({embed_provider})")
        set_embed_provider(embed_provider, embed_api, embed_model)
    except CancelledError:
        # ì·¨ì†Œ ì²˜ë¦¬
        st.session_state.prep_both_running = False
        st.session_state.prep_cancel_requested = False
        _render_progress(g_bar, g_msg, st.session_state.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        _render_progress(o_bar, o_msg, st.session_state.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        st.stop()
    except Exception as e:
        p = _bump_max("p_shared", 100)
        _render_progress(g_bar, g_msg, p, f"ì„ë² ë”© ì„¤ì • ì‹¤íŒ¨: {e}")
        _render_progress(o_bar, o_msg, p, f"ì„ë² ë”© ì„¤ì • ì‹¤íŒ¨: {e}")
        st.session_state.prep_both_running = False
        st.stop()

    # 3) ì¸ë±ìŠ¤ ë¡œë”©/ë¹Œë“œ (ê³µí†µ 1íšŒ)
    try:
        def upd(pct: int, msg: str | None = None):
            if _is_cancelled():
                raise CancelledError("ì‚¬ìš©ì ì·¨ì†Œ(ì§„í–‰ ì¤‘)")
            p = _bump_max("p_shared", pct)
            _render_progress(g_bar, g_msg, p, msg)
            _render_progress(o_bar, o_msg, p, msg)

        def umsg(m: str):
            p = st.session_state.p_shared
            _render_progress(g_bar, g_msg, p, m)
            _render_progress(o_bar, o_msg, p, m)

        index = get_or_build_index(
            update_pct=upd,
            update_msg=umsg,
            gdrive_folder_id=settings.GDRIVE_FOLDER_ID,
            raw_sa=settings.GDRIVE_SERVICE_ACCOUNT_JSON,
            persist_dir=persist_dir,
            manifest_path=getattr(settings, "MANIFEST_PATH", "/tmp/my_ai_teacher/drive_manifest.json"),
            max_docs=(max_docs if fast else None),
            is_cancelled=_is_cancelled,  # ğŸ”´ ì·¨ì†Œ ì½œë°± ì „ë‹¬
        )
    except CancelledError:
        st.session_state.prep_both_running = False
        st.session_state.prep_cancel_requested = False
        _render_progress(g_bar, g_msg, st.session_state.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        _render_progress(o_bar, o_msg, st.session_state.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        st.stop()
    except Exception as e:
        p = _bump_max("p_shared", 100)
        _render_progress(g_bar, g_msg, p, f"ì¸ë±ìŠ¤ ì‹¤íŒ¨: {e}")
        _render_progress(o_bar, o_msg, p, f"ì¸ë±ìŠ¤ ì‹¤íŒ¨: {e}")
        st.session_state.prep_both_running = False
        st.stop()

    # 4) LLM ë‘ ê°œ ì¤€ë¹„
    # 4-1) Gemini
    try:
        _check_cancel()
        g_llm = make_llm(
            provider="google",
            api_key=settings.GEMINI_API_KEY.get_secret_value(),
            llm_model=getattr(settings, "LLM_MODEL", "gemini-1.5-pro"),
            temperature=float(st.session_state.get("temperature", 0.0)),
        )
        qe_g = index.as_query_engine(
            llm=g_llm,
            response_mode=st.session_state.get("response_mode", getattr(settings, "RESPONSE_MODE", "compact")),
            similarity_top_k=int(st.session_state.get("similarity_top_k", getattr(settings, "SIMILARITY_TOP_K", 5))),
        )
        st.session_state["qe_google"] = qe_g
        _render_progress(g_bar, g_msg, 100, "ì™„ë£Œ!")
    except CancelledError:
        st.session_state.prep_both_running = False
        st.session_state.prep_cancel_requested = False
        _render_progress(g_bar, g_msg, st.session_state.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        st.stop()
    except Exception as e:
        _render_progress(g_bar, g_msg, 100, f"Gemini ì¤€ë¹„ ì‹¤íŒ¨: {e}")

    # 4-2) ChatGPT
    try:
        if hasattr(settings, "OPENAI_API_KEY") and settings.OPENAI_API_KEY.get_secret_value():
            _check_cancel()
            o_llm = make_llm(
                provider="openai",
                api_key=settings.OPENAI_API_KEY.get_secret_value(),
                llm_model=getattr(settings, "OPENAI_LLM_MODEL", "gpt-4o-mini"),
                temperature=float(st.session_state.get("temperature", 0.0)),
            )
            qe_o = index.as_query_engine(
                llm=o_llm,
                response_mode=st.session_state.get("response_mode", getattr(settings, "RESPONSE_MODE", "compact")),
                similarity_top_k=int(st.session_state.get("similarity_top_k", getattr(settings, "SIMILARITY_TOP_K", 5))),
            )
            st.session_state["qe_openai"] = qe_o
            _render_progress(o_bar, o_msg, 100, "ì™„ë£Œ!")
        else:
            _render_progress(o_bar, o_msg, 100, "í‚¤ ëˆ„ë½ â€” OPENAI_API_KEY í•„ìš”")
    except CancelledError:
        st.session_state.prep_both_running = False
        st.session_state.prep_cancel_requested = False
        _render_progress(o_bar, o_msg, st.session_state.p_shared, "ì‚¬ìš©ì ì·¨ì†Œ")
        st.stop()
    except Exception as e:
        _render_progress(o_bar, o_msg, 100, f"ChatGPT ì¤€ë¹„ ì‹¤íŒ¨: {e}")

    # 5) ì™„ë£Œ ì²˜ë¦¬: ë²„íŠ¼ ì˜êµ¬ ë¹„í™œì„±í™” (ì¬ì„¤ì • ì „ê¹Œì§€)
    st.session_state.prep_both_running = False
    st.session_state.prep_both_done = True
    time.sleep(0.2)
    st.rerun()

# â–¶ ë²„íŠ¼/ì·¨ì†Œ ë²„íŠ¼ UI
left, right = st.columns([0.7, 0.3])
with left:
    clicked = st.button(
        "ğŸš€ í•œ ë²ˆì— ì¤€ë¹„í•˜ê¸°",
        key="prepare_both",
        use_container_width=True,
        disabled=st.session_state.prep_both_running or st.session_state.prep_both_done,
    )
with right:
    # ì¤€ë¹„ ì¤‘ì¼ ë•Œë§Œ 'â›” ì¤€ë¹„ ì·¨ì†Œ' ë²„íŠ¼ ë…¸ì¶œ (ì‹¤ìˆ˜ í´ë¦­ìœ¼ë¡œëŠ” ì·¨ì†Œë˜ì§€ ì•ŠìŒ!)
    cancel_clicked = st.button(
        "â›” ì¤€ë¹„ ì·¨ì†Œ",
        key="cancel_prepare",
        use_container_width=True,
        type="secondary",
        disabled=not st.session_state.prep_both_running,
    )

if cancel_clicked and st.session_state.prep_both_running:
    st.session_state.prep_cancel_requested = True
    st.rerun()

if clicked and not (st.session_state.prep_both_running or st.session_state.prep_both_done):
    st.session_state.p_shared = 0  # ì§„í–‰ë¥  ìµœëŒ€ê°’ ë¦¬ì…‹
    st.session_state.prep_cancel_requested = False
    st.session_state.prep_both_running = True
    st.rerun()

# â–¶ í”Œë˜ê·¸ê°€ Trueë©´ ì‹¤ì œ ì¤€ë¹„ ë£¨í‹´ ìˆ˜í–‰
if st.session_state.prep_both_running:
    run_prepare_both()

# â–¶ ì¬ì„¤ì •(ë‹¤ì‹œ ì¤€ë¹„ í—ˆìš©)
st.caption("ì¤€ë¹„ ë²„íŠ¼ì„ ë‹¤ì‹œ í™œì„±í™”í•˜ë ¤ë©´ ì•„ë˜ ì¬ì„¤ì • ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
if st.button("ğŸ”§ ì¬ì„¤ì •(ë²„íŠ¼ ë‹¤ì‹œ í™œì„±í™”)", disabled=not st.session_state.prep_both_done):
    st.session_state.prep_both_done = False
    st.session_state.p_shared = 0
    st.experimental_rerun()

# ===== ëŒ€í™” UI â€” í•­ìƒ ğŸ‘¥ ê·¸ë£¹í† ë¡  ==============================================
st.markdown("---")
st.subheader("ğŸ’¬ ê·¸ë£¹í† ë¡  ëŒ€í™” (ì‚¬ìš©ì â†’ Gemini 1ì°¨ â†’ ChatGPT ë³´ì™„/ê²€ì¦)")

ready_google = "qe_google" in st.session_state
ready_openai = "qe_openai" in st.session_state

if st.session_state.session_terminated:
    st.warning("ì„¸ì…˜ì´ ì¢…ë£Œëœ ìƒíƒœì…ë‹ˆë‹¤. í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨(Ctrl/âŒ˜+Shift+R)ìœ¼ë¡œ ë‹¤ì‹œ ì‹œì‘í•˜ì„¸ìš”.")
    st.stop()

if not ready_google:
    st.info("ë¨¼ì € ìœ„ì˜ **[ğŸš€ í•œ ë²ˆì— ì¤€ë¹„í•˜ê¸°]**ë¥¼ í´ë¦­í•´ ë‘ë‡Œë¥¼ ì¤€ë¹„í•˜ì„¸ìš”. (OpenAI í‚¤ê°€ ì—†ìœ¼ë©´ Geminië§Œ ì‘ë‹µ)")
    st.stop()

# ëŒ€í™” ê¸°ë¡
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ë©”ì‹œì§€ ë Œë”
for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# í˜ë¥´ì†Œë‚˜(í•™ìŠµ ëª¨ë“œ)
from src.prompts import EXPLAINER_PROMPT, ANALYST_PROMPT, READER_PROMPT
mode = st.radio("í•™ìŠµ ëª¨ë“œ", ["ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…", "ğŸ” êµ¬ë¬¸ ë¶„ì„", "ğŸ“š ë…í•´ ë° ìš”ì•½"], horizontal=True, key="mode_select")

def _persona():
    return EXPLAINER_PROMPT if mode == "ğŸ’¬ ì´ìœ ë¬¸ë²• ì„¤ëª…" else (ANALYST_PROMPT if mode == "ğŸ” êµ¬ë¬¸ ë¶„ì„" else READER_PROMPT)

def _strip_sources(text: str) -> str:
    """ìš°ë¦¬ get_text_answerê°€ ë’¤ì— ë¶™ì´ëŠ” '*ì°¸ê³  ìë£Œ: ...*' ê¼¬ë¦¬ë¥¼ ì œê±°."""
    return re.sub(r"\n+---\n\*ì°¸ê³  ìë£Œ:.*$", "", text, flags=re.DOTALL)

# ì…ë ¥ì°½ (í•­ìƒ ê·¸ë£¹í† ë¡  ì‹¤í–‰)
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜, ë¶„ì„/ìš”ì•½í•  ë¬¸ì¥ì´ë‚˜ ê¸€ì„ ë¶™ì—¬ë„£ìœ¼ì„¸ìš”.")
if user_input:
    # 0) ì‚¬ìš©ì ë©”ì‹œì§€
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # 1) Gemini 1ì°¨ ë‹µë³€
    with st.spinner("ğŸ¤– Gemini ì„ ìƒë‹˜ì´ ë¨¼ì € ë‹µë³€í•©ë‹ˆë‹¤â€¦"):
        ans_g = get_text_answer(st.session_state["qe_google"], user_input, _persona())
    content_g = f"**ğŸ¤– Gemini**\n\n{ans_g}"
    st.session_state.messages.append({"role": "assistant", "content": content_g})
    with st.chat_message("assistant"):
        st.markdown(content_g)

    # 2) ChatGPT ë³´ì™„/ê²€ì¦ (ìˆì„ ë•Œ)
    if ready_openai:
        review_directive = (
            "ë‹¹ì‹ ì€ ë™ë£Œ AI êµì‚¬ì…ë‹ˆë‹¤. ì•„ë˜ [í•™ìƒ ì§ˆë¬¸]ê³¼ [ë™ë£Œì˜ 1ì°¨ ë‹µë³€]ì„ ì½ê³  "
            "ë¶€ì¡±í•œ ë¶€ë¶„ì„ ë³´ì™„/êµì •í•˜ê³  ì˜ˆì‹œë¥¼ ì¶”ê°€í•œ ë’¤, ë§ˆì§€ë§‰ì— 'ìµœì¢… ì •ë¦¬'ë¥¼ ì œì‹œí•˜ì„¸ìš”. "
            "ê°€ëŠ¥í•˜ë©´ ê·¼ê±°(ìë£Œ íŒŒì¼ëª…)ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ë³´ê°•í•˜ì„¸ìš”."
        )
        augmented_question = (
            f"[í•™ìƒ ì§ˆë¬¸]\n{user_input}\n\n"
            f"[ë™ë£Œì˜ 1ì°¨ ë‹µë³€(Gemini)]\n{_strip_sources(ans_g)}"
        )
        with st.spinner("ğŸ¤ ChatGPT ì„ ìƒë‹˜ì´ ë³´ì™„/ê²€ì¦ ì¤‘â€¦"):
            ans_o = get_text_answer(st.session_state["qe_openai"],
                                    augmented_question,
                                    _persona() + "\n" + review_directive)
        content_o = f"**ğŸ¤– ChatGPT**\n\n{ans_o}"
        st.session_state.messages.append({"role": "assistant", "content": content_o})
        with st.chat_message("assistant"):
            st.markdown(content_o)
    else:
        with st.chat_message("assistant"):
            st.info("ChatGPT í‚¤ê°€ ì—†ì–´ Geminië§Œ ì‘ë‹µí–ˆìŠµë‹ˆë‹¤. OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ë©´ ë³´ì™„/ê²€ì¦ì´ í™œì„±í™”ë©ë‹ˆë‹¤.")

    st.rerun()
